{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44c59d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1,2 AND 3  \n",
    "import urllib.request\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f6b8268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method lower of str object at 0x7face9fc09f0>\n",
      "<built-in method lower of str object at 0x7facd96dbc90>\n",
      "<built-in method lower of str object at 0x7facd9603df0>\n",
      "<built-in method lower of str object at 0x7facca0d83b0>\n",
      "<built-in method lower of str object at 0x7facd9603df0>\n",
      "<built-in method lower of str object at 0x7facc24e6400>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7facca461130>\n",
      "<built-in method lower of str object at 0x7facc8bb3c40>\n",
      "<built-in method lower of str object at 0x7face8050030>\n",
      "<built-in method lower of str object at 0x7face8050030>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7face9fc0c30>\n",
      "<built-in method lower of str object at 0x7faca0b3db20>\n",
      "<built-in method lower of str object at 0x7facc24e6400>\n",
      "<built-in method lower of str object at 0x7facc24e6400>\n",
      "<built-in method lower of str object at 0x7faca0b3aef0>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7face9fac2b0>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7facca461130>\n",
      "<built-in method lower of str object at 0x7facca2cc870>\n",
      "<built-in method lower of str object at 0x7facea028140>\n",
      "<built-in method lower of str object at 0x7face8050030>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7facd94bf6e0>\n",
      "<built-in method lower of str object at 0x7facd9603df0>\n",
      "<built-in method lower of str object at 0x7facd93e7870>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7face9f4c830>\n",
      "<built-in method lower of str object at 0x7face9fac2b0>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7face9f4c830>\n",
      "<built-in method lower of str object at 0x7facc9d38a50>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7face9f4c830>\n",
      "<built-in method lower of str object at 0x7face9f88d30>\n",
      "<built-in method lower of str object at 0x7facd96dbc90>\n",
      "<built-in method lower of str object at 0x7facc8bb3c40>\n",
      "<built-in method lower of str object at 0x7face9fac2b0>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7face8050030>\n",
      "<built-in method lower of str object at 0x7facca461130>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7facd94bf6e0>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7face9f88d30>\n",
      "<built-in method lower of str object at 0x7facc9d38a50>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7face9f67000>\n",
      "<built-in method lower of str object at 0x7faca0b3db10>\n",
      "<built-in method lower of str object at 0x7facc24e6400>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7face9fd4850>\n",
      "<built-in method lower of str object at 0x7facd9665fb0>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7facc9624500>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7faca0b3d7b0>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7faca0b3a180>\n",
      "<built-in method lower of str object at 0x7face9f57f30>\n",
      "<built-in method lower of str object at 0x7face8050030>\n",
      "openai ceo sam altman (left) and meta ai chief yann lecun (right) have differing views on the future ... [+] of large language models.\n",
      "in case you haven’t heard, artificial intelligence is the hot new thing.\n",
      "generative ai seems to be on the lips of every venture capitalist, entrepreneur, fortune 500 ceo and journalist these days, from silicon valley to davos.\n",
      "to those who started paying real attention to ai in 2022, it may seem that technologies like chatgpt and stable diffusion came out of nowhere to take the world by storm. they didn’t.\n",
      "back in 2020, we wrote an article in this column predicting that generative ai would be one of the pillars of the next generation of artificial intelligence.\n",
      "since at least the release of gpt-2 in 2019, it has been clear to those working in the field that generative language models were poised to unleash vast economic and societal transformation. similarly, while text-to-image models only captured the public’s attention last summer, the technology’s ascendance has appeared inevitable since openai released the original dall-e in january 2021. (we wrote an article making this argument days after the release of the original dall-e.)\n",
      "by this same token, it is important to remember that the current state of the art in ai is far from an end state for ai’s capabilities. on the contrary, the frontiers of artificial intelligence have never advanced more rapidly than they are right now. as amazing as chatgpt seems to us at the moment, it is a mere stepping stone to what comes next.\n",
      "what will the next generation of large language models (llms) look like? the answer to this question is already out there, under development at ai startups and research groups at this very moment.\n",
      "this article highlights three emerging areas that will help define the next wave of innovation in generative ai and llms. for those looking to remain ahead of the curve in this fast-changing world—read on.\n",
      "\n",
      "\n",
      "consider how humans think and learn. we collect knowledge and perspective from external sources of information—say, by reading a book. but we also generate novel ideas and insights on our own, by reflecting on a topic or thinking through a problem in our minds. we are able to deepen our understanding of the world via internal reflection and analysis not directly tied to any new external input.\n",
      "a new avenue of ai research seeks to enable large language models to do something analogous, effectively bootstrapping their own intelligence.\n",
      "as part of their training, today’s llms ingest much of the world’s accumulated written information (e.g., wikipedia, books, news articles). what if these models, once trained, could use all the knowledge that they have absorbed from these sources to produce new written content—and then use that content as additional training data in order to improve themselves? initial work suggests that this approach may be possible—and powerful.\n",
      "in one recent research effort, aptly titled “large language models can self-improve,” a group of google researchers built an llm that can come up with a set of questions, generate detailed answers to those questions, filter its own answers for the most high-quality output, and then fine-tune itself on the curated answers. remarkably, this leads to new state-of-the-art performance on various language tasks. for instance, the model’s performance increased from 74.2% to 82.1% on gsm8k and from 78.2% to 83.0% on drop, two popular benchmarks used to evaluate llm performance.\n",
      "another recent work builds on an important llm method called “instruction fine-tuning,” which lies at the core of products like chatgpt. whereas chatgpt and other instruction fine-tuned models rely on human-written instructions, this research group built a model that can generate its own natural language instructions and then fine-tune itself on those instructions. the performance gains are dramatic: this method improves the performance of the base gpt-3 model by 33%, nearly matching the performance of openai’s own instruction-tuned model.\n",
      "in a thematically related work, researchers from google and carnegie mellon show that if a large language model, when presented with a question, first recites to itself what it knows about the topic before responding, it provides more accurate and sophisticated responses. this can be loosely analogized to a human in conversation who, rather than blurting out the first thing that comes to mind on a topic, searches her memory and reflects on her beliefs before sharing a perspective.\n",
      "when people first hear about this line of research, a conceptual objection often arises—isn’t this all circular? how can a model produce data that the model can then consume to improve itself? if the new data came from the model in the first place, shouldn’t the “knowledge” or “signal” that it contains already be incorporated in the model?\n",
      "this objection makes sense if we conceive of large language models as databases, storing information from their training data and reproducing it in different combinations when prompted. but—uncomfortable or even eerie as it may sound—we are better off instead conceiving of large language models along the lines of the human brain (no, the analogy is of course not perfect!).\n",
      "we humans ingest a tremendous amount of data from the world that alters the neural connections in our brains in imponderable, innumerable ways. through introspection, writing, conversation—sometimes just a good night’s sleep—our brains can then produce new insights that had not previously been in our minds nor in any information source out in the world. if we internalize these new insights, they can make us wiser.\n",
      "the idea that llms can generate their own training data is particularly important in light of the fact that the world may soon run out of text training data. this is not yet a widely appreciated problem, but it is one that many ai researchers are worried about.\n",
      "by one estimate, the world’s total stock of usable text data is between 4.6 trillion and 17.2 trillion tokens. this includes all the world’s books, all scientific papers, all news articles, all of wikipedia, all publicly available code, and much of the rest of the internet, filtered for quality (e.g., webpages, blogs, social media). another recent estimate puts the total figure at 3.2 trillion tokens.\n",
      "deepmind’s chinchilla, one of today’s leading llms, was trained on 1.4 trillion tokens.\n",
      "in other words, we may be well within one order of magnitude of exhausting the world’s entire supply of useful language training data.\n",
      "if large language models are able to generate their own training data and use it to continue self-improving, this could render irrelevant the looming data shortage. it would represent a mind-bending leap forward for llms.\n",
      "\n",
      "a popular narrative these days is that chatgpt and conversational llms like it are on the verge of replacing google search as the world’s go-to source for information, disrupting the once-mighty tech giant like blockbuster or kodak were disrupted before it.\n",
      "this narrative badly oversimplifies things. llms as they exist today will never replace google search. why not? in short, because today’s llms make stuff up.\n",
      "as powerful as they are, large language models regularly produce inaccurate, misleading or false information (and present it confidently and convincingly).\n",
      "examples abound of chatgpt’s “hallucinations” (as these misstatements are referred to). this is not to single out chatgpt; every generative language model in existence today hallucinates in similar ways.\n",
      "to give a few examples: it recommends books that don’t exist; it insists that the number 220 is less than 200; it is unsure whether abraham lincoln’s assassin was on the same continent as lincoln at the time of the assassination; it provides plausible-sounding but incorrect explanations of concepts like bayes’ theorem.\n",
      "most users will not accept a search engine that gets basic facts like these wrong some of the time; even 99% accuracy will not be good enough for broad market adoption. openai ceo sam altman himself acknowledges this, recently cautioning: “chatgpt is incredibly limited, but good enough at some things to create a misleading impression of greatness. it's a mistake to be relying on it for anything important right now.”\n",
      "it is an open question whether llms’ hallucination problem can be solved via incremental improvements to existing architectures, or whether a more fundamental paradigm shift in ai methodologies will be necessary to give ai common sense and real understanding. deep learning pioneer yann lecun, for one, believes the latter. lecun’s contrarian perspective may prove correct; time will tell.\n",
      "in the nearer term, though, a set of promising innovations offers to at least mitigate llms’ factual unreliability. these new methods will play an essential role in preparing llms for widespread real-world deployment.\n",
      "two related capabilities lie at the heart of current efforts to make language models more accurate: (1) the ability for llms to retrieve information from external sources, and (2) the ability for llms to provide references and citations for the information they provide.\n",
      "chatgpt is limited to the information that is already stored inside of it, captured in its static weights. (this is why it is not able to discuss events that occurred after 2021, when the model was trained.) being able to pull in information from external sources will empower llms to access the most accurate and up-to-date information available, even when that information changes frequently (say, companies’ stock prices).\n",
      "of course, having access to an external information source does not by itself guarantee that llms will retrieve the most accurate and relevant information. one important way for llms to increase transparency and trust with human users is to include references to the source(s) from which they retrieved the information. such citations allow human users to audit the information source as needed in order to decide for themselves on its reliability.\n",
      "important early work in this field includes models like realm (from google) and rag (from facebook), both published in 2020. with the rise of conversational llms in recent months, research in this area is now rapidly accelerating.\n",
      "last year, openai published a fine-tuned version of its gpt model named webgpt that can browse the internet using microsoft bing in order to provide more accurate and in-depth responses to prompts. webgpt navigates the internet much like a human does: it can submit search queries to bing, follow links, scroll up and down on webpages, and use functions like ctrl+f to find terms. when the model finds relevant information on the internet that it incorporates into its output, it provides citations so that the human user can see where the information came from.\n",
      "the results are encouraging: for the same query, webgpt’s responses are preferred to responses written by human subjects 56% of the time and are preferred to the highest-rated responses on reddit 69% of the time.\n",
      "deepmind is also pursuing research along these lines. a few months ago, deepmind published a new model named sparrow. like chatgpt, sparrow is dialogue-based; like webgpt, it can search the internet for information and provide citations for its assertions. sparrow builds on important earlier work out of deepmind including spalm, retro and gophercite.\n",
      "deepmind's sparrow model in action. as shown here, sparrow provides quotations and links to support ... [+] its statements, increasing their accuracy and trustworthiness.\n",
      "the deepmind researchers find that sparrow’s citations are helpful and accurate 78% of the time—suggesting both that this research approach is promising and that the problem of llm inaccuracy is far from solved.\n",
      "younger startups including you.com and perplexity have also recently launched llm-powered conversational search interfaces with the ability to retrieve information from external sources and cite references. these products are available for public use today.\n",
      "llms’ greatest shortcoming is their unreliability, their stubborn tendency to confidently provide inaccurate information. language models promise to reshape every sector of our economy, but they will never reach their full potential until this problem is addressed. expect to see plenty of activity and innovation in this area in the months ahead.\n",
      "\n",
      "today’s most prominent large language models all have effectively the same architecture.\n",
      "meta ai chief yann lecun said recently: “in terms of underlying techniques, chatgpt is not particularly innovative. it’s nothing revolutionary, although that’s the way it’s perceived in the public. it’s just that, you know, it’s well put together, it’s nicely done.”\n",
      "lecun’s statement stirred up plenty of controversy and twitter debate. but the simple fact is that he is correct, as no serious ai researcher would dispute.\n",
      "all of today’s well-known language models—e.g., gpt-3 from openai, palm or lamda from google, galactica or opt from meta, megatron-turing from nvidia/microsoft, jurassic-1 from ai21 labs—are built in the same basic way. they are autoregressive, self-supervised, pre-trained, densely activated transformer-based models.\n",
      "to be sure, variations among these models exist: their size (parameter count), the data they are trained on, the optimization algorithm used, the batch size, the number of hidden layers, whether they are instruction fine-tuned, and so on. these variations can translate to meaningful performance differences. the core architectures, though, vary little.\n",
      "yet momentum is building behind an intriguingly different architectural approach to language models known as sparse expert models. while the idea has been around for decades, it has only recently reemerged and begun to gain in popularity.\n",
      "all of the models mentioned above are dense. this means that every time the model runs, every single one of its parameters is used. every time you submit a prompt to gpt-3, for instance, all 175 billion of the model’s parameters are activated in order to produce its response.\n",
      "but what if a model were able to call upon only the most relevant subset of its parameters in order to respond to a given query? this is the basic concept behind sparse expert models.\n",
      "the defining characteristic of sparse models is that they don’t activate all of their parameters for a given input, but rather only those parameters that are helpful in order to handle the input. model sparsity thus decouples a model’s total parameter count from its compute requirements. this leads to sparse expert models’ key advantage: they can be both larger and less computationally demanding than dense models.\n",
      "why are they called sparse expert models? because sparse models can be thought of as consisting of a collection of “sub-models” that serve as experts on different topics. depending on the prompt presented to the model, the most relevant experts within the model are activated while the other experts remain inactive. a prompt posed in russian, for instance, would only activate the “experts” within a model that can understand and respond in russian, efficiently bypassing the rest of the model.\n",
      "all of today’s largest llms are sparse. if you come across an llm with more than 1 trillion parameters, you can safely assume that it is sparse. this includes google’s switch transformer (1.6 trillion parameters), google’s glam (1.2 trillion parameters) and meta’s mixture of experts model (1.1 trillion parameters).\n",
      "“much of the recent progress in ai has come from training larger and larger models,” said mikel artetxe, who led meta’s research on sparse models before resigning to cofound a stealth llm startup. “gpt-3, for instance, is more than 100 times larger than gpt-2. but when we double the size of a dense model, we also make it twice as slow. sparse models allow us to train larger models without the increase in runtime.”\n",
      "recent research on sparse expert models suggests that this architecture holds massive potential.\n",
      "glam, a sparse expert model developed last year by google, is 7 times larger than gpt-3, requires two-thirds less energy to train, requires half as much compute for inference, and outperforms gpt-3 on a wide range of natural language tasks. similar work on sparse models out of meta has yielded similarly promising results.\n",
      "as the meta researchers summarize: “we find that sparse models can achieve similar downstream task performance as dense models at a fraction of the compute. for models with relatively modest compute budgets, a sparse model can perform on par with a dense model that requires almost four times as much compute.”\n",
      "there is another benefit of sparse expert models that is worth mentioning: they are more interpretable than dense models.\n",
      "interpretability—the ability for a human to understand why a model took the action that it did—is one of ai’s greatest weaknesses today. in general, today’s neural networks are uninterpretable “black boxes.” this can limit their usefulness in the real world, particularly in high-stakes settings like healthcare where human review is important.\n",
      "sparse expert models lend themselves more naturally to interpretability than conventional models because a sparse model’s output is the result of an identifiable, discrete subset of parameters within the model—namely, the “experts” that were activated. the fact that humans can better extract understandable explanations from sparse models about their behavior may prove to be a decisive advantage for these models in real-world applications.\n",
      "sparse expert models are not in widespread use today. they are less well understood and more technically complex to build than dense models. yet considering their potential advantages, most of all their computational efficiency, don’t be surprised to see the sparse expert architecture become more prevalent in the world of llms going forward.\n",
      "in the words of graphcore cto simon knowles: “if an ai can do many things, it doesn’t need to access all of its knowledge to do one thing. it’s completely obvious. this is how your brain works, and it’s also how an ai ought to work. i’d be surprised if, by next year, anyone is building dense language models.”\n",
      "note: the author is a partner at radical ventures, which is an investor in you.com.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = (\"https://www.forbes.com/sites/robtoews/2023/02/07/the-next-generation-of-large-language-models/?fbclid=IwAR3HM165sf71CJS_RSztDi2D4hQSHvUi93zoGsEW87PqOwhcHTGw3FwsciQ&sh=70cfbfda18db\")\n",
    "html = urllib.request.urlopen(url)\n",
    "htmlParse = BeautifulSoup(html,'html.parser')\n",
    "for para in htmlParse.find_all(\"p\"):\n",
    "    print(para.get_text().lower)\n",
    "paragraphs = []\n",
    "for para in htmlParse.find_all(\"p\"):\n",
    "    paragraphs.append(para.get_text())\n",
    "full_text = \"\\n\".join(paragraphs)\n",
    "full_text1 = full_text.lower()\n",
    "print(full_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f60fdb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56bf29ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# TASK 4 \n",
    "import string \n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e0b689",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai ceo sam altman left and meta ai chief yann lecun right have differing views on the future   of large language models\n",
      "in case you havent heard artificial intelligence is the hot new thing\n",
      "generative ai seems to be on the lips of every venture capitalist entrepreneur fortune 500 ceo and journalist these days from silicon valley to davos\n",
      "to those who started paying real attention to ai in 2022 it may seem that technologies like chatgpt and stable diffusion came out of nowhere to take the world by storm they didnt\n",
      "back in 2020 we wrote an article in this column predicting that generative ai would be one of the pillars of the next generation of artificial intelligence\n",
      "since at least the release of gpt2 in 2019 it has been clear to those working in the field that generative language models were poised to unleash vast economic and societal transformation similarly while texttoimage models only captured the publics attention last summer the technologys ascendance has appeared inevitable since openai released the original dalle in january 2021 we wrote an article making this argument days after the release of the original dalle\n",
      "by this same token it is important to remember that the current state of the art in ai is far from an end state for ais capabilities on the contrary the frontiers of artificial intelligence have never advanced more rapidly than they are right now as amazing as chatgpt seems to us at the moment it is a mere stepping stone to what comes next\n",
      "what will the next generation of large language models llms look like the answer to this question is already out there under development at ai startups and research groups at this very moment\n",
      "this article highlights three emerging areas that will help define the next wave of innovation in generative ai and llms for those looking to remain ahead of the curve in this fastchanging worldread on\n",
      "\n",
      "\n",
      "consider how humans think and learn we collect knowledge and perspective from external sources of informationsay by reading a book but we also generate novel ideas and insights on our own by reflecting on a topic or thinking through a problem in our minds we are able to deepen our understanding of the world via internal reflection and analysis not directly tied to any new external input\n",
      "a new avenue of ai research seeks to enable large language models to do something analogous effectively bootstrapping their own intelligence\n",
      "as part of their training todays llms ingest much of the worlds accumulated written information eg wikipedia books news articles what if these models once trained could use all the knowledge that they have absorbed from these sources to produce new written contentand then use that content as additional training data in order to improve themselves initial work suggests that this approach may be possibleand powerful\n",
      "in one recent research effort aptly titled large language models can selfimprove a group of google researchers built an llm that can come up with a set of questions generate detailed answers to those questions filter its own answers for the most highquality output and then finetune itself on the curated answers remarkably this leads to new stateoftheart performance on various language tasks for instance the models performance increased from 742 to 821 on gsm8k and from 782 to 830 on drop two popular benchmarks used to evaluate llm performance\n",
      "another recent work builds on an important llm method called instruction finetuning which lies at the core of products like chatgpt whereas chatgpt and other instruction finetuned models rely on humanwritten instructions this research group built a model that can generate its own natural language instructions and then finetune itself on those instructions the performance gains are dramatic this method improves the performance of the base gpt3 model by 33 nearly matching the performance of openais own instructiontuned model\n",
      "in a thematically related work researchers from google and carnegie mellon show that if a large language model when presented with a question first recites to itself what it knows about the topic before responding it provides more accurate and sophisticated responses this can be loosely analogized to a human in conversation who rather than blurting out the first thing that comes to mind on a topic searches her memory and reflects on her beliefs before sharing a perspective\n",
      "when people first hear about this line of research a conceptual objection often arisesisnt this all circular how can a model produce data that the model can then consume to improve itself if the new data came from the model in the first place shouldnt the knowledge or signal that it contains already be incorporated in the model\n",
      "this objection makes sense if we conceive of large language models as databases storing information from their training data and reproducing it in different combinations when prompted butuncomfortable or even eerie as it may soundwe are better off instead conceiving of large language models along the lines of the human brain no the analogy is of course not perfect\n",
      "we humans ingest a tremendous amount of data from the world that alters the neural connections in our brains in imponderable innumerable ways through introspection writing conversationsometimes just a good nights sleepour brains can then produce new insights that had not previously been in our minds nor in any information source out in the world if we internalize these new insights they can make us wiser\n",
      "the idea that llms can generate their own training data is particularly important in light of the fact that the world may soon run out of text training data this is not yet a widely appreciated problem but it is one that many ai researchers are worried about\n",
      "by one estimate the worlds total stock of usable text data is between 46 trillion and 172 trillion tokens this includes all the worlds books all scientific papers all news articles all of wikipedia all publicly available code and much of the rest of the internet filtered for quality eg webpages blogs social media another recent estimate puts the total figure at 32 trillion tokens\n",
      "deepminds chinchilla one of todays leading llms was trained on 14 trillion tokens\n",
      "in other words we may be well within one order of magnitude of exhausting the worlds entire supply of useful language training data\n",
      "if large language models are able to generate their own training data and use it to continue selfimproving this could render irrelevant the looming data shortage it would represent a mindbending leap forward for llms\n",
      "\n",
      "a popular narrative these days is that chatgpt and conversational llms like it are on the verge of replacing google search as the worlds goto source for information disrupting the oncemighty tech giant like blockbuster or kodak were disrupted before it\n",
      "this narrative badly oversimplifies things llms as they exist today will never replace google search why not in short because todays llms make stuff up\n",
      "as powerful as they are large language models regularly produce inaccurate misleading or false information and present it confidently and convincingly\n",
      "examples abound of chatgpts hallucinations as these misstatements are referred to this is not to single out chatgpt every generative language model in existence today hallucinates in similar ways\n",
      "to give a few examples it recommends books that dont exist it insists that the number 220 is less than 200 it is unsure whether abraham lincolns assassin was on the same continent as lincoln at the time of the assassination it provides plausiblesounding but incorrect explanations of concepts like bayes theorem\n",
      "most users will not accept a search engine that gets basic facts like these wrong some of the time even 99 accuracy will not be good enough for broad market adoption openai ceo sam altman himself acknowledges this recently cautioning chatgpt is incredibly limited but good enough at some things to create a misleading impression of greatness its a mistake to be relying on it for anything important right now\n",
      "it is an open question whether llms hallucination problem can be solved via incremental improvements to existing architectures or whether a more fundamental paradigm shift in ai methodologies will be necessary to give ai common sense and real understanding deep learning pioneer yann lecun for one believes the latter lecuns contrarian perspective may prove correct time will tell\n",
      "in the nearer term though a set of promising innovations offers to at least mitigate llms factual unreliability these new methods will play an essential role in preparing llms for widespread realworld deployment\n",
      "two related capabilities lie at the heart of current efforts to make language models more accurate 1 the ability for llms to retrieve information from external sources and 2 the ability for llms to provide references and citations for the information they provide\n",
      "chatgpt is limited to the information that is already stored inside of it captured in its static weights this is why it is not able to discuss events that occurred after 2021 when the model was trained being able to pull in information from external sources will empower llms to access the most accurate and uptodate information available even when that information changes frequently say companies stock prices\n",
      "of course having access to an external information source does not by itself guarantee that llms will retrieve the most accurate and relevant information one important way for llms to increase transparency and trust with human users is to include references to the sources from which they retrieved the information such citations allow human users to audit the information source as needed in order to decide for themselves on its reliability\n",
      "important early work in this field includes models like realm from google and rag from facebook both published in 2020 with the rise of conversational llms in recent months research in this area is now rapidly accelerating\n",
      "last year openai published a finetuned version of its gpt model named webgpt that can browse the internet using microsoft bing in order to provide more accurate and indepth responses to prompts webgpt navigates the internet much like a human does it can submit search queries to bing follow links scroll up and down on webpages and use functions like ctrlf to find terms when the model finds relevant information on the internet that it incorporates into its output it provides citations so that the human user can see where the information came from\n",
      "the results are encouraging for the same query webgpts responses are preferred to responses written by human subjects 56 of the time and are preferred to the highestrated responses on reddit 69 of the time\n",
      "deepmind is also pursuing research along these lines a few months ago deepmind published a new model named sparrow like chatgpt sparrow is dialoguebased like webgpt it can search the internet for information and provide citations for its assertions sparrow builds on important earlier work out of deepmind including spalm retro and gophercite\n",
      "deepminds sparrow model in action as shown here sparrow provides quotations and links to support   its statements increasing their accuracy and trustworthiness\n",
      "the deepmind researchers find that sparrows citations are helpful and accurate 78 of the timesuggesting both that this research approach is promising and that the problem of llm inaccuracy is far from solved\n",
      "younger startups including youcom and perplexity have also recently launched llmpowered conversational search interfaces with the ability to retrieve information from external sources and cite references these products are available for public use today\n",
      "llms greatest shortcoming is their unreliability their stubborn tendency to confidently provide inaccurate information language models promise to reshape every sector of our economy but they will never reach their full potential until this problem is addressed expect to see plenty of activity and innovation in this area in the months ahead\n",
      "\n",
      "todays most prominent large language models all have effectively the same architecture\n",
      "meta ai chief yann lecun said recently in terms of underlying techniques chatgpt is not particularly innovative its nothing revolutionary although thats the way its perceived in the public its just that you know its well put together its nicely done\n",
      "lecuns statement stirred up plenty of controversy and twitter debate but the simple fact is that he is correct as no serious ai researcher would dispute\n",
      "all of todays wellknown language modelseg gpt3 from openai palm or lamda from google galactica or opt from meta megatronturing from nvidiamicrosoft jurassic1 from ai21 labsare built in the same basic way they are autoregressive selfsupervised pretrained densely activated transformerbased models\n",
      "to be sure variations among these models exist their size parameter count the data they are trained on the optimization algorithm used the batch size the number of hidden layers whether they are instruction finetuned and so on these variations can translate to meaningful performance differences the core architectures though vary little\n",
      "yet momentum is building behind an intriguingly different architectural approach to language models known as sparse expert models while the idea has been around for decades it has only recently reemerged and begun to gain in popularity\n",
      "all of the models mentioned above are dense this means that every time the model runs every single one of its parameters is used every time you submit a prompt to gpt3 for instance all 175 billion of the models parameters are activated in order to produce its response\n",
      "but what if a model were able to call upon only the most relevant subset of its parameters in order to respond to a given query this is the basic concept behind sparse expert models\n",
      "the defining characteristic of sparse models is that they dont activate all of their parameters for a given input but rather only those parameters that are helpful in order to handle the input model sparsity thus decouples a models total parameter count from its compute requirements this leads to sparse expert models key advantage they can be both larger and less computationally demanding than dense models\n",
      "why are they called sparse expert models because sparse models can be thought of as consisting of a collection of submodels that serve as experts on different topics depending on the prompt presented to the model the most relevant experts within the model are activated while the other experts remain inactive a prompt posed in russian for instance would only activate the experts within a model that can understand and respond in russian efficiently bypassing the rest of the model\n",
      "all of todays largest llms are sparse if you come across an llm with more than 1 trillion parameters you can safely assume that it is sparse this includes googles switch transformer 16 trillion parameters googles glam 12 trillion parameters and metas mixture of experts model 11 trillion parameters\n",
      "much of the recent progress in ai has come from training larger and larger models said mikel artetxe who led metas research on sparse models before resigning to cofound a stealth llm startup gpt3 for instance is more than 100 times larger than gpt2 but when we double the size of a dense model we also make it twice as slow sparse models allow us to train larger models without the increase in runtime\n",
      "recent research on sparse expert models suggests that this architecture holds massive potential\n",
      "glam a sparse expert model developed last year by google is 7 times larger than gpt3 requires twothirds less energy to train requires half as much compute for inference and outperforms gpt3 on a wide range of natural language tasks similar work on sparse models out of meta has yielded similarly promising results\n",
      "as the meta researchers summarize we find that sparse models can achieve similar downstream task performance as dense models at a fraction of the compute for models with relatively modest compute budgets a sparse model can perform on par with a dense model that requires almost four times as much compute\n",
      "there is another benefit of sparse expert models that is worth mentioning they are more interpretable than dense models\n",
      "interpretabilitythe ability for a human to understand why a model took the action that it didis one of ais greatest weaknesses today in general todays neural networks are uninterpretable black boxes this can limit their usefulness in the real world particularly in highstakes settings like healthcare where human review is important\n",
      "sparse expert models lend themselves more naturally to interpretability than conventional models because a sparse models output is the result of an identifiable discrete subset of parameters within the modelnamely the experts that were activated the fact that humans can better extract understandable explanations from sparse models about their behavior may prove to be a decisive advantage for these models in realworld applications\n",
      "sparse expert models are not in widespread use today they are less well understood and more technically complex to build than dense models yet considering their potential advantages most of all their computational efficiency dont be surprised to see the sparse expert architecture become more prevalent in the world of llms going forward\n",
      "in the words of graphcore cto simon knowles if an ai can do many things it doesnt need to access all of its knowledge to do one thing its completely obvious this is how your brain works and its also how an ai ought to work id be surprised if by next year anyone is building dense language models\n",
      "note the author is a partner at radical ventures which is an investor in youcom\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using regular expressions \n",
    "# 1.Make all text lower case \n",
    "import re \n",
    "op_string = re.sub(r'[^\\w\\s]','',full_text1)\n",
    "print(op_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b37bb4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai ceo sam altman left and meta ai chief yann lecun right have differing views on the future   of large language models\n",
      "in case you havent heard artificial intelligence is the hot new thing\n",
      "generative ai seems to be on the lips of every venture capitalist entrepreneur fortune 500 ceo and journalist these days from silicon valley to davos\n",
      "to those who started paying real attention to ai in 2022 it may seem that technologies like chatgpt and stable diffusion came out of nowhere to take the world by storm they didnt\n",
      "back in 2020 we wrote an article in this column predicting that generative ai would be one of the pillars of the next generation of artificial intelligence\n",
      "since at least the release of gpt2 in 2019 it has been clear to those working in the field that generative language models were poised to unleash vast economic and societal transformation similarly while texttoimage models only captured the publics attention last summer the technologys ascendance has appeared inevitable since openai released the original dalle in january 2021 we wrote an article making this argument days after the release of the original dalle\n",
      "by this same token it is important to remember that the current state of the art in ai is far from an end state for ais capabilities on the contrary the frontiers of artificial intelligence have never advanced more rapidly than they are right now as amazing as chatgpt seems to us at the moment it is a mere stepping stone to what comes next\n",
      "what will the next generation of large language models llms look like the answer to this question is already out there under development at ai startups and research groups at this very moment\n",
      "this article highlights three emerging areas that will help define the next wave of innovation in generative ai and llms for those looking to remain ahead of the curve in this fastchanging worldread on\n",
      "\n",
      "\n",
      "consider how humans think and learn we collect knowledge and perspective from external sources of informationsay by reading a book but we also generate novel ideas and insights on our own by reflecting on a topic or thinking through a problem in our minds we are able to deepen our understanding of the world via internal reflection and analysis not directly tied to any new external input\n",
      "a new avenue of ai research seeks to enable large language models to do something analogous effectively bootstrapping their own intelligence\n",
      "as part of their training todays llms ingest much of the worlds accumulated written information eg wikipedia books news articles what if these models once trained could use all the knowledge that they have absorbed from these sources to produce new written contentand then use that content as additional training data in order to improve themselves initial work suggests that this approach may be possibleand powerful\n",
      "in one recent research effort aptly titled large language models can selfimprove a group of google researchers built an llm that can come up with a set of questions generate detailed answers to those questions filter its own answers for the most highquality output and then finetune itself on the curated answers remarkably this leads to new stateoftheart performance on various language tasks for instance the models performance increased from 742 to 821 on gsm8k and from 782 to 830 on drop two popular benchmarks used to evaluate llm performance\n",
      "another recent work builds on an important llm method called instruction finetuning which lies at the core of products like chatgpt whereas chatgpt and other instruction finetuned models rely on humanwritten instructions this research group built a model that can generate its own natural language instructions and then finetune itself on those instructions the performance gains are dramatic this method improves the performance of the base gpt3 model by 33 nearly matching the performance of openais own instructiontuned model\n",
      "in a thematically related work researchers from google and carnegie mellon show that if a large language model when presented with a question first recites to itself what it knows about the topic before responding it provides more accurate and sophisticated responses this can be loosely analogized to a human in conversation who rather than blurting out the first thing that comes to mind on a topic searches her memory and reflects on her beliefs before sharing a perspective\n",
      "when people first hear about this line of research a conceptual objection often arisesisnt this all circular how can a model produce data that the model can then consume to improve itself if the new data came from the model in the first place shouldnt the knowledge or signal that it contains already be incorporated in the model\n",
      "this objection makes sense if we conceive of large language models as databases storing information from their training data and reproducing it in different combinations when prompted butuncomfortable or even eerie as it may soundwe are better off instead conceiving of large language models along the lines of the human brain no the analogy is of course not perfect\n",
      "we humans ingest a tremendous amount of data from the world that alters the neural connections in our brains in imponderable innumerable ways through introspection writing conversationsometimes just a good nights sleepour brains can then produce new insights that had not previously been in our minds nor in any information source out in the world if we internalize these new insights they can make us wiser\n",
      "the idea that llms can generate their own training data is particularly important in light of the fact that the world may soon run out of text training data this is not yet a widely appreciated problem but it is one that many ai researchers are worried about\n",
      "by one estimate the worlds total stock of usable text data is between 46 trillion and 172 trillion tokens this includes all the worlds books all scientific papers all news articles all of wikipedia all publicly available code and much of the rest of the internet filtered for quality eg webpages blogs social media another recent estimate puts the total figure at 32 trillion tokens\n",
      "deepminds chinchilla one of todays leading llms was trained on 14 trillion tokens\n",
      "in other words we may be well within one order of magnitude of exhausting the worlds entire supply of useful language training data\n",
      "if large language models are able to generate their own training data and use it to continue selfimproving this could render irrelevant the looming data shortage it would represent a mindbending leap forward for llms\n",
      "\n",
      "a popular narrative these days is that chatgpt and conversational llms like it are on the verge of replacing google search as the worlds goto source for information disrupting the oncemighty tech giant like blockbuster or kodak were disrupted before it\n",
      "this narrative badly oversimplifies things llms as they exist today will never replace google search why not in short because todays llms make stuff up\n",
      "as powerful as they are large language models regularly produce inaccurate misleading or false information and present it confidently and convincingly\n",
      "examples abound of chatgpts hallucinations as these misstatements are referred to this is not to single out chatgpt every generative language model in existence today hallucinates in similar ways\n",
      "to give a few examples it recommends books that dont exist it insists that the number 220 is less than 200 it is unsure whether abraham lincolns assassin was on the same continent as lincoln at the time of the assassination it provides plausiblesounding but incorrect explanations of concepts like bayes theorem\n",
      "most users will not accept a search engine that gets basic facts like these wrong some of the time even 99 accuracy will not be good enough for broad market adoption openai ceo sam altman himself acknowledges this recently cautioning chatgpt is incredibly limited but good enough at some things to create a misleading impression of greatness its a mistake to be relying on it for anything important right now\n",
      "it is an open question whether llms hallucination problem can be solved via incremental improvements to existing architectures or whether a more fundamental paradigm shift in ai methodologies will be necessary to give ai common sense and real understanding deep learning pioneer yann lecun for one believes the latter lecuns contrarian perspective may prove correct time will tell\n",
      "in the nearer term though a set of promising innovations offers to at least mitigate llms factual unreliability these new methods will play an essential role in preparing llms for widespread realworld deployment\n",
      "two related capabilities lie at the heart of current efforts to make language models more accurate 1 the ability for llms to retrieve information from external sources and 2 the ability for llms to provide references and citations for the information they provide\n",
      "chatgpt is limited to the information that is already stored inside of it captured in its static weights this is why it is not able to discuss events that occurred after 2021 when the model was trained being able to pull in information from external sources will empower llms to access the most accurate and uptodate information available even when that information changes frequently say companies stock prices\n",
      "of course having access to an external information source does not by itself guarantee that llms will retrieve the most accurate and relevant information one important way for llms to increase transparency and trust with human users is to include references to the sources from which they retrieved the information such citations allow human users to audit the information source as needed in order to decide for themselves on its reliability\n",
      "important early work in this field includes models like realm from google and rag from facebook both published in 2020 with the rise of conversational llms in recent months research in this area is now rapidly accelerating\n",
      "last year openai published a finetuned version of its gpt model named webgpt that can browse the internet using microsoft bing in order to provide more accurate and indepth responses to prompts webgpt navigates the internet much like a human does it can submit search queries to bing follow links scroll up and down on webpages and use functions like ctrlf to find terms when the model finds relevant information on the internet that it incorporates into its output it provides citations so that the human user can see where the information came from\n",
      "the results are encouraging for the same query webgpts responses are preferred to responses written by human subjects 56 of the time and are preferred to the highestrated responses on reddit 69 of the time\n",
      "deepmind is also pursuing research along these lines a few months ago deepmind published a new model named sparrow like chatgpt sparrow is dialoguebased like webgpt it can search the internet for information and provide citations for its assertions sparrow builds on important earlier work out of deepmind including spalm retro and gophercite\n",
      "deepminds sparrow model in action as shown here sparrow provides quotations and links to support   its statements increasing their accuracy and trustworthiness\n",
      "the deepmind researchers find that sparrows citations are helpful and accurate 78 of the timesuggesting both that this research approach is promising and that the problem of llm inaccuracy is far from solved\n",
      "younger startups including youcom and perplexity have also recently launched llmpowered conversational search interfaces with the ability to retrieve information from external sources and cite references these products are available for public use today\n",
      "llms greatest shortcoming is their unreliability their stubborn tendency to confidently provide inaccurate information language models promise to reshape every sector of our economy but they will never reach their full potential until this problem is addressed expect to see plenty of activity and innovation in this area in the months ahead\n",
      "\n",
      "todays most prominent large language models all have effectively the same architecture\n",
      "meta ai chief yann lecun said recently in terms of underlying techniques chatgpt is not particularly innovative its nothing revolutionary although thats the way its perceived in the public its just that you know its well put together its nicely done\n",
      "lecuns statement stirred up plenty of controversy and twitter debate but the simple fact is that he is correct as no serious ai researcher would dispute\n",
      "all of todays wellknown language modelseg gpt3 from openai palm or lamda from google galactica or opt from meta megatronturing from nvidiamicrosoft jurassic1 from ai21 labsare built in the same basic way they are autoregressive selfsupervised pretrained densely activated transformerbased models\n",
      "to be sure variations among these models exist their size parameter count the data they are trained on the optimization algorithm used the batch size the number of hidden layers whether they are instruction finetuned and so on these variations can translate to meaningful performance differences the core architectures though vary little\n",
      "yet momentum is building behind an intriguingly different architectural approach to language models known as sparse expert models while the idea has been around for decades it has only recently reemerged and begun to gain in popularity\n",
      "all of the models mentioned above are dense this means that every time the model runs every single one of its parameters is used every time you submit a prompt to gpt3 for instance all 175 billion of the models parameters are activated in order to produce its response\n",
      "but what if a model were able to call upon only the most relevant subset of its parameters in order to respond to a given query this is the basic concept behind sparse expert models\n",
      "the defining characteristic of sparse models is that they dont activate all of their parameters for a given input but rather only those parameters that are helpful in order to handle the input model sparsity thus decouples a models total parameter count from its compute requirements this leads to sparse expert models key advantage they can be both larger and less computationally demanding than dense models\n",
      "why are they called sparse expert models because sparse models can be thought of as consisting of a collection of submodels that serve as experts on different topics depending on the prompt presented to the model the most relevant experts within the model are activated while the other experts remain inactive a prompt posed in russian for instance would only activate the experts within a model that can understand and respond in russian efficiently bypassing the rest of the model\n",
      "all of todays largest llms are sparse if you come across an llm with more than 1 trillion parameters you can safely assume that it is sparse this includes googles switch transformer 16 trillion parameters googles glam 12 trillion parameters and metas mixture of experts model 11 trillion parameters\n",
      "much of the recent progress in ai has come from training larger and larger models said mikel artetxe who led metas research on sparse models before resigning to cofound a stealth llm startup gpt3 for instance is more than 100 times larger than gpt2 but when we double the size of a dense model we also make it twice as slow sparse models allow us to train larger models without the increase in runtime\n",
      "recent research on sparse expert models suggests that this architecture holds massive potential\n",
      "glam a sparse expert model developed last year by google is 7 times larger than gpt3 requires twothirds less energy to train requires half as much compute for inference and outperforms gpt3 on a wide range of natural language tasks similar work on sparse models out of meta has yielded similarly promising results\n",
      "as the meta researchers summarize we find that sparse models can achieve similar downstream task performance as dense models at a fraction of the compute for models with relatively modest compute budgets a sparse model can perform on par with a dense model that requires almost four times as much compute\n",
      "there is another benefit of sparse expert models that is worth mentioning they are more interpretable than dense models\n",
      "interpretabilitythe ability for a human to understand why a model took the action that it didis one of ais greatest weaknesses today in general todays neural networks are uninterpretable black boxes this can limit their usefulness in the real world particularly in highstakes settings like healthcare where human review is important\n",
      "sparse expert models lend themselves more naturally to interpretability than conventional models because a sparse models output is the result of an identifiable discrete subset of parameters within the modelnamely the experts that were activated the fact that humans can better extract understandable explanations from sparse models about their behavior may prove to be a decisive advantage for these models in realworld applications\n",
      "sparse expert models are not in widespread use today they are less well understood and more technically complex to build than dense models yet considering their potential advantages most of all their computational efficiency dont be surprised to see the sparse expert architecture become more prevalent in the world of llms going forward\n",
      "in the words of graphcore cto simon knowles if an ai can do many things it doesnt need to access all of its knowledge to do one thing its completely obvious this is how your brain works and its also how an ai ought to work id be surprised if by next year anyone is building dense language models\n",
      "note the author is a partner at radical ventures which is an investor in youcom\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation marks here\n",
    "import re\n",
    "text = full_text1\n",
    "pattern = r'[^\\w\\s]'\n",
    "no_punct_text = re.sub(pattern, '', text)\n",
    "print(no_punct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2fbf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai ceo sam altman (left) and meta ai chief yann lecun (right) have differing views on the future ... [+] of large language models.\n",
      "in case you haven’t heard, artificial intelligence is the hot new thing.\n",
      "generative ai seems to be on the lips of every venture capitalist, entrepreneur, fortune  ceo and journalist these days, from silicon valley to davos.\n",
      "to those who started paying real attention to ai in , it may seem that technologies like chatgpt and stable diffusion came out of nowhere to take the world by storm. they didn’t.\n",
      "back in , we wrote an article in this column predicting that generative ai would be one of the pillars of the next generation of artificial intelligence.\n",
      "since at least the release of gpt- in , it has been clear to those working in the field that generative language models were poised to unleash vast economic and societal transformation. similarly, while text-to-image models only captured the public’s attention last summer, the technology’s ascendance has appeared inevitable since openai released the original dall-e in january . (we wrote an article making this argument days after the release of the original dall-e.)\n",
      "by this same token, it is important to remember that the current state of the art in ai is far from an end state for ai’s capabilities. on the contrary, the frontiers of artificial intelligence have never advanced more rapidly than they are right now. as amazing as chatgpt seems to us at the moment, it is a mere stepping stone to what comes next.\n",
      "what will the next generation of large language models (llms) look like? the answer to this question is already out there, under development at ai startups and research groups at this very moment.\n",
      "this article highlights three emerging areas that will help define the next wave of innovation in generative ai and llms. for those looking to remain ahead of the curve in this fast-changing world—read on.\n",
      "\n",
      "\n",
      "consider how humans think and learn. we collect knowledge and perspective from external sources of information—say, by reading a book. but we also generate novel ideas and insights on our own, by reflecting on a topic or thinking through a problem in our minds. we are able to deepen our understanding of the world via internal reflection and analysis not directly tied to any new external input.\n",
      "a new avenue of ai research seeks to enable large language models to do something analogous, effectively bootstrapping their own intelligence.\n",
      "as part of their training, today’s llms ingest much of the world’s accumulated written information (e.g., wikipedia, books, news articles). what if these models, once trained, could use all the knowledge that they have absorbed from these sources to produce new written content—and then use that content as additional training data in order to improve themselves? initial work suggests that this approach may be possible—and powerful.\n",
      "in one recent research effort, aptly titled “large language models can self-improve,” a group of google researchers built an llm that can come up with a set of questions, generate detailed answers to those questions, filter its own answers for the most high-quality output, and then fine-tune itself on the curated answers. remarkably, this leads to new state-of-the-art performance on various language tasks. for instance, the model’s performance increased from .% to .% on gsmk and from .% to .% on drop, two popular benchmarks used to evaluate llm performance.\n",
      "another recent work builds on an important llm method called “instruction fine-tuning,” which lies at the core of products like chatgpt. whereas chatgpt and other instruction fine-tuned models rely on human-written instructions, this research group built a model that can generate its own natural language instructions and then fine-tune itself on those instructions. the performance gains are dramatic: this method improves the performance of the base gpt- model by %, nearly matching the performance of openai’s own instruction-tuned model.\n",
      "in a thematically related work, researchers from google and carnegie mellon show that if a large language model, when presented with a question, first recites to itself what it knows about the topic before responding, it provides more accurate and sophisticated responses. this can be loosely analogized to a human in conversation who, rather than blurting out the first thing that comes to mind on a topic, searches her memory and reflects on her beliefs before sharing a perspective.\n",
      "when people first hear about this line of research, a conceptual objection often arises—isn’t this all circular? how can a model produce data that the model can then consume to improve itself? if the new data came from the model in the first place, shouldn’t the “knowledge” or “signal” that it contains already be incorporated in the model?\n",
      "this objection makes sense if we conceive of large language models as databases, storing information from their training data and reproducing it in different combinations when prompted. but—uncomfortable or even eerie as it may sound—we are better off instead conceiving of large language models along the lines of the human brain (no, the analogy is of course not perfect!).\n",
      "we humans ingest a tremendous amount of data from the world that alters the neural connections in our brains in imponderable, innumerable ways. through introspection, writing, conversation—sometimes just a good night’s sleep—our brains can then produce new insights that had not previously been in our minds nor in any information source out in the world. if we internalize these new insights, they can make us wiser.\n",
      "the idea that llms can generate their own training data is particularly important in light of the fact that the world may soon run out of text training data. this is not yet a widely appreciated problem, but it is one that many ai researchers are worried about.\n",
      "by one estimate, the world’s total stock of usable text data is between . trillion and . trillion tokens. this includes all the world’s books, all scientific papers, all news articles, all of wikipedia, all publicly available code, and much of the rest of the internet, filtered for quality (e.g., webpages, blogs, social media). another recent estimate puts the total figure at . trillion tokens.\n",
      "deepmind’s chinchilla, one of today’s leading llms, was trained on . trillion tokens.\n",
      "in other words, we may be well within one order of magnitude of exhausting the world’s entire supply of useful language training data.\n",
      "if large language models are able to generate their own training data and use it to continue self-improving, this could render irrelevant the looming data shortage. it would represent a mind-bending leap forward for llms.\n",
      "\n",
      "a popular narrative these days is that chatgpt and conversational llms like it are on the verge of replacing google search as the world’s go-to source for information, disrupting the once-mighty tech giant like blockbuster or kodak were disrupted before it.\n",
      "this narrative badly oversimplifies things. llms as they exist today will never replace google search. why not? in short, because today’s llms make stuff up.\n",
      "as powerful as they are, large language models regularly produce inaccurate, misleading or false information (and present it confidently and convincingly).\n",
      "examples abound of chatgpt’s “hallucinations” (as these misstatements are referred to). this is not to single out chatgpt; every generative language model in existence today hallucinates in similar ways.\n",
      "to give a few examples: it recommends books that don’t exist; it insists that the number  is less than ; it is unsure whether abraham lincoln’s assassin was on the same continent as lincoln at the time of the assassination; it provides plausible-sounding but incorrect explanations of concepts like bayes’ theorem.\n",
      "most users will not accept a search engine that gets basic facts like these wrong some of the time; even % accuracy will not be good enough for broad market adoption. openai ceo sam altman himself acknowledges this, recently cautioning: “chatgpt is incredibly limited, but good enough at some things to create a misleading impression of greatness. it's a mistake to be relying on it for anything important right now.”\n",
      "it is an open question whether llms’ hallucination problem can be solved via incremental improvements to existing architectures, or whether a more fundamental paradigm shift in ai methodologies will be necessary to give ai common sense and real understanding. deep learning pioneer yann lecun, for one, believes the latter. lecun’s contrarian perspective may prove correct; time will tell.\n",
      "in the nearer term, though, a set of promising innovations offers to at least mitigate llms’ factual unreliability. these new methods will play an essential role in preparing llms for widespread real-world deployment.\n",
      "two related capabilities lie at the heart of current efforts to make language models more accurate: () the ability for llms to retrieve information from external sources, and () the ability for llms to provide references and citations for the information they provide.\n",
      "chatgpt is limited to the information that is already stored inside of it, captured in its static weights. (this is why it is not able to discuss events that occurred after , when the model was trained.) being able to pull in information from external sources will empower llms to access the most accurate and up-to-date information available, even when that information changes frequently (say, companies’ stock prices).\n",
      "of course, having access to an external information source does not by itself guarantee that llms will retrieve the most accurate and relevant information. one important way for llms to increase transparency and trust with human users is to include references to the source(s) from which they retrieved the information. such citations allow human users to audit the information source as needed in order to decide for themselves on its reliability.\n",
      "important early work in this field includes models like realm (from google) and rag (from facebook), both published in . with the rise of conversational llms in recent months, research in this area is now rapidly accelerating.\n",
      "last year, openai published a fine-tuned version of its gpt model named webgpt that can browse the internet using microsoft bing in order to provide more accurate and in-depth responses to prompts. webgpt navigates the internet much like a human does: it can submit search queries to bing, follow links, scroll up and down on webpages, and use functions like ctrl+f to find terms. when the model finds relevant information on the internet that it incorporates into its output, it provides citations so that the human user can see where the information came from.\n",
      "the results are encouraging: for the same query, webgpt’s responses are preferred to responses written by human subjects % of the time and are preferred to the highest-rated responses on reddit % of the time.\n",
      "deepmind is also pursuing research along these lines. a few months ago, deepmind published a new model named sparrow. like chatgpt, sparrow is dialogue-based; like webgpt, it can search the internet for information and provide citations for its assertions. sparrow builds on important earlier work out of deepmind including spalm, retro and gophercite.\n",
      "deepmind's sparrow model in action. as shown here, sparrow provides quotations and links to support ... [+] its statements, increasing their accuracy and trustworthiness.\n",
      "the deepmind researchers find that sparrow’s citations are helpful and accurate % of the time—suggesting both that this research approach is promising and that the problem of llm inaccuracy is far from solved.\n",
      "younger startups including you.com and perplexity have also recently launched llm-powered conversational search interfaces with the ability to retrieve information from external sources and cite references. these products are available for public use today.\n",
      "llms’ greatest shortcoming is their unreliability, their stubborn tendency to confidently provide inaccurate information. language models promise to reshape every sector of our economy, but they will never reach their full potential until this problem is addressed. expect to see plenty of activity and innovation in this area in the months ahead.\n",
      "\n",
      "today’s most prominent large language models all have effectively the same architecture.\n",
      "meta ai chief yann lecun said recently: “in terms of underlying techniques, chatgpt is not particularly innovative. it’s nothing revolutionary, although that’s the way it’s perceived in the public. it’s just that, you know, it’s well put together, it’s nicely done.”\n",
      "lecun’s statement stirred up plenty of controversy and twitter debate. but the simple fact is that he is correct, as no serious ai researcher would dispute.\n",
      "all of today’s well-known language models—e.g., gpt- from openai, palm or lamda from google, galactica or opt from meta, megatron-turing from nvidia/microsoft, jurassic- from ai labs—are built in the same basic way. they are autoregressive, self-supervised, pre-trained, densely activated transformer-based models.\n",
      "to be sure, variations among these models exist: their size (parameter count), the data they are trained on, the optimization algorithm used, the batch size, the number of hidden layers, whether they are instruction fine-tuned, and so on. these variations can translate to meaningful performance differences. the core architectures, though, vary little.\n",
      "yet momentum is building behind an intriguingly different architectural approach to language models known as sparse expert models. while the idea has been around for decades, it has only recently reemerged and begun to gain in popularity.\n",
      "all of the models mentioned above are dense. this means that every time the model runs, every single one of its parameters is used. every time you submit a prompt to gpt-, for instance, all  billion of the model’s parameters are activated in order to produce its response.\n",
      "but what if a model were able to call upon only the most relevant subset of its parameters in order to respond to a given query? this is the basic concept behind sparse expert models.\n",
      "the defining characteristic of sparse models is that they don’t activate all of their parameters for a given input, but rather only those parameters that are helpful in order to handle the input. model sparsity thus decouples a model’s total parameter count from its compute requirements. this leads to sparse expert models’ key advantage: they can be both larger and less computationally demanding than dense models.\n",
      "why are they called sparse expert models? because sparse models can be thought of as consisting of a collection of “sub-models” that serve as experts on different topics. depending on the prompt presented to the model, the most relevant experts within the model are activated while the other experts remain inactive. a prompt posed in russian, for instance, would only activate the “experts” within a model that can understand and respond in russian, efficiently bypassing the rest of the model.\n",
      "all of today’s largest llms are sparse. if you come across an llm with more than  trillion parameters, you can safely assume that it is sparse. this includes google’s switch transformer (. trillion parameters), google’s glam (. trillion parameters) and meta’s mixture of experts model (. trillion parameters).\n",
      "“much of the recent progress in ai has come from training larger and larger models,” said mikel artetxe, who led meta’s research on sparse models before resigning to cofound a stealth llm startup. “gpt-, for instance, is more than  times larger than gpt-. but when we double the size of a dense model, we also make it twice as slow. sparse models allow us to train larger models without the increase in runtime.”\n",
      "recent research on sparse expert models suggests that this architecture holds massive potential.\n",
      "glam, a sparse expert model developed last year by google, is  times larger than gpt-, requires two-thirds less energy to train, requires half as much compute for inference, and outperforms gpt- on a wide range of natural language tasks. similar work on sparse models out of meta has yielded similarly promising results.\n",
      "as the meta researchers summarize: “we find that sparse models can achieve similar downstream task performance as dense models at a fraction of the compute. for models with relatively modest compute budgets, a sparse model can perform on par with a dense model that requires almost four times as much compute.”\n",
      "there is another benefit of sparse expert models that is worth mentioning: they are more interpretable than dense models.\n",
      "interpretability—the ability for a human to understand why a model took the action that it did—is one of ai’s greatest weaknesses today. in general, today’s neural networks are uninterpretable “black boxes.” this can limit their usefulness in the real world, particularly in high-stakes settings like healthcare where human review is important.\n",
      "sparse expert models lend themselves more naturally to interpretability than conventional models because a sparse model’s output is the result of an identifiable, discrete subset of parameters within the model—namely, the “experts” that were activated. the fact that humans can better extract understandable explanations from sparse models about their behavior may prove to be a decisive advantage for these models in real-world applications.\n",
      "sparse expert models are not in widespread use today. they are less well understood and more technically complex to build than dense models. yet considering their potential advantages, most of all their computational efficiency, don’t be surprised to see the sparse expert architecture become more prevalent in the world of llms going forward.\n",
      "in the words of graphcore cto simon knowles: “if an ai can do many things, it doesn’t need to access all of its knowledge to do one thing. it’s completely obvious. this is how your brain works, and it’s also how an ai ought to work. i’d be surprised if, by next year, anyone is building dense language models.”\n",
      "note: the author is a partner at radical ventures, which is an investor in you.com.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using regular expressions \n",
    "# 3.Remove numerical values\n",
    "import re \n",
    "op_string = re.sub(r'[\\d]','',full_text1)\n",
    "print(op_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1662406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai ceo sam altman (left) and meta ai chief yann lecun (right) have differing views on the future ... [+] of large language models.in case you haven’t heard, artificial intelligence is the hot new thing.generative ai seems to be on the lips of every venture capitalist, entrepreneur, fortune 500 ceo and journalist these days, from silicon valley to davos.to those who started paying real attention to ai in 2022, it may seem that technologies like chatgpt and stable diffusion came out of nowhere to take the world by storm. they didn’t.back in 2020, we wrote an article in this column predicting that generative ai would be one of the pillars of the next generation of artificial intelligence.since at least the release of gpt-2 in 2019, it has been clear to those working in the field that generative language models were poised to unleash vast economic and societal transformation. similarly, while text-to-image models only captured the public’s attention last summer, the technology’s ascendance has appeared inevitable since openai released the original dall-e in january 2021. (we wrote an article making this argument days after the release of the original dall-e.)by this same token, it is important to remember that the current state of the art in ai is far from an end state for ai’s capabilities. on the contrary, the frontiers of artificial intelligence have never advanced more rapidly than they are right now. as amazing as chatgpt seems to us at the moment, it is a mere stepping stone to what comes next.what will the next generation of large language models (llms) look like? the answer to this question is already out there, under development at ai startups and research groups at this very moment.this article highlights three emerging areas that will help define the next wave of innovation in generative ai and llms. for those looking to remain ahead of the curve in this fast-changing world—read on.consider how humans think and learn. we collect knowledge and perspective from external sources of information—say, by reading a book. but we also generate novel ideas and insights on our own, by reflecting on a topic or thinking through a problem in our minds. we are able to deepen our understanding of the world via internal reflection and analysis not directly tied to any new external input.a new avenue of ai research seeks to enable large language models to do something analogous, effectively bootstrapping their own intelligence.as part of their training, today’s llms ingest much of the world’s accumulated written information (e.g., wikipedia, books, news articles). what if these models, once trained, could use all the knowledge that they have absorbed from these sources to produce new written content—and then use that content as additional training data in order to improve themselves? initial work suggests that this approach may be possible—and powerful.in one recent research effort, aptly titled “large language models can self-improve,” a group of google researchers built an llm that can come up with a set of questions, generate detailed answers to those questions, filter its own answers for the most high-quality output, and then fine-tune itself on the curated answers. remarkably, this leads to new state-of-the-art performance on various language tasks. for instance, the model’s performance increased from 74.2% to 82.1% on gsm8k and from 78.2% to 83.0% on drop, two popular benchmarks used to evaluate llm performance.another recent work builds on an important llm method called “instruction fine-tuning,” which lies at the core of products like chatgpt. whereas chatgpt and other instruction fine-tuned models rely on human-written instructions, this research group built a model that can generate its own natural language instructions and then fine-tune itself on those instructions. the performance gains are dramatic: this method improves the performance of the base gpt-3 model by 33%, nearly matching the performance of openai’s own instruction-tuned model.in a thematically related work, researchers from google and carnegie mellon show that if a large language model, when presented with a question, first recites to itself what it knows about the topic before responding, it provides more accurate and sophisticated responses. this can be loosely analogized to a human in conversation who, rather than blurting out the first thing that comes to mind on a topic, searches her memory and reflects on her beliefs before sharing a perspective.when people first hear about this line of research, a conceptual objection often arises—isn’t this all circular? how can a model produce data that the model can then consume to improve itself? if the new data came from the model in the first place, shouldn’t the “knowledge” or “signal” that it contains already be incorporated in the model?this objection makes sense if we conceive of large language models as databases, storing information from their training data and reproducing it in different combinations when prompted. but—uncomfortable or even eerie as it may sound—we are better off instead conceiving of large language models along the lines of the human brain (no, the analogy is of course not perfect!).we humans ingest a tremendous amount of data from the world that alters the neural connections in our brains in imponderable, innumerable ways. through introspection, writing, conversation—sometimes just a good night’s sleep—our brains can then produce new insights that had not previously been in our minds nor in any information source out in the world. if we internalize these new insights, they can make us wiser.the idea that llms can generate their own training data is particularly important in light of the fact that the world may soon run out of text training data. this is not yet a widely appreciated problem, but it is one that many ai researchers are worried about.by one estimate, the world’s total stock of usable text data is between 4.6 trillion and 17.2 trillion tokens. this includes all the world’s books, all scientific papers, all news articles, all of wikipedia, all publicly available code, and much of the rest of the internet, filtered for quality (e.g., webpages, blogs, social media). another recent estimate puts the total figure at 3.2 trillion tokens.deepmind’s chinchilla, one of today’s leading llms, was trained on 1.4 trillion tokens.in other words, we may be well within one order of magnitude of exhausting the world’s entire supply of useful language training data.if large language models are able to generate their own training data and use it to continue self-improving, this could render irrelevant the looming data shortage. it would represent a mind-bending leap forward for llms.a popular narrative these days is that chatgpt and conversational llms like it are on the verge of replacing google search as the world’s go-to source for information, disrupting the once-mighty tech giant like blockbuster or kodak were disrupted before it.this narrative badly oversimplifies things. llms as they exist today will never replace google search. why not? in short, because today’s llms make stuff up.as powerful as they are, large language models regularly produce inaccurate, misleading or false information (and present it confidently and convincingly).examples abound of chatgpt’s “hallucinations” (as these misstatements are referred to). this is not to single out chatgpt; every generative language model in existence today hallucinates in similar ways.to give a few examples: it recommends books that don’t exist; it insists that the number 220 is less than 200; it is unsure whether abraham lincoln’s assassin was on the same continent as lincoln at the time of the assassination; it provides plausible-sounding but incorrect explanations of concepts like bayes’ theorem.most users will not accept a search engine that gets basic facts like these wrong some of the time; even 99% accuracy will not be good enough for broad market adoption. openai ceo sam altman himself acknowledges this, recently cautioning: “chatgpt is incredibly limited, but good enough at some things to create a misleading impression of greatness. it's a mistake to be relying on it for anything important right now.”it is an open question whether llms’ hallucination problem can be solved via incremental improvements to existing architectures, or whether a more fundamental paradigm shift in ai methodologies will be necessary to give ai common sense and real understanding. deep learning pioneer yann lecun, for one, believes the latter. lecun’s contrarian perspective may prove correct; time will tell.in the nearer term, though, a set of promising innovations offers to at least mitigate llms’ factual unreliability. these new methods will play an essential role in preparing llms for widespread real-world deployment.two related capabilities lie at the heart of current efforts to make language models more accurate: (1) the ability for llms to retrieve information from external sources, and (2) the ability for llms to provide references and citations for the information they provide.chatgpt is limited to the information that is already stored inside of it, captured in its static weights. (this is why it is not able to discuss events that occurred after 2021, when the model was trained.) being able to pull in information from external sources will empower llms to access the most accurate and up-to-date information available, even when that information changes frequently (say, companies’ stock prices).of course, having access to an external information source does not by itself guarantee that llms will retrieve the most accurate and relevant information. one important way for llms to increase transparency and trust with human users is to include references to the source(s) from which they retrieved the information. such citations allow human users to audit the information source as needed in order to decide for themselves on its reliability.important early work in this field includes models like realm (from google) and rag (from facebook), both published in 2020. with the rise of conversational llms in recent months, research in this area is now rapidly accelerating.last year, openai published a fine-tuned version of its gpt model named webgpt that can browse the internet using microsoft bing in order to provide more accurate and in-depth responses to prompts. webgpt navigates the internet much like a human does: it can submit search queries to bing, follow links, scroll up and down on webpages, and use functions like ctrl+f to find terms. when the model finds relevant information on the internet that it incorporates into its output, it provides citations so that the human user can see where the information came from.the results are encouraging: for the same query, webgpt’s responses are preferred to responses written by human subjects 56% of the time and are preferred to the highest-rated responses on reddit 69% of the time.deepmind is also pursuing research along these lines. a few months ago, deepmind published a new model named sparrow. like chatgpt, sparrow is dialogue-based; like webgpt, it can search the internet for information and provide citations for its assertions. sparrow builds on important earlier work out of deepmind including spalm, retro and gophercite.deepmind's sparrow model in action. as shown here, sparrow provides quotations and links to support ... [+] its statements, increasing their accuracy and trustworthiness.the deepmind researchers find that sparrow’s citations are helpful and accurate 78% of the time—suggesting both that this research approach is promising and that the problem of llm inaccuracy is far from solved.younger startups including you.com and perplexity have also recently launched llm-powered conversational search interfaces with the ability to retrieve information from external sources and cite references. these products are available for public use today.llms’ greatest shortcoming is their unreliability, their stubborn tendency to confidently provide inaccurate information. language models promise to reshape every sector of our economy, but they will never reach their full potential until this problem is addressed. expect to see plenty of activity and innovation in this area in the months ahead.today’s most prominent large language models all have effectively the same architecture.meta ai chief yann lecun said recently: “in terms of underlying techniques, chatgpt is not particularly innovative. it’s nothing revolutionary, although that’s the way it’s perceived in the public. it’s just that, you know, it’s well put together, it’s nicely done.”lecun’s statement stirred up plenty of controversy and twitter debate. but the simple fact is that he is correct, as no serious ai researcher would dispute.all of today’s well-known language models—e.g., gpt-3 from openai, palm or lamda from google, galactica or opt from meta, megatron-turing from nvidia/microsoft, jurassic-1 from ai21 labs—are built in the same basic way. they are autoregressive, self-supervised, pre-trained, densely activated transformer-based models.to be sure, variations among these models exist: their size (parameter count), the data they are trained on, the optimization algorithm used, the batch size, the number of hidden layers, whether they are instruction fine-tuned, and so on. these variations can translate to meaningful performance differences. the core architectures, though, vary little.yet momentum is building behind an intriguingly different architectural approach to language models known as sparse expert models. while the idea has been around for decades, it has only recently reemerged and begun to gain in popularity.all of the models mentioned above are dense. this means that every time the model runs, every single one of its parameters is used. every time you submit a prompt to gpt-3, for instance, all 175 billion of the model’s parameters are activated in order to produce its response.but what if a model were able to call upon only the most relevant subset of its parameters in order to respond to a given query? this is the basic concept behind sparse expert models.the defining characteristic of sparse models is that they don’t activate all of their parameters for a given input, but rather only those parameters that are helpful in order to handle the input. model sparsity thus decouples a model’s total parameter count from its compute requirements. this leads to sparse expert models’ key advantage: they can be both larger and less computationally demanding than dense models.why are they called sparse expert models? because sparse models can be thought of as consisting of a collection of “sub-models” that serve as experts on different topics. depending on the prompt presented to the model, the most relevant experts within the model are activated while the other experts remain inactive. a prompt posed in russian, for instance, would only activate the “experts” within a model that can understand and respond in russian, efficiently bypassing the rest of the model.all of today’s largest llms are sparse. if you come across an llm with more than 1 trillion parameters, you can safely assume that it is sparse. this includes google’s switch transformer (1.6 trillion parameters), google’s glam (1.2 trillion parameters) and meta’s mixture of experts model (1.1 trillion parameters).“much of the recent progress in ai has come from training larger and larger models,” said mikel artetxe, who led meta’s research on sparse models before resigning to cofound a stealth llm startup. “gpt-3, for instance, is more than 100 times larger than gpt-2. but when we double the size of a dense model, we also make it twice as slow. sparse models allow us to train larger models without the increase in runtime.”recent research on sparse expert models suggests that this architecture holds massive potential.glam, a sparse expert model developed last year by google, is 7 times larger than gpt-3, requires two-thirds less energy to train, requires half as much compute for inference, and outperforms gpt-3 on a wide range of natural language tasks. similar work on sparse models out of meta has yielded similarly promising results.as the meta researchers summarize: “we find that sparse models can achieve similar downstream task performance as dense models at a fraction of the compute. for models with relatively modest compute budgets, a sparse model can perform on par with a dense model that requires almost four times as much compute.”there is another benefit of sparse expert models that is worth mentioning: they are more interpretable than dense models.interpretability—the ability for a human to understand why a model took the action that it did—is one of ai’s greatest weaknesses today. in general, today’s neural networks are uninterpretable “black boxes.” this can limit their usefulness in the real world, particularly in high-stakes settings like healthcare where human review is important.sparse expert models lend themselves more naturally to interpretability than conventional models because a sparse model’s output is the result of an identifiable, discrete subset of parameters within the model—namely, the “experts” that were activated. the fact that humans can better extract understandable explanations from sparse models about their behavior may prove to be a decisive advantage for these models in real-world applications.sparse expert models are not in widespread use today. they are less well understood and more technically complex to build than dense models. yet considering their potential advantages, most of all their computational efficiency, don’t be surprised to see the sparse expert architecture become more prevalent in the world of llms going forward.in the words of graphcore cto simon knowles: “if an ai can do many things, it doesn’t need to access all of its knowledge to do one thing. it’s completely obvious. this is how your brain works, and it’s also how an ai ought to work. i’d be surprised if, by next year, anyone is building dense language models.”note: the author is a partner at radical ventures, which is an investor in you.com.\n"
     ]
    }
   ],
   "source": [
    "# using regular expressions \n",
    "# 4.Remove common non-sensical text (/n)\n",
    "import re \n",
    "op_string = re.sub(r'[\\n]','',full_text1)\n",
    "print(op_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88d1cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openai', 'ceo', 'sam', 'altman', '(', 'left', ')', 'meta', 'ai', 'chief', 'yann', 'lecun', '(', 'right', ')', 'differing', 'views', 'future', '...', '[', '+', ']', 'large', 'language', 'models', '.', 'case', '’', 'heard', ',', 'artificial', 'intelligence', 'hot', 'new', 'thing', '.', 'generative', 'ai', 'seems', 'lips', 'every', 'venture', 'capitalist', ',', 'entrepreneur', ',', 'fortune', '500', 'ceo', 'journalist', 'days', ',', 'silicon', 'valley', 'davos', '.', 'started', 'paying', 'real', 'attention', 'ai', '2022', ',', 'may', 'seem', 'technologies', 'like', 'chatgpt', 'stable', 'diffusion', 'came', 'nowhere', 'take', 'world', 'storm', '.', '’', '.', 'back', '2020', ',', 'wrote', 'article', 'column', 'predicting', 'generative', 'ai', 'would', 'one', 'pillars', 'next', 'generation', 'artificial', 'intelligence', '.', 'since', 'least', 'release', 'gpt-2', '2019', ',', 'clear', 'working', 'field', 'generative', 'language', 'models', 'poised', 'unleash', 'vast', 'economic', 'societal', 'transformation', '.', 'similarly', ',', 'text-to-image', 'models', 'captured', 'public', '’', 'attention', 'last', 'summer', ',', 'technology', '’', 'ascendance', 'appeared', 'inevitable', 'since', 'openai', 'released', 'original', 'dall-e', 'january', '2021', '.', '(', 'wrote', 'article', 'making', 'argument', 'days', 'release', 'original', 'dall-e.', ')', 'token', ',', 'important', 'remember', 'current', 'state', 'art', 'ai', 'far', 'end', 'state', 'ai', '’', 'capabilities', '.', 'contrary', ',', 'frontiers', 'artificial', 'intelligence', 'never', 'advanced', 'rapidly', 'right', '.', 'amazing', 'chatgpt', 'seems', 'us', 'moment', ',', 'mere', 'stepping', 'stone', 'comes', 'next', '.', 'next', 'generation', 'large', 'language', 'models', '(', 'llms', ')', 'look', 'like', '?', 'answer', 'question', 'already', ',', 'development', 'ai', 'startups', 'research', 'groups', 'moment', '.', 'article', 'highlights', 'three', 'emerging', 'areas', 'help', 'define', 'next', 'wave', 'innovation', 'generative', 'ai', 'llms', '.', 'looking', 'remain', 'ahead', 'curve', 'fast-changing', 'world—read', '.', 'consider', 'humans', 'think', 'learn', '.', 'collect', 'knowledge', 'perspective', 'external', 'sources', 'information—say', ',', 'reading', 'book', '.', 'also', 'generate', 'novel', 'ideas', 'insights', ',', 'reflecting', 'topic', 'thinking', 'problem', 'minds', '.', 'able', 'deepen', 'understanding', 'world', 'via', 'internal', 'reflection', 'analysis', 'directly', 'tied', 'new', 'external', 'input', '.', 'new', 'avenue', 'ai', 'research', 'seeks', 'enable', 'large', 'language', 'models', 'something', 'analogous', ',', 'effectively', 'bootstrapping', 'intelligence', '.', 'part', 'training', ',', 'today', '’', 'llms', 'ingest', 'much', 'world', '’', 'accumulated', 'written', 'information', '(', 'e.g.', ',', 'wikipedia', ',', 'books', ',', 'news', 'articles', ')', '.', 'models', ',', 'trained', ',', 'could', 'use', 'knowledge', 'absorbed', 'sources', 'produce', 'new', 'written', 'content—and', 'use', 'content', 'additional', 'training', 'data', 'order', 'improve', '?', 'initial', 'work', 'suggests', 'approach', 'may', 'possible—and', 'powerful', '.', 'one', 'recent', 'research', 'effort', ',', 'aptly', 'titled', '“', 'large', 'language', 'models', 'self-improve', ',', '”', 'group', 'google', 'researchers', 'built', 'llm', 'come', 'set', 'questions', ',', 'generate', 'detailed', 'answers', 'questions', ',', 'filter', 'answers', 'high-quality', 'output', ',', 'fine-tune', 'curated', 'answers', '.', 'remarkably', ',', 'leads', 'new', 'state-of-the-art', 'performance', 'various', 'language', 'tasks', '.', 'instance', ',', 'model', '’', 'performance', 'increased', '74.2', '%', '82.1', '%', 'gsm8k', '78.2', '%', '83.0', '%', 'drop', ',', 'two', 'popular', 'benchmarks', 'used', 'evaluate', 'llm', 'performance', '.', 'another', 'recent', 'work', 'builds', 'important', 'llm', 'method', 'called', '“', 'instruction', 'fine-tuning', ',', '”', 'lies', 'core', 'products', 'like', 'chatgpt', '.', 'whereas', 'chatgpt', 'instruction', 'fine-tuned', 'models', 'rely', 'human-written', 'instructions', ',', 'research', 'group', 'built', 'model', 'generate', 'natural', 'language', 'instructions', 'fine-tune', 'instructions', '.', 'performance', 'gains', 'dramatic', ':', 'method', 'improves', 'performance', 'base', 'gpt-3', 'model', '33', '%', ',', 'nearly', 'matching', 'performance', 'openai', '’', 'instruction-tuned', 'model', '.', 'thematically', 'related', 'work', ',', 'researchers', 'google', 'carnegie', 'mellon', 'show', 'large', 'language', 'model', ',', 'presented', 'question', ',', 'first', 'recites', 'knows', 'topic', 'responding', ',', 'provides', 'accurate', 'sophisticated', 'responses', '.', 'loosely', 'analogized', 'human', 'conversation', ',', 'rather', 'blurting', 'first', 'thing', 'comes', 'mind', 'topic', ',', 'searches', 'memory', 'reflects', 'beliefs', 'sharing', 'perspective', '.', 'people', 'first', 'hear', 'line', 'research', ',', 'conceptual', 'objection', 'often', 'arises—isn', '’', 'circular', '?', 'model', 'produce', 'data', 'model', 'consume', 'improve', '?', 'new', 'data', 'came', 'model', 'first', 'place', ',', '’', '“', 'knowledge', '”', '“', 'signal', '”', 'contains', 'already', 'incorporated', 'model', '?', 'objection', 'makes', 'sense', 'conceive', 'large', 'language', 'models', 'databases', ',', 'storing', 'information', 'training', 'data', 'reproducing', 'different', 'combinations', 'prompted', '.', 'but—uncomfortable', 'even', 'eerie', 'may', 'sound—we', 'better', 'instead', 'conceiving', 'large', 'language', 'models', 'along', 'lines', 'human', 'brain', '(', ',', 'analogy', 'course', 'perfect', '!', ')', '.', 'humans', 'ingest', 'tremendous', 'amount', 'data', 'world', 'alters', 'neural', 'connections', 'brains', 'imponderable', ',', 'innumerable', 'ways', '.', 'introspection', ',', 'writing', ',', 'conversation—sometimes', 'good', 'night', '’', 'sleep—our', 'brains', 'produce', 'new', 'insights', 'previously', 'minds', 'information', 'source', 'world', '.', 'internalize', 'new', 'insights', ',', 'make', 'us', 'wiser', '.', 'idea', 'llms', 'generate', 'training', 'data', 'particularly', 'important', 'light', 'fact', 'world', 'may', 'soon', 'run', 'text', 'training', 'data', '.', 'yet', 'widely', 'appreciated', 'problem', ',', 'one', 'many', 'ai', 'researchers', 'worried', '.', 'one', 'estimate', ',', 'world', '’', 'total', 'stock', 'usable', 'text', 'data', '4.6', 'trillion', '17.2', 'trillion', 'tokens', '.', 'includes', 'world', '’', 'books', ',', 'scientific', 'papers', ',', 'news', 'articles', ',', 'wikipedia', ',', 'publicly', 'available', 'code', ',', 'much', 'rest', 'internet', ',', 'filtered', 'quality', '(', 'e.g.', ',', 'webpages', ',', 'blogs', ',', 'social', 'media', ')', '.', 'another', 'recent', 'estimate', 'puts', 'total', 'figure', '3.2', 'trillion', 'tokens', '.', 'deepmind', '’', 'chinchilla', ',', 'one', 'today', '’', 'leading', 'llms', ',', 'trained', '1.4', 'trillion', 'tokens', '.', 'words', ',', 'may', 'well', 'within', 'one', 'order', 'magnitude', 'exhausting', 'world', '’', 'entire', 'supply', 'useful', 'language', 'training', 'data', '.', 'large', 'language', 'models', 'able', 'generate', 'training', 'data', 'use', 'continue', 'self-improving', ',', 'could', 'render', 'irrelevant', 'looming', 'data', 'shortage', '.', 'would', 'represent', 'mind-bending', 'leap', 'forward', 'llms', '.', 'popular', 'narrative', 'days', 'chatgpt', 'conversational', 'llms', 'like', 'verge', 'replacing', 'google', 'search', 'world', '’', 'go-to', 'source', 'information', ',', 'disrupting', 'once-mighty', 'tech', 'giant', 'like', 'blockbuster', 'kodak', 'disrupted', '.', 'narrative', 'badly', 'oversimplifies', 'things', '.', 'llms', 'exist', 'today', 'never', 'replace', 'google', 'search', '.', '?', 'short', ',', 'today', '’', 'llms', 'make', 'stuff', '.', 'powerful', ',', 'large', 'language', 'models', 'regularly', 'produce', 'inaccurate', ',', 'misleading', 'false', 'information', '(', 'present', 'confidently', 'convincingly', ')', '.', 'examples', 'abound', 'chatgpt', '’', '“', 'hallucinations', '”', '(', 'misstatements', 'referred', ')', '.', 'single', 'chatgpt', ';', 'every', 'generative', 'language', 'model', 'existence', 'today', 'hallucinates', 'similar', 'ways', '.', 'give', 'examples', ':', 'recommends', 'books', '’', 'exist', ';', 'insists', 'number', '220', 'less', '200', ';', 'unsure', 'whether', 'abraham', 'lincoln', '’', 'assassin', 'continent', 'lincoln', 'time', 'assassination', ';', 'provides', 'plausible-sounding', 'incorrect', 'explanations', 'concepts', 'like', 'bayes', '’', 'theorem', '.', 'users', 'accept', 'search', 'engine', 'gets', 'basic', 'facts', 'like', 'wrong', 'time', ';', 'even', '99', '%', 'accuracy', 'good', 'enough', 'broad', 'market', 'adoption', '.', 'openai', 'ceo', 'sam', 'altman', 'acknowledges', ',', 'recently', 'cautioning', ':', '“', 'chatgpt', 'incredibly', 'limited', ',', 'good', 'enough', 'things', 'create', 'misleading', 'impression', 'greatness', '.', \"'s\", 'mistake', 'relying', 'anything', 'important', 'right', 'now.', '”', 'open', 'question', 'whether', 'llms', '’', 'hallucination', 'problem', 'solved', 'via', 'incremental', 'improvements', 'existing', 'architectures', ',', 'whether', 'fundamental', 'paradigm', 'shift', 'ai', 'methodologies', 'necessary', 'give', 'ai', 'common', 'sense', 'real', 'understanding', '.', 'deep', 'learning', 'pioneer', 'yann', 'lecun', ',', 'one', ',', 'believes', 'latter', '.', 'lecun', '’', 'contrarian', 'perspective', 'may', 'prove', 'correct', ';', 'time', 'tell', '.', 'nearer', 'term', ',', 'though', ',', 'set', 'promising', 'innovations', 'offers', 'least', 'mitigate', 'llms', '’', 'factual', 'unreliability', '.', 'new', 'methods', 'play', 'essential', 'role', 'preparing', 'llms', 'widespread', 'real-world', 'deployment', '.', 'two', 'related', 'capabilities', 'lie', 'heart', 'current', 'efforts', 'make', 'language', 'models', 'accurate', ':', '(', '1', ')', 'ability', 'llms', 'retrieve', 'information', 'external', 'sources', ',', '(', '2', ')', 'ability', 'llms', 'provide', 'references', 'citations', 'information', 'provide', '.', 'chatgpt', 'limited', 'information', 'already', 'stored', 'inside', ',', 'captured', 'static', 'weights', '.', '(', 'able', 'discuss', 'events', 'occurred', '2021', ',', 'model', 'trained', '.', ')', 'able', 'pull', 'information', 'external', 'sources', 'empower', 'llms', 'access', 'accurate', 'up-to-date', 'information', 'available', ',', 'even', 'information', 'changes', 'frequently', '(', 'say', ',', 'companies', '’', 'stock', 'prices', ')', '.', 'course', ',', 'access', 'external', 'information', 'source', 'guarantee', 'llms', 'retrieve', 'accurate', 'relevant', 'information', '.', 'one', 'important', 'way', 'llms', 'increase', 'transparency', 'trust', 'human', 'users', 'include', 'references', 'source', '(', ')', 'retrieved', 'information', '.', 'citations', 'allow', 'human', 'users', 'audit', 'information', 'source', 'needed', 'order', 'decide', 'reliability', '.', 'important', 'early', 'work', 'field', 'includes', 'models', 'like', 'realm', '(', 'google', ')', 'rag', '(', 'facebook', ')', ',', 'published', '2020.', 'rise', 'conversational', 'llms', 'recent', 'months', ',', 'research', 'area', 'rapidly', 'accelerating', '.', 'last', 'year', ',', 'openai', 'published', 'fine-tuned', 'version', 'gpt', 'model', 'named', 'webgpt', 'browse', 'internet', 'using', 'microsoft', 'bing', 'order', 'provide', 'accurate', 'in-depth', 'responses', 'prompts', '.', 'webgpt', 'navigates', 'internet', 'much', 'like', 'human', ':', 'submit', 'search', 'queries', 'bing', ',', 'follow', 'links', ',', 'scroll', 'webpages', ',', 'use', 'functions', 'like', 'ctrl+f', 'find', 'terms', '.', 'model', 'finds', 'relevant', 'information', 'internet', 'incorporates', 'output', ',', 'provides', 'citations', 'human', 'user', 'see', 'information', 'came', '.', 'results', 'encouraging', ':', 'query', ',', 'webgpt', '’', 'responses', 'preferred', 'responses', 'written', 'human', 'subjects', '56', '%', 'time', 'preferred', 'highest-rated', 'responses', 'reddit', '69', '%', 'time', '.', 'deepmind', 'also', 'pursuing', 'research', 'along', 'lines', '.', 'months', 'ago', ',', 'deepmind', 'published', 'new', 'model', 'named', 'sparrow', '.', 'like', 'chatgpt', ',', 'sparrow', 'dialogue-based', ';', 'like', 'webgpt', ',', 'search', 'internet', 'information', 'provide', 'citations', 'assertions', '.', 'sparrow', 'builds', 'important', 'earlier', 'work', 'deepmind', 'including', 'spalm', ',', 'retro', 'gophercite', '.', 'deepmind', \"'s\", 'sparrow', 'model', 'action', '.', 'shown', ',', 'sparrow', 'provides', 'quotations', 'links', 'support', '...', '[', '+', ']', 'statements', ',', 'increasing', 'accuracy', 'trustworthiness', '.', 'deepmind', 'researchers', 'find', 'sparrow', '’', 'citations', 'helpful', 'accurate', '78', '%', 'time—suggesting', 'research', 'approach', 'promising', 'problem', 'llm', 'inaccuracy', 'far', 'solved', '.', 'younger', 'startups', 'including', 'you.com', 'perplexity', 'also', 'recently', 'launched', 'llm-powered', 'conversational', 'search', 'interfaces', 'ability', 'retrieve', 'information', 'external', 'sources', 'cite', 'references', '.', 'products', 'available', 'public', 'use', 'today', '.', 'llms', '’', 'greatest', 'shortcoming', 'unreliability', ',', 'stubborn', 'tendency', 'confidently', 'provide', 'inaccurate', 'information', '.', 'language', 'models', 'promise', 'reshape', 'every', 'sector', 'economy', ',', 'never', 'reach', 'full', 'potential', 'problem', 'addressed', '.', 'expect', 'see', 'plenty', 'activity', 'innovation', 'area', 'months', 'ahead', '.', 'today', '’', 'prominent', 'large', 'language', 'models', 'effectively', 'architecture', '.', 'meta', 'ai', 'chief', 'yann', 'lecun', 'said', 'recently', ':', '“', 'terms', 'underlying', 'techniques', ',', 'chatgpt', 'particularly', 'innovative', '.', '’', 'nothing', 'revolutionary', ',', 'although', '’', 'way', '’', 'perceived', 'public', '.', '’', ',', 'know', ',', '’', 'well', 'put', 'together', ',', '’', 'nicely', 'done.', '”', 'lecun', '’', 'statement', 'stirred', 'plenty', 'controversy', 'twitter', 'debate', '.', 'simple', 'fact', 'correct', ',', 'serious', 'ai', 'researcher', 'would', 'dispute', '.', 'today', '’', 'well-known', 'language', 'models—e.g.', ',', 'gpt-3', 'openai', ',', 'palm', 'lamda', 'google', ',', 'galactica', 'opt', 'meta', ',', 'megatron-turing', 'nvidia/microsoft', ',', 'jurassic-1', 'ai21', 'labs—are', 'built', 'basic', 'way', '.', 'autoregressive', ',', 'self-supervised', ',', 'pre-trained', ',', 'densely', 'activated', 'transformer-based', 'models', '.', 'sure', ',', 'variations', 'among', 'models', 'exist', ':', 'size', '(', 'parameter', 'count', ')', ',', 'data', 'trained', ',', 'optimization', 'algorithm', 'used', ',', 'batch', 'size', ',', 'number', 'hidden', 'layers', ',', 'whether', 'instruction', 'fine-tuned', ',', '.', 'variations', 'translate', 'meaningful', 'performance', 'differences', '.', 'core', 'architectures', ',', 'though', ',', 'vary', 'little', '.', 'yet', 'momentum', 'building', 'behind', 'intriguingly', 'different', 'architectural', 'approach', 'language', 'models', 'known', 'sparse', 'expert', 'models', '.', 'idea', 'around', 'decades', ',', 'recently', 'reemerged', 'begun', 'gain', 'popularity', '.', 'models', 'mentioned', 'dense', '.', 'means', 'every', 'time', 'model', 'runs', ',', 'every', 'single', 'one', 'parameters', 'used', '.', 'every', 'time', 'submit', 'prompt', 'gpt-3', ',', 'instance', ',', '175', 'billion', 'model', '’', 'parameters', 'activated', 'order', 'produce', 'response', '.', 'model', 'able', 'call', 'upon', 'relevant', 'subset', 'parameters', 'order', 'respond', 'given', 'query', '?', 'basic', 'concept', 'behind', 'sparse', 'expert', 'models', '.', 'defining', 'characteristic', 'sparse', 'models', '’', 'activate', 'parameters', 'given', 'input', ',', 'rather', 'parameters', 'helpful', 'order', 'handle', 'input', '.', 'model', 'sparsity', 'thus', 'decouples', 'model', '’', 'total', 'parameter', 'count', 'compute', 'requirements', '.', 'leads', 'sparse', 'expert', 'models', '’', 'key', 'advantage', ':', 'larger', 'less', 'computationally', 'demanding', 'dense', 'models', '.', 'called', 'sparse', 'expert', 'models', '?', 'sparse', 'models', 'thought', 'consisting', 'collection', '“', 'sub-models', '”', 'serve', 'experts', 'different', 'topics', '.', 'depending', 'prompt', 'presented', 'model', ',', 'relevant', 'experts', 'within', 'model', 'activated', 'experts', 'remain', 'inactive', '.', 'prompt', 'posed', 'russian', ',', 'instance', ',', 'would', 'activate', '“', 'experts', '”', 'within', 'model', 'understand', 'respond', 'russian', ',', 'efficiently', 'bypassing', 'rest', 'model', '.', 'today', '’', 'largest', 'llms', 'sparse', '.', 'come', 'across', 'llm', '1', 'trillion', 'parameters', ',', 'safely', 'assume', 'sparse', '.', 'includes', 'google', '’', 'switch', 'transformer', '(', '1.6', 'trillion', 'parameters', ')', ',', 'google', '’', 'glam', '(', '1.2', 'trillion', 'parameters', ')', 'meta', '’', 'mixture', 'experts', 'model', '(', '1.1', 'trillion', 'parameters', ')', '.', '“', 'much', 'recent', 'progress', 'ai', 'come', 'training', 'larger', 'larger', 'models', ',', '”', 'said', 'mikel', 'artetxe', ',', 'led', 'meta', '’', 'research', 'sparse', 'models', 'resigning', 'cofound', 'stealth', 'llm', 'startup', '.', '“', 'gpt-3', ',', 'instance', ',', '100', 'times', 'larger', 'gpt-2', '.', 'double', 'size', 'dense', 'model', ',', 'also', 'make', 'twice', 'slow', '.', 'sparse', 'models', 'allow', 'us', 'train', 'larger', 'models', 'without', 'increase', 'runtime.', '”', 'recent', 'research', 'sparse', 'expert', 'models', 'suggests', 'architecture', 'holds', 'massive', 'potential', '.', 'glam', ',', 'sparse', 'expert', 'model', 'developed', 'last', 'year', 'google', ',', '7', 'times', 'larger', 'gpt-3', ',', 'requires', 'two-thirds', 'less', 'energy', 'train', ',', 'requires', 'half', 'much', 'compute', 'inference', ',', 'outperforms', 'gpt-3', 'wide', 'range', 'natural', 'language', 'tasks', '.', 'similar', 'work', 'sparse', 'models', 'meta', 'yielded', 'similarly', 'promising', 'results', '.', 'meta', 'researchers', 'summarize', ':', '“', 'find', 'sparse', 'models', 'achieve', 'similar', 'downstream', 'task', 'performance', 'dense', 'models', 'fraction', 'compute', '.', 'models', 'relatively', 'modest', 'compute', 'budgets', ',', 'sparse', 'model', 'perform', 'par', 'dense', 'model', 'requires', 'almost', 'four', 'times', 'much', 'compute.', '”', 'another', 'benefit', 'sparse', 'expert', 'models', 'worth', 'mentioning', ':', 'interpretable', 'dense', 'models', '.', 'interpretability—the', 'ability', 'human', 'understand', 'model', 'took', 'action', 'did—is', 'one', 'ai', '’', 'greatest', 'weaknesses', 'today', '.', 'general', ',', 'today', '’', 'neural', 'networks', 'uninterpretable', '“', 'black', 'boxes.', '”', 'limit', 'usefulness', 'real', 'world', ',', 'particularly', 'high-stakes', 'settings', 'like', 'healthcare', 'human', 'review', 'important', '.', 'sparse', 'expert', 'models', 'lend', 'naturally', 'interpretability', 'conventional', 'models', 'sparse', 'model', '’', 'output', 'result', 'identifiable', ',', 'discrete', 'subset', 'parameters', 'within', 'model—namely', ',', '“', 'experts', '”', 'activated', '.', 'fact', 'humans', 'better', 'extract', 'understandable', 'explanations', 'sparse', 'models', 'behavior', 'may', 'prove', 'decisive', 'advantage', 'models', 'real-world', 'applications', '.', 'sparse', 'expert', 'models', 'widespread', 'use', 'today', '.', 'less', 'well', 'understood', 'technically', 'complex', 'build', 'dense', 'models', '.', 'yet', 'considering', 'potential', 'advantages', ',', 'computational', 'efficiency', ',', '’', 'surprised', 'see', 'sparse', 'expert', 'architecture', 'become', 'prevalent', 'world', 'llms', 'going', 'forward', '.', 'words', 'graphcore', 'cto', 'simon', 'knowles', ':', '“', 'ai', 'many', 'things', ',', '’', 'need', 'access', 'knowledge', 'one', 'thing', '.', '’', 'completely', 'obvious', '.', 'brain', 'works', ',', '’', 'also', 'ai', 'ought', 'work', '.', '’', 'surprised', ',', 'next', 'year', ',', 'anyone', 'building', 'dense', 'language', 'models.', '”', 'note', ':', 'author', 'partner', 'radical', 'ventures', ',', 'investor', 'you.com', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shashankgoyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 6.Remove stop words\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "full_text = full_text1\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "word = word_tokenize(full_text1)\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0facd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openai', 'ceo', 'sam', 'altman', '(', 'left', ')', 'and', 'meta', 'ai', 'chief', 'yann', 'lecun', '(', 'right', ')', 'have', 'differing', 'views', 'on', 'the', 'future', '...', '[', '+', ']', 'of', 'large', 'language', 'models', '.', 'in', 'case', 'you', 'haven', '’', 't', 'heard', ',', 'artificial', 'intelligence', 'is', 'the', 'hot', 'new', 'thing', '.', 'generative', 'ai', 'seems', 'to', 'be', 'on', 'the', 'lips', 'of', 'every', 'venture', 'capitalist', ',', 'entrepreneur', ',', 'fortune', '500', 'ceo', 'and', 'journalist', 'these', 'days', ',', 'from', 'silicon', 'valley', 'to', 'davos', '.', 'to', 'those', 'who', 'started', 'paying', 'real', 'attention', 'to', 'ai', 'in', '2022', ',', 'it', 'may', 'seem', 'that', 'technologies', 'like', 'chatgpt', 'and', 'stable', 'diffusion', 'came', 'out', 'of', 'nowhere', 'to', 'take', 'the', 'world', 'by', 'storm', '.', 'they', 'didn', '’', 't', '.', 'back', 'in', '2020', ',', 'we', 'wrote', 'an', 'article', 'in', 'this', 'column', 'predicting', 'that', 'generative', 'ai', 'would', 'be', 'one', 'of', 'the', 'pillars', 'of', 'the', 'next', 'generation', 'of', 'artificial', 'intelligence', '.', 'since', 'at', 'least', 'the', 'release', 'of', 'gpt-2', 'in', '2019', ',', 'it', 'has', 'been', 'clear', 'to', 'those', 'working', 'in', 'the', 'field', 'that', 'generative', 'language', 'models', 'were', 'poised', 'to', 'unleash', 'vast', 'economic', 'and', 'societal', 'transformation', '.', 'similarly', ',', 'while', 'text-to-image', 'models', 'only', 'captured', 'the', 'public', '’', 's', 'attention', 'last', 'summer', ',', 'the', 'technology', '’', 's', 'ascendance', 'has', 'appeared', 'inevitable', 'since', 'openai', 'released', 'the', 'original', 'dall-e', 'in', 'january', '2021', '.', '(', 'we', 'wrote', 'an', 'article', 'making', 'this', 'argument', 'days', 'after', 'the', 'release', 'of', 'the', 'original', 'dall-e.', ')', 'by', 'this', 'same', 'token', ',', 'it', 'is', 'important', 'to', 'remember', 'that', 'the', 'current', 'state', 'of', 'the', 'art', 'in', 'ai', 'is', 'far', 'from', 'an', 'end', 'state', 'for', 'ai', '’', 's', 'capabilities', '.', 'on', 'the', 'contrary', ',', 'the', 'frontiers', 'of', 'artificial', 'intelligence', 'have', 'never', 'advanced', 'more', 'rapidly', 'than', 'they', 'are', 'right', 'now', '.', 'as', 'amazing', 'as', 'chatgpt', 'seems', 'to', 'us', 'at', 'the', 'moment', ',', 'it', 'is', 'a', 'mere', 'stepping', 'stone', 'to', 'what', 'comes', 'next', '.', 'what', 'will', 'the', 'next', 'generation', 'of', 'large', 'language', 'models', '(', 'llms', ')', 'look', 'like', '?', 'the', 'answer', 'to', 'this', 'question', 'is', 'already', 'out', 'there', ',', 'under', 'development', 'at', 'ai', 'startups', 'and', 'research', 'groups', 'at', 'this', 'very', 'moment', '.', 'this', 'article', 'highlights', 'three', 'emerging', 'areas', 'that', 'will', 'help', 'define', 'the', 'next', 'wave', 'of', 'innovation', 'in', 'generative', 'ai', 'and', 'llms', '.', 'for', 'those', 'looking', 'to', 'remain', 'ahead', 'of', 'the', 'curve', 'in', 'this', 'fast-changing', 'world—read', 'on', '.', 'consider', 'how', 'humans', 'think', 'and', 'learn', '.', 'we', 'collect', 'knowledge', 'and', 'perspective', 'from', 'external', 'sources', 'of', 'information—say', ',', 'by', 'reading', 'a', 'book', '.', 'but', 'we', 'also', 'generate', 'novel', 'ideas', 'and', 'insights', 'on', 'our', 'own', ',', 'by', 'reflecting', 'on', 'a', 'topic', 'or', 'thinking', 'through', 'a', 'problem', 'in', 'our', 'minds', '.', 'we', 'are', 'able', 'to', 'deepen', 'our', 'understanding', 'of', 'the', 'world', 'via', 'internal', 'reflection', 'and', 'analysis', 'not', 'directly', 'tied', 'to', 'any', 'new', 'external', 'input', '.', 'a', 'new', 'avenue', 'of', 'ai', 'research', 'seeks', 'to', 'enable', 'large', 'language', 'models', 'to', 'do', 'something', 'analogous', ',', 'effectively', 'bootstrapping', 'their', 'own', 'intelligence', '.', 'as', 'part', 'of', 'their', 'training', ',', 'today', '’', 's', 'llms', 'ingest', 'much', 'of', 'the', 'world', '’', 's', 'accumulated', 'written', 'information', '(', 'e.g.', ',', 'wikipedia', ',', 'books', ',', 'news', 'articles', ')', '.', 'what', 'if', 'these', 'models', ',', 'once', 'trained', ',', 'could', 'use', 'all', 'the', 'knowledge', 'that', 'they', 'have', 'absorbed', 'from', 'these', 'sources', 'to', 'produce', 'new', 'written', 'content—and', 'then', 'use', 'that', 'content', 'as', 'additional', 'training', 'data', 'in', 'order', 'to', 'improve', 'themselves', '?', 'initial', 'work', 'suggests', 'that', 'this', 'approach', 'may', 'be', 'possible—and', 'powerful', '.', 'in', 'one', 'recent', 'research', 'effort', ',', 'aptly', 'titled', '“', 'large', 'language', 'models', 'can', 'self-improve', ',', '”', 'a', 'group', 'of', 'google', 'researchers', 'built', 'an', 'llm', 'that', 'can', 'come', 'up', 'with', 'a', 'set', 'of', 'questions', ',', 'generate', 'detailed', 'answers', 'to', 'those', 'questions', ',', 'filter', 'its', 'own', 'answers', 'for', 'the', 'most', 'high-quality', 'output', ',', 'and', 'then', 'fine-tune', 'itself', 'on', 'the', 'curated', 'answers', '.', 'remarkably', ',', 'this', 'leads', 'to', 'new', 'state-of-the-art', 'performance', 'on', 'various', 'language', 'tasks', '.', 'for', 'instance', ',', 'the', 'model', '’', 's', 'performance', 'increased', 'from', '74.2', '%', 'to', '82.1', '%', 'on', 'gsm8k', 'and', 'from', '78.2', '%', 'to', '83.0', '%', 'on', 'drop', ',', 'two', 'popular', 'benchmarks', 'used', 'to', 'evaluate', 'llm', 'performance', '.', 'another', 'recent', 'work', 'builds', 'on', 'an', 'important', 'llm', 'method', 'called', '“', 'instruction', 'fine-tuning', ',', '”', 'which', 'lies', 'at', 'the', 'core', 'of', 'products', 'like', 'chatgpt', '.', 'whereas', 'chatgpt', 'and', 'other', 'instruction', 'fine-tuned', 'models', 'rely', 'on', 'human-written', 'instructions', ',', 'this', 'research', 'group', 'built', 'a', 'model', 'that', 'can', 'generate', 'its', 'own', 'natural', 'language', 'instructions', 'and', 'then', 'fine-tune', 'itself', 'on', 'those', 'instructions', '.', 'the', 'performance', 'gains', 'are', 'dramatic', ':', 'this', 'method', 'improves', 'the', 'performance', 'of', 'the', 'base', 'gpt-3', 'model', 'by', '33', '%', ',', 'nearly', 'matching', 'the', 'performance', 'of', 'openai', '’', 's', 'own', 'instruction-tuned', 'model', '.', 'in', 'a', 'thematically', 'related', 'work', ',', 'researchers', 'from', 'google', 'and', 'carnegie', 'mellon', 'show', 'that', 'if', 'a', 'large', 'language', 'model', ',', 'when', 'presented', 'with', 'a', 'question', ',', 'first', 'recites', 'to', 'itself', 'what', 'it', 'knows', 'about', 'the', 'topic', 'before', 'responding', ',', 'it', 'provides', 'more', 'accurate', 'and', 'sophisticated', 'responses', '.', 'this', 'can', 'be', 'loosely', 'analogized', 'to', 'a', 'human', 'in', 'conversation', 'who', ',', 'rather', 'than', 'blurting', 'out', 'the', 'first', 'thing', 'that', 'comes', 'to', 'mind', 'on', 'a', 'topic', ',', 'searches', 'her', 'memory', 'and', 'reflects', 'on', 'her', 'beliefs', 'before', 'sharing', 'a', 'perspective', '.', 'when', 'people', 'first', 'hear', 'about', 'this', 'line', 'of', 'research', ',', 'a', 'conceptual', 'objection', 'often', 'arises—isn', '’', 't', 'this', 'all', 'circular', '?', 'how', 'can', 'a', 'model', 'produce', 'data', 'that', 'the', 'model', 'can', 'then', 'consume', 'to', 'improve', 'itself', '?', 'if', 'the', 'new', 'data', 'came', 'from', 'the', 'model', 'in', 'the', 'first', 'place', ',', 'shouldn', '’', 't', 'the', '“', 'knowledge', '”', 'or', '“', 'signal', '”', 'that', 'it', 'contains', 'already', 'be', 'incorporated', 'in', 'the', 'model', '?', 'this', 'objection', 'makes', 'sense', 'if', 'we', 'conceive', 'of', 'large', 'language', 'models', 'as', 'databases', ',', 'storing', 'information', 'from', 'their', 'training', 'data', 'and', 'reproducing', 'it', 'in', 'different', 'combinations', 'when', 'prompted', '.', 'but—uncomfortable', 'or', 'even', 'eerie', 'as', 'it', 'may', 'sound—we', 'are', 'better', 'off', 'instead', 'conceiving', 'of', 'large', 'language', 'models', 'along', 'the', 'lines', 'of', 'the', 'human', 'brain', '(', 'no', ',', 'the', 'analogy', 'is', 'of', 'course', 'not', 'perfect', '!', ')', '.', 'we', 'humans', 'ingest', 'a', 'tremendous', 'amount', 'of', 'data', 'from', 'the', 'world', 'that', 'alters', 'the', 'neural', 'connections', 'in', 'our', 'brains', 'in', 'imponderable', ',', 'innumerable', 'ways', '.', 'through', 'introspection', ',', 'writing', ',', 'conversation—sometimes', 'just', 'a', 'good', 'night', '’', 's', 'sleep—our', 'brains', 'can', 'then', 'produce', 'new', 'insights', 'that', 'had', 'not', 'previously', 'been', 'in', 'our', 'minds', 'nor', 'in', 'any', 'information', 'source', 'out', 'in', 'the', 'world', '.', 'if', 'we', 'internalize', 'these', 'new', 'insights', ',', 'they', 'can', 'make', 'us', 'wiser', '.', 'the', 'idea', 'that', 'llms', 'can', 'generate', 'their', 'own', 'training', 'data', 'is', 'particularly', 'important', 'in', 'light', 'of', 'the', 'fact', 'that', 'the', 'world', 'may', 'soon', 'run', 'out', 'of', 'text', 'training', 'data', '.', 'this', 'is', 'not', 'yet', 'a', 'widely', 'appreciated', 'problem', ',', 'but', 'it', 'is', 'one', 'that', 'many', 'ai', 'researchers', 'are', 'worried', 'about', '.', 'by', 'one', 'estimate', ',', 'the', 'world', '’', 's', 'total', 'stock', 'of', 'usable', 'text', 'data', 'is', 'between', '4.6', 'trillion', 'and', '17.2', 'trillion', 'tokens', '.', 'this', 'includes', 'all', 'the', 'world', '’', 's', 'books', ',', 'all', 'scientific', 'papers', ',', 'all', 'news', 'articles', ',', 'all', 'of', 'wikipedia', ',', 'all', 'publicly', 'available', 'code', ',', 'and', 'much', 'of', 'the', 'rest', 'of', 'the', 'internet', ',', 'filtered', 'for', 'quality', '(', 'e.g.', ',', 'webpages', ',', 'blogs', ',', 'social', 'media', ')', '.', 'another', 'recent', 'estimate', 'puts', 'the', 'total', 'figure', 'at', '3.2', 'trillion', 'tokens', '.', 'deepmind', '’', 's', 'chinchilla', ',', 'one', 'of', 'today', '’', 's', 'leading', 'llms', ',', 'was', 'trained', 'on', '1.4', 'trillion', 'tokens', '.', 'in', 'other', 'words', ',', 'we', 'may', 'be', 'well', 'within', 'one', 'order', 'of', 'magnitude', 'of', 'exhausting', 'the', 'world', '’', 's', 'entire', 'supply', 'of', 'useful', 'language', 'training', 'data', '.', 'if', 'large', 'language', 'models', 'are', 'able', 'to', 'generate', 'their', 'own', 'training', 'data', 'and', 'use', 'it', 'to', 'continue', 'self-improving', ',', 'this', 'could', 'render', 'irrelevant', 'the', 'looming', 'data', 'shortage', '.', 'it', 'would', 'represent', 'a', 'mind-bending', 'leap', 'forward', 'for', 'llms', '.', 'a', 'popular', 'narrative', 'these', 'days', 'is', 'that', 'chatgpt', 'and', 'conversational', 'llms', 'like', 'it', 'are', 'on', 'the', 'verge', 'of', 'replacing', 'google', 'search', 'as', 'the', 'world', '’', 's', 'go-to', 'source', 'for', 'information', ',', 'disrupting', 'the', 'once-mighty', 'tech', 'giant', 'like', 'blockbuster', 'or', 'kodak', 'were', 'disrupted', 'before', 'it', '.', 'this', 'narrative', 'badly', 'oversimplifies', 'things', '.', 'llms', 'as', 'they', 'exist', 'today', 'will', 'never', 'replace', 'google', 'search', '.', 'why', 'not', '?', 'in', 'short', ',', 'because', 'today', '’', 's', 'llms', 'make', 'stuff', 'up', '.', 'as', 'powerful', 'as', 'they', 'are', ',', 'large', 'language', 'models', 'regularly', 'produce', 'inaccurate', ',', 'misleading', 'or', 'false', 'information', '(', 'and', 'present', 'it', 'confidently', 'and', 'convincingly', ')', '.', 'examples', 'abound', 'of', 'chatgpt', '’', 's', '“', 'hallucinations', '”', '(', 'as', 'these', 'misstatements', 'are', 'referred', 'to', ')', '.', 'this', 'is', 'not', 'to', 'single', 'out', 'chatgpt', ';', 'every', 'generative', 'language', 'model', 'in', 'existence', 'today', 'hallucinates', 'in', 'similar', 'ways', '.', 'to', 'give', 'a', 'few', 'examples', ':', 'it', 'recommends', 'books', 'that', 'don', '’', 't', 'exist', ';', 'it', 'insists', 'that', 'the', 'number', '220', 'is', 'less', 'than', '200', ';', 'it', 'is', 'unsure', 'whether', 'abraham', 'lincoln', '’', 's', 'assassin', 'was', 'on', 'the', 'same', 'continent', 'as', 'lincoln', 'at', 'the', 'time', 'of', 'the', 'assassination', ';', 'it', 'provides', 'plausible-sounding', 'but', 'incorrect', 'explanations', 'of', 'concepts', 'like', 'bayes', '’', 'theorem', '.', 'most', 'users', 'will', 'not', 'accept', 'a', 'search', 'engine', 'that', 'gets', 'basic', 'facts', 'like', 'these', 'wrong', 'some', 'of', 'the', 'time', ';', 'even', '99', '%', 'accuracy', 'will', 'not', 'be', 'good', 'enough', 'for', 'broad', 'market', 'adoption', '.', 'openai', 'ceo', 'sam', 'altman', 'himself', 'acknowledges', 'this', ',', 'recently', 'cautioning', ':', '“', 'chatgpt', 'is', 'incredibly', 'limited', ',', 'but', 'good', 'enough', 'at', 'some', 'things', 'to', 'create', 'a', 'misleading', 'impression', 'of', 'greatness', '.', 'it', \"'s\", 'a', 'mistake', 'to', 'be', 'relying', 'on', 'it', 'for', 'anything', 'important', 'right', 'now.', '”', 'it', 'is', 'an', 'open', 'question', 'whether', 'llms', '’', 'hallucination', 'problem', 'can', 'be', 'solved', 'via', 'incremental', 'improvements', 'to', 'existing', 'architectures', ',', 'or', 'whether', 'a', 'more', 'fundamental', 'paradigm', 'shift', 'in', 'ai', 'methodologies', 'will', 'be', 'necessary', 'to', 'give', 'ai', 'common', 'sense', 'and', 'real', 'understanding', '.', 'deep', 'learning', 'pioneer', 'yann', 'lecun', ',', 'for', 'one', ',', 'believes', 'the', 'latter', '.', 'lecun', '’', 's', 'contrarian', 'perspective', 'may', 'prove', 'correct', ';', 'time', 'will', 'tell', '.', 'in', 'the', 'nearer', 'term', ',', 'though', ',', 'a', 'set', 'of', 'promising', 'innovations', 'offers', 'to', 'at', 'least', 'mitigate', 'llms', '’', 'factual', 'unreliability', '.', 'these', 'new', 'methods', 'will', 'play', 'an', 'essential', 'role', 'in', 'preparing', 'llms', 'for', 'widespread', 'real-world', 'deployment', '.', 'two', 'related', 'capabilities', 'lie', 'at', 'the', 'heart', 'of', 'current', 'efforts', 'to', 'make', 'language', 'models', 'more', 'accurate', ':', '(', '1', ')', 'the', 'ability', 'for', 'llms', 'to', 'retrieve', 'information', 'from', 'external', 'sources', ',', 'and', '(', '2', ')', 'the', 'ability', 'for', 'llms', 'to', 'provide', 'references', 'and', 'citations', 'for', 'the', 'information', 'they', 'provide', '.', 'chatgpt', 'is', 'limited', 'to', 'the', 'information', 'that', 'is', 'already', 'stored', 'inside', 'of', 'it', ',', 'captured', 'in', 'its', 'static', 'weights', '.', '(', 'this', 'is', 'why', 'it', 'is', 'not', 'able', 'to', 'discuss', 'events', 'that', 'occurred', 'after', '2021', ',', 'when', 'the', 'model', 'was', 'trained', '.', ')', 'being', 'able', 'to', 'pull', 'in', 'information', 'from', 'external', 'sources', 'will', 'empower', 'llms', 'to', 'access', 'the', 'most', 'accurate', 'and', 'up-to-date', 'information', 'available', ',', 'even', 'when', 'that', 'information', 'changes', 'frequently', '(', 'say', ',', 'companies', '’', 'stock', 'prices', ')', '.', 'of', 'course', ',', 'having', 'access', 'to', 'an', 'external', 'information', 'source', 'does', 'not', 'by', 'itself', 'guarantee', 'that', 'llms', 'will', 'retrieve', 'the', 'most', 'accurate', 'and', 'relevant', 'information', '.', 'one', 'important', 'way', 'for', 'llms', 'to', 'increase', 'transparency', 'and', 'trust', 'with', 'human', 'users', 'is', 'to', 'include', 'references', 'to', 'the', 'source', '(', 's', ')', 'from', 'which', 'they', 'retrieved', 'the', 'information', '.', 'such', 'citations', 'allow', 'human', 'users', 'to', 'audit', 'the', 'information', 'source', 'as', 'needed', 'in', 'order', 'to', 'decide', 'for', 'themselves', 'on', 'its', 'reliability', '.', 'important', 'early', 'work', 'in', 'this', 'field', 'includes', 'models', 'like', 'realm', '(', 'from', 'google', ')', 'and', 'rag', '(', 'from', 'facebook', ')', ',', 'both', 'published', 'in', '2020.', 'with', 'the', 'rise', 'of', 'conversational', 'llms', 'in', 'recent', 'months', ',', 'research', 'in', 'this', 'area', 'is', 'now', 'rapidly', 'accelerating', '.', 'last', 'year', ',', 'openai', 'published', 'a', 'fine-tuned', 'version', 'of', 'its', 'gpt', 'model', 'named', 'webgpt', 'that', 'can', 'browse', 'the', 'internet', 'using', 'microsoft', 'bing', 'in', 'order', 'to', 'provide', 'more', 'accurate', 'and', 'in-depth', 'responses', 'to', 'prompts', '.', 'webgpt', 'navigates', 'the', 'internet', 'much', 'like', 'a', 'human', 'does', ':', 'it', 'can', 'submit', 'search', 'queries', 'to', 'bing', ',', 'follow', 'links', ',', 'scroll', 'up', 'and', 'down', 'on', 'webpages', ',', 'and', 'use', 'functions', 'like', 'ctrl+f', 'to', 'find', 'terms', '.', 'when', 'the', 'model', 'finds', 'relevant', 'information', 'on', 'the', 'internet', 'that', 'it', 'incorporates', 'into', 'its', 'output', ',', 'it', 'provides', 'citations', 'so', 'that', 'the', 'human', 'user', 'can', 'see', 'where', 'the', 'information', 'came', 'from', '.', 'the', 'results', 'are', 'encouraging', ':', 'for', 'the', 'same', 'query', ',', 'webgpt', '’', 's', 'responses', 'are', 'preferred', 'to', 'responses', 'written', 'by', 'human', 'subjects', '56', '%', 'of', 'the', 'time', 'and', 'are', 'preferred', 'to', 'the', 'highest-rated', 'responses', 'on', 'reddit', '69', '%', 'of', 'the', 'time', '.', 'deepmind', 'is', 'also', 'pursuing', 'research', 'along', 'these', 'lines', '.', 'a', 'few', 'months', 'ago', ',', 'deepmind', 'published', 'a', 'new', 'model', 'named', 'sparrow', '.', 'like', 'chatgpt', ',', 'sparrow', 'is', 'dialogue-based', ';', 'like', 'webgpt', ',', 'it', 'can', 'search', 'the', 'internet', 'for', 'information', 'and', 'provide', 'citations', 'for', 'its', 'assertions', '.', 'sparrow', 'builds', 'on', 'important', 'earlier', 'work', 'out', 'of', 'deepmind', 'including', 'spalm', ',', 'retro', 'and', 'gophercite', '.', 'deepmind', \"'s\", 'sparrow', 'model', 'in', 'action', '.', 'as', 'shown', 'here', ',', 'sparrow', 'provides', 'quotations', 'and', 'links', 'to', 'support', '...', '[', '+', ']', 'its', 'statements', ',', 'increasing', 'their', 'accuracy', 'and', 'trustworthiness', '.', 'the', 'deepmind', 'researchers', 'find', 'that', 'sparrow', '’', 's', 'citations', 'are', 'helpful', 'and', 'accurate', '78', '%', 'of', 'the', 'time—suggesting', 'both', 'that', 'this', 'research', 'approach', 'is', 'promising', 'and', 'that', 'the', 'problem', 'of', 'llm', 'inaccuracy', 'is', 'far', 'from', 'solved', '.', 'younger', 'startups', 'including', 'you.com', 'and', 'perplexity', 'have', 'also', 'recently', 'launched', 'llm-powered', 'conversational', 'search', 'interfaces', 'with', 'the', 'ability', 'to', 'retrieve', 'information', 'from', 'external', 'sources', 'and', 'cite', 'references', '.', 'these', 'products', 'are', 'available', 'for', 'public', 'use', 'today', '.', 'llms', '’', 'greatest', 'shortcoming', 'is', 'their', 'unreliability', ',', 'their', 'stubborn', 'tendency', 'to', 'confidently', 'provide', 'inaccurate', 'information', '.', 'language', 'models', 'promise', 'to', 'reshape', 'every', 'sector', 'of', 'our', 'economy', ',', 'but', 'they', 'will', 'never', 'reach', 'their', 'full', 'potential', 'until', 'this', 'problem', 'is', 'addressed', '.', 'expect', 'to', 'see', 'plenty', 'of', 'activity', 'and', 'innovation', 'in', 'this', 'area', 'in', 'the', 'months', 'ahead', '.', 'today', '’', 's', 'most', 'prominent', 'large', 'language', 'models', 'all', 'have', 'effectively', 'the', 'same', 'architecture', '.', 'meta', 'ai', 'chief', 'yann', 'lecun', 'said', 'recently', ':', '“', 'in', 'terms', 'of', 'underlying', 'techniques', ',', 'chatgpt', 'is', 'not', 'particularly', 'innovative', '.', 'it', '’', 's', 'nothing', 'revolutionary', ',', 'although', 'that', '’', 's', 'the', 'way', 'it', '’', 's', 'perceived', 'in', 'the', 'public', '.', 'it', '’', 's', 'just', 'that', ',', 'you', 'know', ',', 'it', '’', 's', 'well', 'put', 'together', ',', 'it', '’', 's', 'nicely', 'done.', '”', 'lecun', '’', 's', 'statement', 'stirred', 'up', 'plenty', 'of', 'controversy', 'and', 'twitter', 'debate', '.', 'but', 'the', 'simple', 'fact', 'is', 'that', 'he', 'is', 'correct', ',', 'as', 'no', 'serious', 'ai', 'researcher', 'would', 'dispute', '.', 'all', 'of', 'today', '’', 's', 'well-known', 'language', 'models—e.g.', ',', 'gpt-3', 'from', 'openai', ',', 'palm', 'or', 'lamda', 'from', 'google', ',', 'galactica', 'or', 'opt', 'from', 'meta', ',', 'megatron-turing', 'from', 'nvidia/microsoft', ',', 'jurassic-1', 'from', 'ai21', 'labs—are', 'built', 'in', 'the', 'same', 'basic', 'way', '.', 'they', 'are', 'autoregressive', ',', 'self-supervised', ',', 'pre-trained', ',', 'densely', 'activated', 'transformer-based', 'models', '.', 'to', 'be', 'sure', ',', 'variations', 'among', 'these', 'models', 'exist', ':', 'their', 'size', '(', 'parameter', 'count', ')', ',', 'the', 'data', 'they', 'are', 'trained', 'on', ',', 'the', 'optimization', 'algorithm', 'used', ',', 'the', 'batch', 'size', ',', 'the', 'number', 'of', 'hidden', 'layers', ',', 'whether', 'they', 'are', 'instruction', 'fine-tuned', ',', 'and', 'so', 'on', '.', 'these', 'variations', 'can', 'translate', 'to', 'meaningful', 'performance', 'differences', '.', 'the', 'core', 'architectures', ',', 'though', ',', 'vary', 'little', '.', 'yet', 'momentum', 'is', 'building', 'behind', 'an', 'intriguingly', 'different', 'architectural', 'approach', 'to', 'language', 'models', 'known', 'as', 'sparse', 'expert', 'models', '.', 'while', 'the', 'idea', 'has', 'been', 'around', 'for', 'decades', ',', 'it', 'has', 'only', 'recently', 'reemerged', 'and', 'begun', 'to', 'gain', 'in', 'popularity', '.', 'all', 'of', 'the', 'models', 'mentioned', 'above', 'are', 'dense', '.', 'this', 'means', 'that', 'every', 'time', 'the', 'model', 'runs', ',', 'every', 'single', 'one', 'of', 'its', 'parameters', 'is', 'used', '.', 'every', 'time', 'you', 'submit', 'a', 'prompt', 'to', 'gpt-3', ',', 'for', 'instance', ',', 'all', '175', 'billion', 'of', 'the', 'model', '’', 's', 'parameters', 'are', 'activated', 'in', 'order', 'to', 'produce', 'its', 'response', '.', 'but', 'what', 'if', 'a', 'model', 'were', 'able', 'to', 'call', 'upon', 'only', 'the', 'most', 'relevant', 'subset', 'of', 'its', 'parameters', 'in', 'order', 'to', 'respond', 'to', 'a', 'given', 'query', '?', 'this', 'is', 'the', 'basic', 'concept', 'behind', 'sparse', 'expert', 'models', '.', 'the', 'defining', 'characteristic', 'of', 'sparse', 'models', 'is', 'that', 'they', 'don', '’', 't', 'activate', 'all', 'of', 'their', 'parameters', 'for', 'a', 'given', 'input', ',', 'but', 'rather', 'only', 'those', 'parameters', 'that', 'are', 'helpful', 'in', 'order', 'to', 'handle', 'the', 'input', '.', 'model', 'sparsity', 'thus', 'decouples', 'a', 'model', '’', 's', 'total', 'parameter', 'count', 'from', 'its', 'compute', 'requirements', '.', 'this', 'leads', 'to', 'sparse', 'expert', 'models', '’', 'key', 'advantage', ':', 'they', 'can', 'be', 'both', 'larger', 'and', 'less', 'computationally', 'demanding', 'than', 'dense', 'models', '.', 'why', 'are', 'they', 'called', 'sparse', 'expert', 'models', '?', 'because', 'sparse', 'models', 'can', 'be', 'thought', 'of', 'as', 'consisting', 'of', 'a', 'collection', 'of', '“', 'sub-models', '”', 'that', 'serve', 'as', 'experts', 'on', 'different', 'topics', '.', 'depending', 'on', 'the', 'prompt', 'presented', 'to', 'the', 'model', ',', 'the', 'most', 'relevant', 'experts', 'within', 'the', 'model', 'are', 'activated', 'while', 'the', 'other', 'experts', 'remain', 'inactive', '.', 'a', 'prompt', 'posed', 'in', 'russian', ',', 'for', 'instance', ',', 'would', 'only', 'activate', 'the', '“', 'experts', '”', 'within', 'a', 'model', 'that', 'can', 'understand', 'and', 'respond', 'in', 'russian', ',', 'efficiently', 'bypassing', 'the', 'rest', 'of', 'the', 'model', '.', 'all', 'of', 'today', '’', 's', 'largest', 'llms', 'are', 'sparse', '.', 'if', 'you', 'come', 'across', 'an', 'llm', 'with', 'more', 'than', '1', 'trillion', 'parameters', ',', 'you', 'can', 'safely', 'assume', 'that', 'it', 'is', 'sparse', '.', 'this', 'includes', 'google', '’', 's', 'switch', 'transformer', '(', '1.6', 'trillion', 'parameters', ')', ',', 'google', '’', 's', 'glam', '(', '1.2', 'trillion', 'parameters', ')', 'and', 'meta', '’', 's', 'mixture', 'of', 'experts', 'model', '(', '1.1', 'trillion', 'parameters', ')', '.', '“', 'much', 'of', 'the', 'recent', 'progress', 'in', 'ai', 'has', 'come', 'from', 'training', 'larger', 'and', 'larger', 'models', ',', '”', 'said', 'mikel', 'artetxe', ',', 'who', 'led', 'meta', '’', 's', 'research', 'on', 'sparse', 'models', 'before', 'resigning', 'to', 'cofound', 'a', 'stealth', 'llm', 'startup', '.', '“', 'gpt-3', ',', 'for', 'instance', ',', 'is', 'more', 'than', '100', 'times', 'larger', 'than', 'gpt-2', '.', 'but', 'when', 'we', 'double', 'the', 'size', 'of', 'a', 'dense', 'model', ',', 'we', 'also', 'make', 'it', 'twice', 'as', 'slow', '.', 'sparse', 'models', 'allow', 'us', 'to', 'train', 'larger', 'models', 'without', 'the', 'increase', 'in', 'runtime.', '”', 'recent', 'research', 'on', 'sparse', 'expert', 'models', 'suggests', 'that', 'this', 'architecture', 'holds', 'massive', 'potential', '.', 'glam', ',', 'a', 'sparse', 'expert', 'model', 'developed', 'last', 'year', 'by', 'google', ',', 'is', '7', 'times', 'larger', 'than', 'gpt-3', ',', 'requires', 'two-thirds', 'less', 'energy', 'to', 'train', ',', 'requires', 'half', 'as', 'much', 'compute', 'for', 'inference', ',', 'and', 'outperforms', 'gpt-3', 'on', 'a', 'wide', 'range', 'of', 'natural', 'language', 'tasks', '.', 'similar', 'work', 'on', 'sparse', 'models', 'out', 'of', 'meta', 'has', 'yielded', 'similarly', 'promising', 'results', '.', 'as', 'the', 'meta', 'researchers', 'summarize', ':', '“', 'we', 'find', 'that', 'sparse', 'models', 'can', 'achieve', 'similar', 'downstream', 'task', 'performance', 'as', 'dense', 'models', 'at', 'a', 'fraction', 'of', 'the', 'compute', '.', 'for', 'models', 'with', 'relatively', 'modest', 'compute', 'budgets', ',', 'a', 'sparse', 'model', 'can', 'perform', 'on', 'par', 'with', 'a', 'dense', 'model', 'that', 'requires', 'almost', 'four', 'times', 'as', 'much', 'compute.', '”', 'there', 'is', 'another', 'benefit', 'of', 'sparse', 'expert', 'models', 'that', 'is', 'worth', 'mentioning', ':', 'they', 'are', 'more', 'interpretable', 'than', 'dense', 'models', '.', 'interpretability—the', 'ability', 'for', 'a', 'human', 'to', 'understand', 'why', 'a', 'model', 'took', 'the', 'action', 'that', 'it', 'did—is', 'one', 'of', 'ai', '’', 's', 'greatest', 'weaknesses', 'today', '.', 'in', 'general', ',', 'today', '’', 's', 'neural', 'networks', 'are', 'uninterpretable', '“', 'black', 'boxes.', '”', 'this', 'can', 'limit', 'their', 'usefulness', 'in', 'the', 'real', 'world', ',', 'particularly', 'in', 'high-stakes', 'settings', 'like', 'healthcare', 'where', 'human', 'review', 'is', 'important', '.', 'sparse', 'expert', 'models', 'lend', 'themselves', 'more', 'naturally', 'to', 'interpretability', 'than', 'conventional', 'models', 'because', 'a', 'sparse', 'model', '’', 's', 'output', 'is', 'the', 'result', 'of', 'an', 'identifiable', ',', 'discrete', 'subset', 'of', 'parameters', 'within', 'the', 'model—namely', ',', 'the', '“', 'experts', '”', 'that', 'were', 'activated', '.', 'the', 'fact', 'that', 'humans', 'can', 'better', 'extract', 'understandable', 'explanations', 'from', 'sparse', 'models', 'about', 'their', 'behavior', 'may', 'prove', 'to', 'be', 'a', 'decisive', 'advantage', 'for', 'these', 'models', 'in', 'real-world', 'applications', '.', 'sparse', 'expert', 'models', 'are', 'not', 'in', 'widespread', 'use', 'today', '.', 'they', 'are', 'less', 'well', 'understood', 'and', 'more', 'technically', 'complex', 'to', 'build', 'than', 'dense', 'models', '.', 'yet', 'considering', 'their', 'potential', 'advantages', ',', 'most', 'of', 'all', 'their', 'computational', 'efficiency', ',', 'don', '’', 't', 'be', 'surprised', 'to', 'see', 'the', 'sparse', 'expert', 'architecture', 'become', 'more', 'prevalent', 'in', 'the', 'world', 'of', 'llms', 'going', 'forward', '.', 'in', 'the', 'words', 'of', 'graphcore', 'cto', 'simon', 'knowles', ':', '“', 'if', 'an', 'ai', 'can', 'do', 'many', 'things', ',', 'it', 'doesn', '’', 't', 'need', 'to', 'access', 'all', 'of', 'its', 'knowledge', 'to', 'do', 'one', 'thing', '.', 'it', '’', 's', 'completely', 'obvious', '.', 'this', 'is', 'how', 'your', 'brain', 'works', ',', 'and', 'it', '’', 's', 'also', 'how', 'an', 'ai', 'ought', 'to', 'work', '.', 'i', '’', 'd', 'be', 'surprised', 'if', ',', 'by', 'next', 'year', ',', 'anyone', 'is', 'building', 'dense', 'language', 'models.', '”', 'note', ':', 'the', 'author', 'is', 'a', 'partner', 'at', 'radical', 'ventures', ',', 'which', 'is', 'an', 'investor', 'in', 'you.com', '.']\n"
     ]
    }
   ],
   "source": [
    "# 5.Tokenize text\n",
    "import nltk\n",
    "text = full_text1\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5edad82b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+00A0 (681478045.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    import nltk\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+00A0\n"
     ]
    }
   ],
   "source": [
    "# Task 5\n",
    "# apply POS tagging \n",
    "import nltk \n",
    "from nltk.corpus import state_union \n",
    "from nltk.tokenize import PunktSentenceTokenizer \n",
    "\n",
    "full_text1 = state_union.raw(full_text)\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(full_text1)\n",
    "tokenized = custom_sent_tokenizer.tokenize(full_text1)\n",
    "\n",
    "def process_content():\n",
    "  try:\n",
    "    for i in tokenized:\n",
    "      words = nltk.word_tokenize(i)\n",
    "      tagged = nltk.pos_tag(words)\n",
    "       \n",
    "      print(tagged)\n",
    "     \n",
    "  expect Exception as e:\n",
    "    print(str(e))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b820dd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o\n",
      "p\n",
      "e\n",
      "n\n",
      "a\n",
      "i\n",
      " \n",
      "c\n",
      "e\n",
      "o\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      " \n",
      "a\n",
      "l\n",
      "t\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "(\n",
      "l\n",
      "e\n",
      "f\n",
      "t\n",
      ")\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "a\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "c\n",
      "h\n",
      "i\n",
      "e\n",
      "f\n",
      " \n",
      "y\n",
      "a\n",
      "n\n",
      "n\n",
      " \n",
      "l\n",
      "e\n",
      "c\n",
      "u\n",
      "n\n",
      " \n",
      "(\n",
      "r\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      ")\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "d\n",
      "i\n",
      "f\n",
      "f\n",
      "e\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "v\n",
      "i\n",
      "e\n",
      "w\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "u\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      " \n",
      ".\n",
      ".\n",
      ".\n",
      " \n",
      "[\n",
      "+\n",
      "]\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "n\n",
      " \n",
      "c\n",
      "a\n",
      "s\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      "n\n",
      "’\n",
      "t\n",
      " \n",
      "h\n",
      "e\n",
      "a\n",
      "r\n",
      "d\n",
      ",\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "f\n",
      "i\n",
      "c\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "l\n",
      "l\n",
      "i\n",
      "g\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "h\n",
      "o\n",
      "t\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      ".\n",
      "\n",
      "\n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      "m\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "l\n",
      "i\n",
      "p\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "v\n",
      "e\n",
      "n\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      " \n",
      "c\n",
      "a\n",
      "p\n",
      "i\n",
      "t\n",
      "a\n",
      "l\n",
      "i\n",
      "s\n",
      "t\n",
      ",\n",
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "r\n",
      "e\n",
      "p\n",
      "r\n",
      "e\n",
      "n\n",
      "e\n",
      "u\n",
      "r\n",
      ",\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      "t\n",
      "u\n",
      "n\n",
      "e\n",
      " \n",
      "5\n",
      "0\n",
      "0\n",
      " \n",
      "c\n",
      "e\n",
      "o\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "j\n",
      "o\n",
      "u\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "d\n",
      "a\n",
      "y\n",
      "s\n",
      ",\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "s\n",
      "i\n",
      "l\n",
      "i\n",
      "c\n",
      "o\n",
      "n\n",
      " \n",
      "v\n",
      "a\n",
      "l\n",
      "l\n",
      "e\n",
      "y\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "a\n",
      "v\n",
      "o\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "o\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "r\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "p\n",
      "a\n",
      "y\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "l\n",
      " \n",
      "a\n",
      "t\n",
      "t\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      "m\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "e\n",
      "c\n",
      "h\n",
      "n\n",
      "o\n",
      "l\n",
      "o\n",
      "g\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "d\n",
      "i\n",
      "f\n",
      "f\n",
      "u\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "n\n",
      "o\n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "a\n",
      "k\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "s\n",
      "t\n",
      "o\n",
      "r\n",
      "m\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "d\n",
      "i\n",
      "d\n",
      "n\n",
      "’\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "b\n",
      "a\n",
      "c\n",
      "k\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      ",\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "w\n",
      "r\n",
      "o\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "l\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "l\n",
      "u\n",
      "m\n",
      "n\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "d\n",
      "i\n",
      "c\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "w\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "i\n",
      "l\n",
      "l\n",
      "a\n",
      "r\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "x\n",
      "t\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "f\n",
      "i\n",
      "c\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "l\n",
      "l\n",
      "i\n",
      "g\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "s\n",
      "i\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "g\n",
      "p\n",
      "t\n",
      "-\n",
      "2\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "2\n",
      "0\n",
      "1\n",
      "9\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "b\n",
      "e\n",
      "e\n",
      "n\n",
      " \n",
      "c\n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "i\n",
      "e\n",
      "l\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "p\n",
      "o\n",
      "i\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "u\n",
      "n\n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "h\n",
      " \n",
      "v\n",
      "a\n",
      "s\n",
      "t\n",
      " \n",
      "e\n",
      "c\n",
      "o\n",
      "n\n",
      "o\n",
      "m\n",
      "i\n",
      "c\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "s\n",
      "o\n",
      "c\n",
      "i\n",
      "e\n",
      "t\n",
      "a\n",
      "l\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "n\n",
      "s\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "s\n",
      "i\n",
      "m\n",
      "i\n",
      "l\n",
      "a\n",
      "r\n",
      "l\n",
      "y\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "e\n",
      "x\n",
      "t\n",
      "-\n",
      "t\n",
      "o\n",
      "-\n",
      "i\n",
      "m\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "c\n",
      "a\n",
      "p\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "u\n",
      "b\n",
      "l\n",
      "i\n",
      "c\n",
      "’\n",
      "s\n",
      " \n",
      "a\n",
      "t\n",
      "t\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "l\n",
      "a\n",
      "s\n",
      "t\n",
      " \n",
      "s\n",
      "u\n",
      "m\n",
      "m\n",
      "e\n",
      "r\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "e\n",
      "c\n",
      "h\n",
      "n\n",
      "o\n",
      "l\n",
      "o\n",
      "g\n",
      "y\n",
      "’\n",
      "s\n",
      " \n",
      "a\n",
      "s\n",
      "c\n",
      "e\n",
      "n\n",
      "d\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "a\n",
      "p\n",
      "p\n",
      "e\n",
      "a\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      "e\n",
      "v\n",
      "i\n",
      "t\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "o\n",
      "p\n",
      "e\n",
      "n\n",
      "a\n",
      "i\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "o\n",
      "r\n",
      "i\n",
      "g\n",
      "i\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "d\n",
      "a\n",
      "l\n",
      "l\n",
      "-\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "j\n",
      "a\n",
      "n\n",
      "u\n",
      "a\n",
      "r\n",
      "y\n",
      " \n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      ".\n",
      " \n",
      "(\n",
      "w\n",
      "e\n",
      " \n",
      "w\n",
      "r\n",
      "o\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "l\n",
      "e\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "g\n",
      "u\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "d\n",
      "a\n",
      "y\n",
      "s\n",
      " \n",
      "a\n",
      "f\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "o\n",
      "r\n",
      "i\n",
      "g\n",
      "i\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "d\n",
      "a\n",
      "l\n",
      "l\n",
      "-\n",
      "e\n",
      ".\n",
      ")\n",
      "\n",
      "\n",
      "b\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "k\n",
      "e\n",
      "n\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "m\n",
      "e\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "u\n",
      "r\n",
      "r\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "f\n",
      "a\n",
      "r\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "e\n",
      "n\n",
      "d\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "a\n",
      "i\n",
      "’\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "p\n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "i\n",
      "e\n",
      "s\n",
      ".\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "r\n",
      "a\n",
      "r\n",
      "y\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "n\n",
      "t\n",
      "i\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "f\n",
      "i\n",
      "c\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "l\n",
      "l\n",
      "i\n",
      "g\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      " \n",
      "a\n",
      "d\n",
      "v\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      "d\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "r\n",
      "a\n",
      "p\n",
      "i\n",
      "d\n",
      "l\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "r\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      "w\n",
      ".\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "a\n",
      "m\n",
      "a\n",
      "z\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      "m\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "u\n",
      "s\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "s\n",
      "t\n",
      "e\n",
      "p\n",
      "p\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "s\n",
      "t\n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      "s\n",
      " \n",
      "n\n",
      "e\n",
      "x\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "x\n",
      "t\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "(\n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      ")\n",
      " \n",
      "l\n",
      "o\n",
      "o\n",
      "k\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      "?\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "s\n",
      "w\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "q\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "r\n",
      "e\n",
      "a\n",
      "d\n",
      "y\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      ",\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "d\n",
      "e\n",
      "v\n",
      "e\n",
      "l\n",
      "o\n",
      "p\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "r\n",
      "t\n",
      "u\n",
      "p\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "g\n",
      "r\n",
      "o\n",
      "u\n",
      "p\n",
      "s\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "m\n",
      "o\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "l\n",
      "e\n",
      " \n",
      "h\n",
      "i\n",
      "g\n",
      "h\n",
      "l\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "r\n",
      "e\n",
      "e\n",
      " \n",
      "e\n",
      "m\n",
      "e\n",
      "r\n",
      "g\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "h\n",
      "e\n",
      "l\n",
      "p\n",
      " \n",
      "d\n",
      "e\n",
      "f\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "x\n",
      "t\n",
      " \n",
      "w\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "n\n",
      "n\n",
      "o\n",
      "v\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      ".\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "l\n",
      "o\n",
      "o\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "m\n",
      "a\n",
      "i\n",
      "n\n",
      " \n",
      "a\n",
      "h\n",
      "e\n",
      "a\n",
      "d\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "u\n",
      "r\n",
      "v\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "f\n",
      "a\n",
      "s\n",
      "t\n",
      "-\n",
      "c\n",
      "h\n",
      "a\n",
      "n\n",
      "g\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "—\n",
      "r\n",
      "e\n",
      "a\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "c\n",
      "o\n",
      "n\n",
      "s\n",
      "i\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "h\n",
      "o\n",
      "w\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "k\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      ".\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "l\n",
      "l\n",
      "e\n",
      "c\n",
      "t\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "l\n",
      "e\n",
      "d\n",
      "g\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "s\n",
      "p\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "e\n",
      "x\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "—\n",
      "s\n",
      "a\n",
      "y\n",
      ",\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      " \n",
      "b\n",
      "o\n",
      "o\n",
      "k\n",
      ".\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "a\n",
      "l\n",
      "s\n",
      "o\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "n\n",
      "o\n",
      "v\n",
      "e\n",
      "l\n",
      " \n",
      "i\n",
      "d\n",
      "e\n",
      "a\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      ",\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "f\n",
      "l\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      " \n",
      "t\n",
      "o\n",
      "p\n",
      "i\n",
      "c\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "r\n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      " \n",
      "a\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "b\n",
      "l\n",
      "e\n",
      "m\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      "s\n",
      ".\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      "e\n",
      "n\n",
      " \n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "v\n",
      "i\n",
      "a\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "r\n",
      "e\n",
      "f\n",
      "l\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "a\n",
      "l\n",
      "y\n",
      "s\n",
      "i\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "d\n",
      "i\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "t\n",
      "i\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      "n\n",
      "y\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "e\n",
      "x\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      "p\n",
      "u\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "a\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "a\n",
      "v\n",
      "e\n",
      "n\n",
      "u\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      "k\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "e\n",
      "n\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "o\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "n\n",
      "a\n",
      "l\n",
      "o\n",
      "g\n",
      "o\n",
      "u\n",
      "s\n",
      ",\n",
      " \n",
      "e\n",
      "f\n",
      "f\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "b\n",
      "o\n",
      "o\n",
      "t\n",
      "s\n",
      "t\n",
      "r\n",
      "a\n",
      "p\n",
      "p\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "l\n",
      "l\n",
      "i\n",
      "g\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "a\n",
      "s\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      "’\n",
      "s\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      "g\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "’\n",
      "s\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "m\n",
      "u\n",
      "l\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "w\n",
      "r\n",
      "i\n",
      "t\n",
      "t\n",
      "e\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "(\n",
      "e\n",
      ".\n",
      "g\n",
      ".\n",
      ",\n",
      " \n",
      "w\n",
      "i\n",
      "k\n",
      "i\n",
      "p\n",
      "e\n",
      "d\n",
      "i\n",
      "a\n",
      ",\n",
      " \n",
      "b\n",
      "o\n",
      "o\n",
      "k\n",
      "s\n",
      ",\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "l\n",
      "e\n",
      "s\n",
      ")\n",
      ".\n",
      " \n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ",\n",
      " \n",
      "o\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "e\n",
      "d\n",
      ",\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "l\n",
      "e\n",
      "d\n",
      "g\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "s\n",
      "o\n",
      "r\n",
      "b\n",
      "e\n",
      "d\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "d\n",
      "u\n",
      "c\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "w\n",
      "r\n",
      "i\n",
      "t\n",
      "t\n",
      "e\n",
      "n\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "e\n",
      "n\n",
      "t\n",
      "—\n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "a\n",
      "d\n",
      "d\n",
      "i\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "r\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "m\n",
      "s\n",
      "e\n",
      "l\n",
      "v\n",
      "e\n",
      "s\n",
      "?\n",
      " \n",
      "i\n",
      "n\n",
      "i\n",
      "t\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      " \n",
      "s\n",
      "u\n",
      "g\n",
      "g\n",
      "e\n",
      "s\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "p\n",
      "p\n",
      "r\n",
      "o\n",
      "a\n",
      "c\n",
      "h\n",
      " \n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "p\n",
      "o\n",
      "s\n",
      "s\n",
      "i\n",
      "b\n",
      "l\n",
      "e\n",
      "—\n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "p\n",
      "o\n",
      "w\n",
      "e\n",
      "r\n",
      "f\n",
      "u\n",
      "l\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "e\n",
      "f\n",
      "f\n",
      "o\n",
      "r\n",
      "t\n",
      ",\n",
      " \n",
      "a\n",
      "p\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "t\n",
      "i\n",
      "t\n",
      "l\n",
      "e\n",
      "d\n",
      " \n",
      "“\n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      "-\n",
      "i\n",
      "m\n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      ",\n",
      "”\n",
      " \n",
      "a\n",
      " \n",
      "g\n",
      "r\n",
      "o\n",
      "u\n",
      "p\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "t\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "u\n",
      "p\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "e\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "q\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      ",\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "t\n",
      "a\n",
      "i\n",
      "l\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "s\n",
      "w\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "q\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      ",\n",
      " \n",
      "f\n",
      "i\n",
      "l\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "a\n",
      "n\n",
      "s\n",
      "w\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "h\n",
      "i\n",
      "g\n",
      "h\n",
      "-\n",
      "q\n",
      "u\n",
      "a\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      "p\n",
      "u\n",
      "t\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "e\n",
      "-\n",
      "t\n",
      "u\n",
      "n\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "s\n",
      "w\n",
      "e\n",
      "r\n",
      "s\n",
      ".\n",
      " \n",
      "r\n",
      "e\n",
      "m\n",
      "a\n",
      "r\n",
      "k\n",
      "a\n",
      "b\n",
      "l\n",
      "y\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "d\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "t\n",
      "e\n",
      "-\n",
      "o\n",
      "f\n",
      "-\n",
      "t\n",
      "h\n",
      "e\n",
      "-\n",
      "a\n",
      "r\n",
      "t\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "v\n",
      "a\n",
      "r\n",
      "i\n",
      "o\n",
      "u\n",
      "s\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "s\n",
      "k\n",
      "s\n",
      ".\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "’\n",
      "s\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "7\n",
      "4\n",
      ".\n",
      "2\n",
      "%\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "8\n",
      "2\n",
      ".\n",
      "1\n",
      "%\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "g\n",
      "s\n",
      "m\n",
      "8\n",
      "k\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "7\n",
      "8\n",
      ".\n",
      "2\n",
      "%\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "8\n",
      "3\n",
      ".\n",
      "0\n",
      "%\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "d\n",
      "r\n",
      "o\n",
      "p\n",
      ",\n",
      " \n",
      "t\n",
      "w\n",
      "o\n",
      " \n",
      "p\n",
      "o\n",
      "p\n",
      "u\n",
      "l\n",
      "a\n",
      "r\n",
      " \n",
      "b\n",
      "e\n",
      "n\n",
      "c\n",
      "h\n",
      "m\n",
      "a\n",
      "r\n",
      "k\n",
      "s\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "e\n",
      "v\n",
      "a\n",
      "l\n",
      "u\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "a\n",
      "n\n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "d\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "h\n",
      "o\n",
      "d\n",
      " \n",
      "c\n",
      "a\n",
      "l\n",
      "l\n",
      "e\n",
      "d\n",
      " \n",
      "“\n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "r\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "e\n",
      "-\n",
      "t\n",
      "u\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      "”\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "c\n",
      "h\n",
      " \n",
      "l\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "d\n",
      "u\n",
      "c\n",
      "t\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      ".\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "r\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "e\n",
      "-\n",
      "t\n",
      "u\n",
      "n\n",
      "e\n",
      "d\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      "-\n",
      "w\n",
      "r\n",
      "i\n",
      "t\n",
      "t\n",
      "e\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "r\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "g\n",
      "r\n",
      "o\n",
      "u\n",
      "p\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "n\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "a\n",
      "l\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "r\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "e\n",
      "-\n",
      "t\n",
      "u\n",
      "n\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "r\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "g\n",
      "a\n",
      "i\n",
      "n\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "d\n",
      "r\n",
      "a\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "c\n",
      ":\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "h\n",
      "o\n",
      "d\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "b\n",
      "a\n",
      "s\n",
      "e\n",
      " \n",
      "g\n",
      "p\n",
      "t\n",
      "-\n",
      "3\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "3\n",
      "3\n",
      "%\n",
      ",\n",
      " \n",
      "n\n",
      "e\n",
      "a\n",
      "r\n",
      "l\n",
      "y\n",
      " \n",
      "m\n",
      "a\n",
      "t\n",
      "c\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "o\n",
      "p\n",
      "e\n",
      "n\n",
      "a\n",
      "i\n",
      "’\n",
      "s\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "r\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "-\n",
      "t\n",
      "u\n",
      "n\n",
      "e\n",
      "d\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "n\n",
      " \n",
      "a\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "c\n",
      "a\n",
      "l\n",
      "l\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      ",\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "c\n",
      "a\n",
      "r\n",
      "n\n",
      "e\n",
      "g\n",
      "i\n",
      "e\n",
      " \n",
      "m\n",
      "e\n",
      "l\n",
      "l\n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "w\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "f\n",
      " \n",
      "a\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "n\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "a\n",
      " \n",
      "q\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ",\n",
      " \n",
      "f\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "i\n",
      "t\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "s\n",
      " \n",
      "a\n",
      "b\n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "p\n",
      "i\n",
      "c\n",
      " \n",
      "b\n",
      "e\n",
      "f\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      "s\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "s\n",
      "o\n",
      "p\n",
      "h\n",
      "i\n",
      "s\n",
      "t\n",
      "i\n",
      "c\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "s\n",
      "e\n",
      "s\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "l\n",
      "o\n",
      "o\n",
      "s\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "a\n",
      "l\n",
      "o\n",
      "g\n",
      "i\n",
      "z\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "w\n",
      "h\n",
      "o\n",
      ",\n",
      " \n",
      "r\n",
      "a\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "b\n",
      "l\n",
      "u\n",
      "r\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      " \n",
      "t\n",
      "o\n",
      "p\n",
      "i\n",
      "c\n",
      ",\n",
      " \n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "e\n",
      "s\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "m\n",
      "e\n",
      "m\n",
      "o\n",
      "r\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "f\n",
      "l\n",
      "e\n",
      "c\n",
      "t\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "b\n",
      "e\n",
      "l\n",
      "i\n",
      "e\n",
      "f\n",
      "s\n",
      " \n",
      "b\n",
      "e\n",
      "f\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "s\n",
      "h\n",
      "a\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "s\n",
      "p\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "w\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "p\n",
      "e\n",
      "o\n",
      "p\n",
      "l\n",
      "e\n",
      " \n",
      "f\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "h\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "a\n",
      "b\n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      ",\n",
      " \n",
      "a\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "c\n",
      "e\n",
      "p\n",
      "t\n",
      "u\n",
      "a\n",
      "l\n",
      " \n",
      "o\n",
      "b\n",
      "j\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "f\n",
      "t\n",
      "e\n",
      "n\n",
      " \n",
      "a\n",
      "r\n",
      "i\n",
      "s\n",
      "e\n",
      "s\n",
      "—\n",
      "i\n",
      "s\n",
      "n\n",
      "’\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "c\n",
      "i\n",
      "r\n",
      "c\n",
      "u\n",
      "l\n",
      "a\n",
      "r\n",
      "?\n",
      " \n",
      "h\n",
      "o\n",
      "w\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "d\n",
      "u\n",
      "c\n",
      "e\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "s\n",
      "u\n",
      "m\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      "?\n",
      " \n",
      "i\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "p\n",
      "l\n",
      "a\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "n\n",
      "’\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "“\n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "l\n",
      "e\n",
      "d\n",
      "g\n",
      "e\n",
      "”\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "“\n",
      "s\n",
      "i\n",
      "g\n",
      "n\n",
      "a\n",
      "l\n",
      "”\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "a\n",
      "i\n",
      "n\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "r\n",
      "e\n",
      "a\n",
      "d\n",
      "y\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "o\n",
      "r\n",
      "p\n",
      "o\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "?\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "o\n",
      "b\n",
      "j\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "e\n",
      "s\n",
      " \n",
      "s\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "f\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "c\n",
      "e\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      "b\n",
      "a\n",
      "s\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "s\n",
      "t\n",
      "o\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "p\n",
      "r\n",
      "o\n",
      "d\n",
      "u\n",
      "c\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "d\n",
      "i\n",
      "f\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "b\n",
      "i\n",
      "n\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "p\n",
      "t\n",
      "e\n",
      "d\n",
      ".\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      "—\n",
      "u\n",
      "n\n",
      "c\n",
      "o\n",
      "m\n",
      "f\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "n\n",
      " \n",
      "e\n",
      "e\n",
      "r\n",
      "i\n",
      "e\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "n\n",
      "d\n",
      "—\n",
      "w\n",
      "e\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "b\n",
      "e\n",
      "t\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "o\n",
      "f\n",
      "f\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "e\n",
      "a\n",
      "d\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "c\n",
      "e\n",
      "i\n",
      "v\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "o\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "l\n",
      "i\n",
      "n\n",
      "e\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "b\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      " \n",
      "(\n",
      "n\n",
      "o\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "a\n",
      "l\n",
      "o\n",
      "g\n",
      "y\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "e\n",
      "c\n",
      "t\n",
      "!\n",
      ")\n",
      ".\n",
      "\n",
      "\n",
      "w\n",
      "e\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      "g\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "t\n",
      "r\n",
      "e\n",
      "m\n",
      "e\n",
      "n\n",
      "d\n",
      "o\n",
      "u\n",
      "s\n",
      " \n",
      "a\n",
      "m\n",
      "o\n",
      "u\n",
      "n\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "l\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "u\n",
      "r\n",
      "a\n",
      "l\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "n\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "b\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      ",\n",
      " \n",
      "i\n",
      "n\n",
      "n\n",
      "u\n",
      "m\n",
      "e\n",
      "r\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "w\n",
      "a\n",
      "y\n",
      "s\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "r\n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "r\n",
      "o\n",
      "s\n",
      "p\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ",\n",
      " \n",
      "w\n",
      "r\n",
      "i\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "—\n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      "s\n",
      " \n",
      "j\n",
      "u\n",
      "s\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "d\n",
      " \n",
      "n\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "s\n",
      "l\n",
      "e\n",
      "e\n",
      "p\n",
      "—\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "b\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "d\n",
      "u\n",
      "c\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "h\n",
      "a\n",
      "d\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "v\n",
      "i\n",
      "o\n",
      "u\n",
      "s\n",
      "l\n",
      "y\n",
      " \n",
      "b\n",
      "e\n",
      "e\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "a\n",
      "n\n",
      "y\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      ".\n",
      " \n",
      "i\n",
      "f\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "z\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "s\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "e\n",
      " \n",
      "u\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "s\n",
      "e\n",
      "r\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "d\n",
      "e\n",
      "a\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "u\n",
      "l\n",
      "a\n",
      "r\n",
      "l\n",
      "y\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "l\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "a\n",
      "c\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "s\n",
      "o\n",
      "o\n",
      "n\n",
      " \n",
      "r\n",
      "u\n",
      "n\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "e\n",
      "x\n",
      "t\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "y\n",
      "e\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "w\n",
      "i\n",
      "d\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "p\n",
      "p\n",
      "r\n",
      "e\n",
      "c\n",
      "i\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "b\n",
      "l\n",
      "e\n",
      "m\n",
      ",\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "m\n",
      "a\n",
      "n\n",
      "y\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "r\n",
      "i\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "b\n",
      "o\n",
      "u\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "b\n",
      "y\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "m\n",
      "a\n",
      "t\n",
      "e\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "’\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      "t\n",
      "a\n",
      "l\n",
      " \n",
      "s\n",
      "t\n",
      "o\n",
      "c\n",
      "k\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "u\n",
      "s\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "e\n",
      "x\n",
      "t\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "b\n",
      "e\n",
      "t\n",
      "w\n",
      "e\n",
      "e\n",
      "n\n",
      " \n",
      "4\n",
      ".\n",
      "6\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "1\n",
      "7\n",
      ".\n",
      "2\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "o\n",
      "k\n",
      "e\n",
      "n\n",
      "s\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "l\n",
      "u\n",
      "d\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "’\n",
      "s\n",
      " \n",
      "b\n",
      "o\n",
      "o\n",
      "k\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "s\n",
      "c\n",
      "i\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "f\n",
      "i\n",
      "c\n",
      " \n",
      "p\n",
      "a\n",
      "p\n",
      "e\n",
      "r\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "l\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "w\n",
      "i\n",
      "k\n",
      "i\n",
      "p\n",
      "e\n",
      "d\n",
      "i\n",
      "a\n",
      ",\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "p\n",
      "u\n",
      "b\n",
      "l\n",
      "i\n",
      "c\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "v\n",
      "a\n",
      "i\n",
      "l\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "d\n",
      "e\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "e\n",
      "t\n",
      ",\n",
      " \n",
      "f\n",
      "i\n",
      "l\n",
      "t\n",
      "e\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "q\n",
      "u\n",
      "a\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "(\n",
      "e\n",
      ".\n",
      "g\n",
      ".\n",
      ",\n",
      " \n",
      "w\n",
      "e\n",
      "b\n",
      "p\n",
      "a\n",
      "g\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "b\n",
      "l\n",
      "o\n",
      "g\n",
      "s\n",
      ",\n",
      " \n",
      "s\n",
      "o\n",
      "c\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "m\n",
      "e\n",
      "d\n",
      "i\n",
      "a\n",
      ")\n",
      ".\n",
      " \n",
      "a\n",
      "n\n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "m\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "p\n",
      "u\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "t\n",
      "a\n",
      "l\n",
      " \n",
      "f\n",
      "i\n",
      "g\n",
      "u\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "3\n",
      ".\n",
      "2\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "o\n",
      "k\n",
      "e\n",
      "n\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      "’\n",
      "s\n",
      " \n",
      "c\n",
      "h\n",
      "i\n",
      "n\n",
      "c\n",
      "h\n",
      "i\n",
      "l\n",
      "l\n",
      "a\n",
      ",\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      "’\n",
      "s\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      ",\n",
      " \n",
      "w\n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "e\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "1\n",
      ".\n",
      "4\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "o\n",
      "k\n",
      "e\n",
      "n\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "d\n",
      "s\n",
      ",\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "w\n",
      "e\n",
      "l\n",
      "l\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "r\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "m\n",
      "a\n",
      "g\n",
      "n\n",
      "i\n",
      "t\n",
      "u\n",
      "d\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "e\n",
      "x\n",
      "h\n",
      "a\n",
      "u\n",
      "s\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "’\n",
      "s\n",
      " \n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "r\n",
      "e\n",
      " \n",
      "s\n",
      "u\n",
      "p\n",
      "p\n",
      "l\n",
      "y\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "f\n",
      "u\n",
      "l\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "f\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "i\n",
      "n\n",
      "u\n",
      "e\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      "-\n",
      "i\n",
      "m\n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "i\n",
      "r\n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "v\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "l\n",
      "o\n",
      "o\n",
      "m\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "g\n",
      "e\n",
      ".\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "w\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "p\n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      "-\n",
      "b\n",
      "e\n",
      "n\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "p\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      "w\n",
      "a\n",
      "r\n",
      "d\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a\n",
      " \n",
      "p\n",
      "o\n",
      "p\n",
      "u\n",
      "l\n",
      "a\n",
      "r\n",
      " \n",
      "n\n",
      "a\n",
      "r\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "d\n",
      "a\n",
      "y\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "v\n",
      "e\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "r\n",
      "e\n",
      "p\n",
      "l\n",
      "a\n",
      "c\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      " \n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      "’\n",
      "s\n",
      " \n",
      "g\n",
      "o\n",
      "-\n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ",\n",
      " \n",
      "d\n",
      "i\n",
      "s\n",
      "r\n",
      "u\n",
      "p\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "o\n",
      "n\n",
      "c\n",
      "e\n",
      "-\n",
      "m\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "y\n",
      " \n",
      "t\n",
      "e\n",
      "c\n",
      "h\n",
      " \n",
      "g\n",
      "i\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "b\n",
      "l\n",
      "o\n",
      "c\n",
      "k\n",
      "b\n",
      "u\n",
      "s\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "k\n",
      "o\n",
      "d\n",
      "a\n",
      "k\n",
      " \n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "d\n",
      "i\n",
      "s\n",
      "r\n",
      "u\n",
      "p\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "b\n",
      "e\n",
      "f\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "n\n",
      "a\n",
      "r\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "b\n",
      "a\n",
      "d\n",
      "l\n",
      "y\n",
      " \n",
      "o\n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "i\n",
      "m\n",
      "p\n",
      "l\n",
      "i\n",
      "f\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      "s\n",
      ".\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "e\n",
      "x\n",
      "i\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "n\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      " \n",
      "r\n",
      "e\n",
      "p\n",
      "l\n",
      "a\n",
      "c\n",
      "e\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      " \n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      ".\n",
      " \n",
      "w\n",
      "h\n",
      "y\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      "?\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "r\n",
      "t\n",
      ",\n",
      " \n",
      "b\n",
      "e\n",
      "c\n",
      "a\n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      "’\n",
      "s\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "e\n",
      " \n",
      "s\n",
      "t\n",
      "u\n",
      "f\n",
      "f\n",
      " \n",
      "u\n",
      "p\n",
      ".\n",
      "\n",
      "\n",
      "a\n",
      "s\n",
      " \n",
      "p\n",
      "o\n",
      "w\n",
      "e\n",
      "r\n",
      "f\n",
      "u\n",
      "l\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      ",\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "g\n",
      "u\n",
      "l\n",
      "a\n",
      "r\n",
      "l\n",
      "y\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "d\n",
      "u\n",
      "c\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      ",\n",
      " \n",
      "m\n",
      "i\n",
      "s\n",
      "l\n",
      "e\n",
      "a\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "f\n",
      "a\n",
      "l\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "(\n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "f\n",
      "i\n",
      "d\n",
      "e\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "v\n",
      "i\n",
      "n\n",
      "c\n",
      "i\n",
      "n\n",
      "g\n",
      "l\n",
      "y\n",
      ")\n",
      ".\n",
      "\n",
      "\n",
      "e\n",
      "x\n",
      "a\n",
      "m\n",
      "p\n",
      "l\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "b\n",
      "o\n",
      "u\n",
      "n\n",
      "d\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "“\n",
      "h\n",
      "a\n",
      "l\n",
      "l\n",
      "u\n",
      "c\n",
      "i\n",
      "n\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      "”\n",
      " \n",
      "(\n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "i\n",
      "s\n",
      "s\n",
      "t\n",
      "a\n",
      "t\n",
      "e\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "f\n",
      "e\n",
      "r\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      ")\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      "l\n",
      "e\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      ";\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "e\n",
      "x\n",
      "i\n",
      "s\n",
      "t\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      " \n",
      "h\n",
      "a\n",
      "l\n",
      "l\n",
      "u\n",
      "c\n",
      "i\n",
      "n\n",
      "a\n",
      "t\n",
      "e\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "s\n",
      "i\n",
      "m\n",
      "i\n",
      "l\n",
      "a\n",
      "r\n",
      " \n",
      "w\n",
      "a\n",
      "y\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "o\n",
      " \n",
      "g\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      " \n",
      "f\n",
      "e\n",
      "w\n",
      " \n",
      "e\n",
      "x\n",
      "a\n",
      "m\n",
      "p\n",
      "l\n",
      "e\n",
      "s\n",
      ":\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "o\n",
      "m\n",
      "m\n",
      "e\n",
      "n\n",
      "d\n",
      "s\n",
      " \n",
      "b\n",
      "o\n",
      "o\n",
      "k\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "d\n",
      "o\n",
      "n\n",
      "’\n",
      "t\n",
      " \n",
      "e\n",
      "x\n",
      "i\n",
      "s\n",
      "t\n",
      ";\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "i\n",
      "s\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "n\n",
      "u\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      " \n",
      "2\n",
      "2\n",
      "0\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "l\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "2\n",
      "0\n",
      "0\n",
      ";\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "u\n",
      "n\n",
      "s\n",
      "u\n",
      "r\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "a\n",
      "b\n",
      "r\n",
      "a\n",
      "h\n",
      "a\n",
      "m\n",
      " \n",
      "l\n",
      "i\n",
      "n\n",
      "c\n",
      "o\n",
      "l\n",
      "n\n",
      "’\n",
      "s\n",
      " \n",
      "a\n",
      "s\n",
      "s\n",
      "a\n",
      "s\n",
      "s\n",
      "i\n",
      "n\n",
      " \n",
      "w\n",
      "a\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "i\n",
      "n\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "n\n",
      "c\n",
      "o\n",
      "l\n",
      "n\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "s\n",
      "s\n",
      "a\n",
      "s\n",
      "s\n",
      "i\n",
      "n\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ";\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      "s\n",
      " \n",
      "p\n",
      "l\n",
      "a\n",
      "u\n",
      "s\n",
      "i\n",
      "b\n",
      "l\n",
      "e\n",
      "-\n",
      "s\n",
      "o\n",
      "u\n",
      "n\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "o\n",
      "r\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "l\n",
      "a\n",
      "n\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "c\n",
      "e\n",
      "p\n",
      "t\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "b\n",
      "a\n",
      "y\n",
      "e\n",
      "s\n",
      "’\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "o\n",
      "r\n",
      "e\n",
      "m\n",
      ".\n",
      "\n",
      "\n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "e\n",
      "p\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "e\n",
      "n\n",
      "g\n",
      "i\n",
      "n\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "g\n",
      "e\n",
      "t\n",
      "s\n",
      " \n",
      "b\n",
      "a\n",
      "s\n",
      "i\n",
      "c\n",
      " \n",
      "f\n",
      "a\n",
      "c\n",
      "t\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "w\n",
      "r\n",
      "o\n",
      "n\n",
      "g\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      ";\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "n\n",
      " \n",
      "9\n",
      "9\n",
      "%\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "c\n",
      "y\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "d\n",
      " \n",
      "e\n",
      "n\n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "b\n",
      "r\n",
      "o\n",
      "a\n",
      "d\n",
      " \n",
      "m\n",
      "a\n",
      "r\n",
      "k\n",
      "e\n",
      "t\n",
      " \n",
      "a\n",
      "d\n",
      "o\n",
      "p\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "o\n",
      "p\n",
      "e\n",
      "n\n",
      "a\n",
      "i\n",
      " \n",
      "c\n",
      "e\n",
      "o\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      " \n",
      "a\n",
      "l\n",
      "t\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "h\n",
      "i\n",
      "m\n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "a\n",
      "c\n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "l\n",
      "e\n",
      "d\n",
      "g\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      ",\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "c\n",
      "a\n",
      "u\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      ":\n",
      " \n",
      "“\n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "d\n",
      "i\n",
      "b\n",
      "l\n",
      "y\n",
      " \n",
      "l\n",
      "i\n",
      "m\n",
      "i\n",
      "t\n",
      "e\n",
      "d\n",
      ",\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "d\n",
      " \n",
      "e\n",
      "n\n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "s\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "i\n",
      "s\n",
      "l\n",
      "e\n",
      "a\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "r\n",
      "e\n",
      "s\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "g\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "n\n",
      "e\n",
      "s\n",
      "s\n",
      ".\n",
      " \n",
      "i\n",
      "t\n",
      "'\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "i\n",
      "s\n",
      "t\n",
      "a\n",
      "k\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "y\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "a\n",
      "n\n",
      "y\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "r\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "n\n",
      "o\n",
      "w\n",
      ".\n",
      "”\n",
      "\n",
      "\n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "o\n",
      "p\n",
      "e\n",
      "n\n",
      " \n",
      "q\n",
      "u\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      "’\n",
      " \n",
      "h\n",
      "a\n",
      "l\n",
      "l\n",
      "u\n",
      "c\n",
      "i\n",
      "n\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "b\n",
      "l\n",
      "e\n",
      "m\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "s\n",
      "o\n",
      "l\n",
      "v\n",
      "e\n",
      "d\n",
      " \n",
      "v\n",
      "i\n",
      "a\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "a\n",
      "l\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "e\n",
      "x\n",
      "i\n",
      "s\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "i\n",
      "t\n",
      "e\n",
      "c\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "f\n",
      "u\n",
      "n\n",
      "d\n",
      "a\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "a\n",
      "l\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "d\n",
      "i\n",
      "g\n",
      "m\n",
      " \n",
      "s\n",
      "h\n",
      "i\n",
      "f\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "h\n",
      "o\n",
      "d\n",
      "o\n",
      "l\n",
      "o\n",
      "g\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "c\n",
      "e\n",
      "s\n",
      "s\n",
      "a\n",
      "r\n",
      "y\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "g\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "m\n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "l\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      ".\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "r\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "p\n",
      "i\n",
      "o\n",
      "n\n",
      "e\n",
      "e\n",
      "r\n",
      " \n",
      "y\n",
      "a\n",
      "n\n",
      "n\n",
      " \n",
      "l\n",
      "e\n",
      "c\n",
      "u\n",
      "n\n",
      ",\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      ",\n",
      " \n",
      "b\n",
      "e\n",
      "l\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "t\n",
      "t\n",
      "e\n",
      "r\n",
      ".\n",
      " \n",
      "l\n",
      "e\n",
      "c\n",
      "u\n",
      "n\n",
      "’\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "r\n",
      "a\n",
      "r\n",
      "i\n",
      "a\n",
      "n\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "s\n",
      "p\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "r\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ";\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "t\n",
      "e\n",
      "l\n",
      "l\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "a\n",
      "r\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "e\n",
      "r\n",
      "m\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      ",\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "e\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "i\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "n\n",
      "n\n",
      "o\n",
      "v\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      "f\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "t\n",
      " \n",
      "m\n",
      "i\n",
      "t\n",
      "i\n",
      "g\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      "’\n",
      " \n",
      "f\n",
      "a\n",
      "c\n",
      "t\n",
      "u\n",
      "a\n",
      "l\n",
      " \n",
      "u\n",
      "n\n",
      "r\n",
      "e\n",
      "l\n",
      "i\n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "h\n",
      "o\n",
      "d\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "p\n",
      "l\n",
      "a\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "e\n",
      "s\n",
      "s\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "r\n",
      "o\n",
      "l\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "p\n",
      "a\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "w\n",
      "i\n",
      "d\n",
      "e\n",
      "s\n",
      "p\n",
      "r\n",
      "e\n",
      "a\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "l\n",
      "-\n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "d\n",
      "e\n",
      "p\n",
      "l\n",
      "o\n",
      "y\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "w\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "c\n",
      "a\n",
      "p\n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "e\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "h\n",
      "e\n",
      "a\n",
      "r\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "c\n",
      "u\n",
      "r\n",
      "r\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "e\n",
      "f\n",
      "f\n",
      "o\n",
      "r\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      ":\n",
      " \n",
      "(\n",
      "1\n",
      ")\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "t\n",
      "r\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "e\n",
      "x\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "(\n",
      "2\n",
      ")\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "c\n",
      "i\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "m\n",
      "i\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "r\n",
      "e\n",
      "a\n",
      "d\n",
      "y\n",
      " \n",
      "s\n",
      "t\n",
      "o\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "t\n",
      ",\n",
      " \n",
      "c\n",
      "a\n",
      "p\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "c\n",
      " \n",
      "w\n",
      "e\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "s\n",
      ".\n",
      " \n",
      "(\n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "w\n",
      "h\n",
      "y\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "i\n",
      "s\n",
      "c\n",
      "u\n",
      "s\n",
      "s\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "n\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "o\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "f\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "w\n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "e\n",
      "d\n",
      ".\n",
      ")\n",
      " \n",
      "b\n",
      "e\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "p\n",
      "u\n",
      "l\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "e\n",
      "x\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "e\n",
      "m\n",
      "p\n",
      "o\n",
      "w\n",
      "e\n",
      "r\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "u\n",
      "p\n",
      "-\n",
      "t\n",
      "o\n",
      "-\n",
      "d\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      "v\n",
      "a\n",
      "i\n",
      "l\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      ",\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "n\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "n\n",
      "g\n",
      "e\n",
      "s\n",
      " \n",
      "f\n",
      "r\n",
      "e\n",
      "q\n",
      "u\n",
      "e\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "(\n",
      "s\n",
      "a\n",
      "y\n",
      ",\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "a\n",
      "n\n",
      "i\n",
      "e\n",
      "s\n",
      "’\n",
      " \n",
      "s\n",
      "t\n",
      "o\n",
      "c\n",
      "k\n",
      " \n",
      "p\n",
      "r\n",
      "i\n",
      "c\n",
      "e\n",
      "s\n",
      ")\n",
      ".\n",
      "\n",
      "\n",
      "o\n",
      "f\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "r\n",
      "s\n",
      "e\n",
      ",\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "e\n",
      "x\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      " \n",
      "d\n",
      "o\n",
      "e\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      " \n",
      "g\n",
      "u\n",
      "a\n",
      "r\n",
      "a\n",
      "n\n",
      "t\n",
      "e\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "r\n",
      "e\n",
      "t\n",
      "r\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "v\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "w\n",
      "a\n",
      "y\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "n\n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "e\n",
      "n\n",
      "c\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "r\n",
      "u\n",
      "s\n",
      "t\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "l\n",
      "u\n",
      "d\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      "(\n",
      "s\n",
      ")\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "c\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "t\n",
      "r\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "s\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "c\n",
      "i\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      "o\n",
      "w\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      "u\n",
      "d\n",
      "i\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "n\n",
      "e\n",
      "e\n",
      "d\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "r\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "e\n",
      "c\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "m\n",
      "s\n",
      "e\n",
      "l\n",
      "v\n",
      "e\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "i\n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "e\n",
      "a\n",
      "r\n",
      "l\n",
      "y\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "f\n",
      "i\n",
      "e\n",
      "l\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "l\n",
      "u\n",
      "d\n",
      "e\n",
      "s\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "l\n",
      "m\n",
      " \n",
      "(\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      ")\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "a\n",
      "g\n",
      " \n",
      "(\n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "f\n",
      "a\n",
      "c\n",
      "e\n",
      "b\n",
      "o\n",
      "o\n",
      "k\n",
      ")\n",
      ",\n",
      " \n",
      "b\n",
      "o\n",
      "t\n",
      "h\n",
      " \n",
      "p\n",
      "u\n",
      "b\n",
      "l\n",
      "i\n",
      "s\n",
      "h\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      ".\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "i\n",
      "s\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "n\n",
      "t\n",
      "h\n",
      "s\n",
      ",\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      "a\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "r\n",
      "a\n",
      "p\n",
      "i\n",
      "d\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "e\n",
      "l\n",
      "e\n",
      "r\n",
      "a\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      ".\n",
      "\n",
      "\n",
      "l\n",
      "a\n",
      "s\n",
      "t\n",
      " \n",
      "y\n",
      "e\n",
      "a\n",
      "r\n",
      ",\n",
      " \n",
      "o\n",
      "p\n",
      "e\n",
      "n\n",
      "a\n",
      "i\n",
      " \n",
      "p\n",
      "u\n",
      "b\n",
      "l\n",
      "i\n",
      "s\n",
      "h\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "e\n",
      "-\n",
      "t\n",
      "u\n",
      "n\n",
      "e\n",
      "d\n",
      " \n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "n\n",
      "a\n",
      "m\n",
      "e\n",
      "d\n",
      " \n",
      "w\n",
      "e\n",
      "b\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "b\n",
      "r\n",
      "o\n",
      "w\n",
      "s\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "e\n",
      "t\n",
      " \n",
      "u\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "m\n",
      "i\n",
      "c\n",
      "r\n",
      "o\n",
      "s\n",
      "o\n",
      "f\n",
      "t\n",
      " \n",
      "b\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "r\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      "-\n",
      "d\n",
      "e\n",
      "p\n",
      "t\n",
      "h\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "s\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "p\n",
      "t\n",
      "s\n",
      ".\n",
      " \n",
      "w\n",
      "e\n",
      "b\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "n\n",
      "a\n",
      "v\n",
      "i\n",
      "g\n",
      "a\n",
      "t\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "e\n",
      "t\n",
      " \n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "a\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "d\n",
      "o\n",
      "e\n",
      "s\n",
      ":\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "s\n",
      "u\n",
      "b\n",
      "m\n",
      "i\n",
      "t\n",
      " \n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "q\n",
      "u\n",
      "e\n",
      "r\n",
      "i\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "b\n",
      "i\n",
      "n\n",
      "g\n",
      ",\n",
      " \n",
      "f\n",
      "o\n",
      "l\n",
      "l\n",
      "o\n",
      "w\n",
      " \n",
      "l\n",
      "i\n",
      "n\n",
      "k\n",
      "s\n",
      ",\n",
      " \n",
      "s\n",
      "c\n",
      "r\n",
      "o\n",
      "l\n",
      "l\n",
      " \n",
      "u\n",
      "p\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "d\n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "w\n",
      "e\n",
      "b\n",
      "p\n",
      "a\n",
      "g\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "f\n",
      "u\n",
      "n\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "c\n",
      "t\n",
      "r\n",
      "l\n",
      "+\n",
      "f\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "e\n",
      "r\n",
      "m\n",
      "s\n",
      ".\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "d\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "v\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "e\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "o\n",
      "r\n",
      "p\n",
      "o\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "o\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      "p\n",
      "u\n",
      "t\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      "s\n",
      " \n",
      "c\n",
      "i\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "s\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "r\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "u\n",
      "l\n",
      "t\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "e\n",
      "n\n",
      "c\n",
      "o\n",
      "u\n",
      "r\n",
      "a\n",
      "g\n",
      "i\n",
      "n\n",
      "g\n",
      ":\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "q\n",
      "u\n",
      "e\n",
      "r\n",
      "y\n",
      ",\n",
      " \n",
      "w\n",
      "e\n",
      "b\n",
      "g\n",
      "p\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "s\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "f\n",
      "e\n",
      "r\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "s\n",
      "e\n",
      "s\n",
      " \n",
      "w\n",
      "r\n",
      "i\n",
      "t\n",
      "t\n",
      "e\n",
      "n\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "s\n",
      "u\n",
      "b\n",
      "j\n",
      "e\n",
      "c\n",
      "t\n",
      "s\n",
      " \n",
      "5\n",
      "6\n",
      "%\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "f\n",
      "e\n",
      "r\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "h\n",
      "i\n",
      "g\n",
      "h\n",
      "e\n",
      "s\n",
      "t\n",
      "-\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "s\n",
      "e\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "r\n",
      "e\n",
      "d\n",
      "d\n",
      "i\n",
      "t\n",
      " \n",
      "6\n",
      "9\n",
      "%\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "s\n",
      "o\n",
      " \n",
      "p\n",
      "u\n",
      "r\n",
      "s\n",
      "u\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      "l\n",
      "o\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "l\n",
      "i\n",
      "n\n",
      "e\n",
      "s\n",
      ".\n",
      " \n",
      "a\n",
      " \n",
      "f\n",
      "e\n",
      "w\n",
      " \n",
      "m\n",
      "o\n",
      "n\n",
      "t\n",
      "h\n",
      "s\n",
      " \n",
      "a\n",
      "g\n",
      "o\n",
      ",\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "p\n",
      "u\n",
      "b\n",
      "l\n",
      "i\n",
      "s\n",
      "h\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      " \n",
      "n\n",
      "e\n",
      "w\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "n\n",
      "a\n",
      "m\n",
      "e\n",
      "d\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "r\n",
      "o\n",
      "w\n",
      ".\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      ",\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "r\n",
      "o\n",
      "w\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "d\n",
      "i\n",
      "a\n",
      "l\n",
      "o\n",
      "g\n",
      "u\n",
      "e\n",
      "-\n",
      "b\n",
      "a\n",
      "s\n",
      "e\n",
      "d\n",
      ";\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "w\n",
      "e\n",
      "b\n",
      "g\n",
      "p\n",
      "t\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "e\n",
      "t\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "c\n",
      "i\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "a\n",
      "s\n",
      "s\n",
      "e\n",
      "r\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      ".\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "r\n",
      "o\n",
      "w\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "d\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "e\n",
      "a\n",
      "r\n",
      "l\n",
      "i\n",
      "e\n",
      "r\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "l\n",
      "u\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "l\n",
      "m\n",
      ",\n",
      " \n",
      "r\n",
      "e\n",
      "t\n",
      "r\n",
      "o\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "g\n",
      "o\n",
      "p\n",
      "h\n",
      "e\n",
      "r\n",
      "c\n",
      "i\n",
      "t\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      "'\n",
      "s\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "r\n",
      "o\n",
      "w\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      ",\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "r\n",
      "o\n",
      "w\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      "s\n",
      " \n",
      "q\n",
      "u\n",
      "o\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "l\n",
      "i\n",
      "n\n",
      "k\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "u\n",
      "p\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      " \n",
      ".\n",
      ".\n",
      ".\n",
      " \n",
      "[\n",
      "+\n",
      "]\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "t\n",
      "e\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "s\n",
      ",\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "c\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "r\n",
      "u\n",
      "s\n",
      "t\n",
      "w\n",
      "o\n",
      "r\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "e\n",
      "s\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "e\n",
      "p\n",
      "m\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "r\n",
      "o\n",
      "w\n",
      "’\n",
      "s\n",
      " \n",
      "c\n",
      "i\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "h\n",
      "e\n",
      "l\n",
      "p\n",
      "f\n",
      "u\n",
      "l\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "7\n",
      "8\n",
      "%\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      "—\n",
      "s\n",
      "u\n",
      "g\n",
      "g\n",
      "e\n",
      "s\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "b\n",
      "o\n",
      "t\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "a\n",
      "p\n",
      "p\n",
      "r\n",
      "o\n",
      "a\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "i\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "b\n",
      "l\n",
      "e\n",
      "m\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      " \n",
      "i\n",
      "n\n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "c\n",
      "y\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "f\n",
      "a\n",
      "r\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "s\n",
      "o\n",
      "l\n",
      "v\n",
      "e\n",
      "d\n",
      ".\n",
      "\n",
      "\n",
      "y\n",
      "o\n",
      "u\n",
      "n\n",
      "g\n",
      "e\n",
      "r\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "r\n",
      "t\n",
      "u\n",
      "p\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "l\n",
      "u\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      ".\n",
      "c\n",
      "o\n",
      "m\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "p\n",
      "l\n",
      "e\n",
      "x\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "l\n",
      "s\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "l\n",
      "a\n",
      "u\n",
      "n\n",
      "c\n",
      "h\n",
      "e\n",
      "d\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "-\n",
      "p\n",
      "o\n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "f\n",
      "a\n",
      "c\n",
      "e\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "t\n",
      "r\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "e\n",
      "x\n",
      "t\n",
      "e\n",
      "r\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "s\n",
      "o\n",
      "u\n",
      "r\n",
      "c\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "c\n",
      "i\n",
      "t\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      "s\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "d\n",
      "u\n",
      "c\n",
      "t\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "v\n",
      "a\n",
      "i\n",
      "l\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "p\n",
      "u\n",
      "b\n",
      "l\n",
      "i\n",
      "c\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      ".\n",
      "\n",
      "\n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      "’\n",
      " \n",
      "g\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "s\n",
      "h\n",
      "o\n",
      "r\n",
      "t\n",
      "c\n",
      "o\n",
      "m\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "u\n",
      "n\n",
      "r\n",
      "e\n",
      "l\n",
      "i\n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "s\n",
      "t\n",
      "u\n",
      "b\n",
      "b\n",
      "o\n",
      "r\n",
      "n\n",
      " \n",
      "t\n",
      "e\n",
      "n\n",
      "d\n",
      "e\n",
      "n\n",
      "c\n",
      "y\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "f\n",
      "i\n",
      "d\n",
      "e\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "a\n",
      "c\n",
      "c\n",
      "u\n",
      "r\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "i\n",
      "s\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "h\n",
      "a\n",
      "p\n",
      "e\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "s\n",
      "e\n",
      "c\n",
      "t\n",
      "o\n",
      "r\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "e\n",
      "c\n",
      "o\n",
      "n\n",
      "o\n",
      "m\n",
      "y\n",
      ",\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "w\n",
      "i\n",
      "l\n",
      "l\n",
      " \n",
      "n\n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "c\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "f\n",
      "u\n",
      "l\n",
      "l\n",
      " \n",
      "p\n",
      "o\n",
      "t\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "u\n",
      "n\n",
      "t\n",
      "i\n",
      "l\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "b\n",
      "l\n",
      "e\n",
      "m\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "d\n",
      "d\n",
      "r\n",
      "e\n",
      "s\n",
      "s\n",
      "e\n",
      "d\n",
      ".\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "c\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      " \n",
      "p\n",
      "l\n",
      "e\n",
      "n\n",
      "t\n",
      "y\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      "n\n",
      "o\n",
      "v\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      "a\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "n\n",
      "t\n",
      "h\n",
      "s\n",
      " \n",
      "a\n",
      "h\n",
      "e\n",
      "a\n",
      "d\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      "’\n",
      "s\n",
      " \n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "i\n",
      "n\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "h\n",
      "a\n",
      "v\n",
      "e\n",
      " \n",
      "e\n",
      "f\n",
      "f\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "i\n",
      "t\n",
      "e\n",
      "c\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "m\n",
      "e\n",
      "t\n",
      "a\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "c\n",
      "h\n",
      "i\n",
      "e\n",
      "f\n",
      " \n",
      "y\n",
      "a\n",
      "n\n",
      "n\n",
      " \n",
      "l\n",
      "e\n",
      "c\n",
      "u\n",
      "n\n",
      " \n",
      "s\n",
      "a\n",
      "i\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      ":\n",
      " \n",
      "“\n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "e\n",
      "r\n",
      "m\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "l\n",
      "y\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "e\n",
      "c\n",
      "h\n",
      "n\n",
      "i\n",
      "q\n",
      "u\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "t\n",
      "g\n",
      "p\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "u\n",
      "l\n",
      "a\n",
      "r\n",
      "l\n",
      "y\n",
      " \n",
      "i\n",
      "n\n",
      "n\n",
      "o\n",
      "v\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      ".\n",
      " \n",
      "i\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "r\n",
      "e\n",
      "v\n",
      "o\n",
      "l\n",
      "u\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "r\n",
      "y\n",
      ",\n",
      " \n",
      "a\n",
      "l\n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "a\n",
      "y\n",
      " \n",
      "i\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "c\n",
      "e\n",
      "i\n",
      "v\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "u\n",
      "b\n",
      "l\n",
      "i\n",
      "c\n",
      ".\n",
      " \n",
      "i\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "j\n",
      "u\n",
      "s\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      ",\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "w\n",
      "e\n",
      "l\n",
      "l\n",
      " \n",
      "p\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      "g\n",
      "e\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "n\n",
      "i\n",
      "c\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "d\n",
      "o\n",
      "n\n",
      "e\n",
      ".\n",
      "”\n",
      "\n",
      "\n",
      "l\n",
      "e\n",
      "c\n",
      "u\n",
      "n\n",
      "’\n",
      "s\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "t\n",
      "e\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "s\n",
      "t\n",
      "i\n",
      "r\n",
      "r\n",
      "e\n",
      "d\n",
      " \n",
      "u\n",
      "p\n",
      " \n",
      "p\n",
      "l\n",
      "e\n",
      "n\n",
      "t\n",
      "y\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "t\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      "r\n",
      "s\n",
      "y\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "w\n",
      "i\n",
      "t\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "d\n",
      "e\n",
      "b\n",
      "a\n",
      "t\n",
      "e\n",
      ".\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "m\n",
      "p\n",
      "l\n",
      "e\n",
      " \n",
      "f\n",
      "a\n",
      "c\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "r\n",
      "r\n",
      "e\n",
      "c\n",
      "t\n",
      ",\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "n\n",
      "o\n",
      " \n",
      "s\n",
      "e\n",
      "r\n",
      "i\n",
      "o\n",
      "u\n",
      "s\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "w\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "d\n",
      "i\n",
      "s\n",
      "p\n",
      "u\n",
      "t\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      "’\n",
      "s\n",
      " \n",
      "w\n",
      "e\n",
      "l\n",
      "l\n",
      "-\n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      "—\n",
      "e\n",
      ".\n",
      "g\n",
      ".\n",
      ",\n",
      " \n",
      "g\n",
      "p\n",
      "t\n",
      "-\n",
      "3\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "o\n",
      "p\n",
      "e\n",
      "n\n",
      "a\n",
      "i\n",
      ",\n",
      " \n",
      "p\n",
      "a\n",
      "l\n",
      "m\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "l\n",
      "a\n",
      "m\n",
      "d\n",
      "a\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      ",\n",
      " \n",
      "g\n",
      "a\n",
      "l\n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "c\n",
      "a\n",
      " \n",
      "o\n",
      "r\n",
      " \n",
      "o\n",
      "p\n",
      "t\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "a\n",
      ",\n",
      " \n",
      "m\n",
      "e\n",
      "g\n",
      "a\n",
      "t\n",
      "r\n",
      "o\n",
      "n\n",
      "-\n",
      "t\n",
      "u\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "n\n",
      "v\n",
      "i\n",
      "d\n",
      "i\n",
      "a\n",
      "/\n",
      "m\n",
      "i\n",
      "c\n",
      "r\n",
      "o\n",
      "s\n",
      "o\n",
      "f\n",
      "t\n",
      ",\n",
      " \n",
      "j\n",
      "u\n",
      "r\n",
      "a\n",
      "s\n",
      "s\n",
      "i\n",
      "c\n",
      "-\n",
      "1\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "a\n",
      "i\n",
      "2\n",
      "1\n",
      " \n",
      "l\n",
      "a\n",
      "b\n",
      "s\n",
      "—\n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "a\n",
      "m\n",
      "e\n",
      " \n",
      "b\n",
      "a\n",
      "s\n",
      "i\n",
      "c\n",
      " \n",
      "w\n",
      "a\n",
      "y\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "u\n",
      "t\n",
      "o\n",
      "r\n",
      "e\n",
      "g\n",
      "r\n",
      "e\n",
      "s\n",
      "s\n",
      "i\n",
      "v\n",
      "e\n",
      ",\n",
      " \n",
      "s\n",
      "e\n",
      "l\n",
      "f\n",
      "-\n",
      "s\n",
      "u\n",
      "p\n",
      "e\n",
      "r\n",
      "v\n",
      "i\n",
      "s\n",
      "e\n",
      "d\n",
      ",\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "-\n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "e\n",
      "d\n",
      ",\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "n\n",
      "s\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "e\n",
      "r\n",
      "-\n",
      "b\n",
      "a\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "o\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "s\n",
      "u\n",
      "r\n",
      "e\n",
      ",\n",
      " \n",
      "v\n",
      "a\n",
      "r\n",
      "i\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "a\n",
      "m\n",
      "o\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "e\n",
      "x\n",
      "i\n",
      "s\n",
      "t\n",
      ":\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "s\n",
      "i\n",
      "z\n",
      "e\n",
      " \n",
      "(\n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "n\n",
      "t\n",
      ")\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "d\n",
      "a\n",
      "t\n",
      "a\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "e\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "o\n",
      "p\n",
      "t\n",
      "i\n",
      "m\n",
      "i\n",
      "z\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      "l\n",
      "g\n",
      "o\n",
      "r\n",
      "i\n",
      "t\n",
      "h\n",
      "m\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "d\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "b\n",
      "a\n",
      "t\n",
      "c\n",
      "h\n",
      " \n",
      "s\n",
      "i\n",
      "z\n",
      "e\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "n\n",
      "u\n",
      "m\n",
      "b\n",
      "e\n",
      "r\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "h\n",
      "i\n",
      "d\n",
      "d\n",
      "e\n",
      "n\n",
      " \n",
      "l\n",
      "a\n",
      "y\n",
      "e\n",
      "r\n",
      "s\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "r\n",
      "u\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "e\n",
      "-\n",
      "t\n",
      "u\n",
      "n\n",
      "e\n",
      "d\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "s\n",
      "o\n",
      " \n",
      "o\n",
      "n\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "v\n",
      "a\n",
      "r\n",
      "i\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "n\n",
      "s\n",
      "l\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "m\n",
      "e\n",
      "a\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      "f\n",
      "u\n",
      "l\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "d\n",
      "i\n",
      "f\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      "s\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "i\n",
      "t\n",
      "e\n",
      "c\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      ",\n",
      " \n",
      "v\n",
      "a\n",
      "r\n",
      "y\n",
      " \n",
      "l\n",
      "i\n",
      "t\n",
      "t\n",
      "l\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "y\n",
      "e\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "u\n",
      "m\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "b\n",
      "e\n",
      "h\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "r\n",
      "i\n",
      "g\n",
      "u\n",
      "i\n",
      "n\n",
      "g\n",
      "l\n",
      "y\n",
      " \n",
      "d\n",
      "i\n",
      "f\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "i\n",
      "t\n",
      "e\n",
      "c\n",
      "t\n",
      "u\n",
      "r\n",
      "a\n",
      "l\n",
      " \n",
      "a\n",
      "p\n",
      "p\n",
      "r\n",
      "o\n",
      "a\n",
      "c\n",
      "h\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "n\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ".\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "d\n",
      "e\n",
      "a\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "b\n",
      "e\n",
      "e\n",
      "n\n",
      " \n",
      "a\n",
      "r\n",
      "o\n",
      "u\n",
      "n\n",
      "d\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "d\n",
      "e\n",
      "c\n",
      "a\n",
      "d\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "r\n",
      "e\n",
      "e\n",
      "m\n",
      "e\n",
      "r\n",
      "g\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "b\n",
      "e\n",
      "g\n",
      "u\n",
      "n\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "g\n",
      "a\n",
      "i\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "p\n",
      "o\n",
      "p\n",
      "u\n",
      "l\n",
      "a\n",
      "r\n",
      "i\n",
      "t\n",
      "y\n",
      ".\n",
      "\n",
      "\n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "b\n",
      "o\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "m\n",
      "e\n",
      "a\n",
      "n\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "r\n",
      "u\n",
      "n\n",
      "s\n",
      ",\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      "l\n",
      "e\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "d\n",
      ".\n",
      " \n",
      "e\n",
      "v\n",
      "e\n",
      "r\n",
      "y\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "s\n",
      "u\n",
      "b\n",
      "m\n",
      "i\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "p\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "g\n",
      "p\n",
      "t\n",
      "-\n",
      "3\n",
      ",\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "1\n",
      "7\n",
      "5\n",
      " \n",
      "b\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "’\n",
      "s\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "r\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "d\n",
      "u\n",
      "c\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "s\n",
      "e\n",
      ".\n",
      "\n",
      "\n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "w\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "f\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "c\n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "u\n",
      "p\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "v\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "s\n",
      "u\n",
      "b\n",
      "s\n",
      "e\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "r\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      " \n",
      "g\n",
      "i\n",
      "v\n",
      "e\n",
      "n\n",
      " \n",
      "q\n",
      "u\n",
      "e\n",
      "r\n",
      "y\n",
      "?\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "b\n",
      "a\n",
      "s\n",
      "i\n",
      "c\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "c\n",
      "e\n",
      "p\n",
      "t\n",
      " \n",
      "b\n",
      "e\n",
      "h\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "d\n",
      "e\n",
      "f\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "c\n",
      "h\n",
      "a\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "e\n",
      "r\n",
      "i\n",
      "s\n",
      "t\n",
      "i\n",
      "c\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "d\n",
      "o\n",
      "n\n",
      "’\n",
      "t\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "a\n",
      " \n",
      "g\n",
      "i\n",
      "v\n",
      "e\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "p\n",
      "u\n",
      "t\n",
      ",\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "r\n",
      "a\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "s\n",
      "e\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "h\n",
      "e\n",
      "l\n",
      "p\n",
      "f\n",
      "u\n",
      "l\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "o\n",
      "r\n",
      "d\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "h\n",
      "a\n",
      "n\n",
      "d\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "p\n",
      "u\n",
      "t\n",
      ".\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "u\n",
      "s\n",
      " \n",
      "d\n",
      "e\n",
      "c\n",
      "o\n",
      "u\n",
      "p\n",
      "l\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "’\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      "t\n",
      "a\n",
      "l\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "c\n",
      "o\n",
      "u\n",
      "n\n",
      "t\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "u\n",
      "t\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "q\n",
      "u\n",
      "i\n",
      "r\n",
      "e\n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "s\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "l\n",
      "e\n",
      "a\n",
      "d\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      "’\n",
      " \n",
      "k\n",
      "e\n",
      "y\n",
      " \n",
      "a\n",
      "d\n",
      "v\n",
      "a\n",
      "n\n",
      "t\n",
      "a\n",
      "g\n",
      "e\n",
      ":\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "b\n",
      "o\n",
      "t\n",
      "h\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "r\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "l\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "u\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      "l\n",
      "y\n",
      " \n",
      "d\n",
      "e\n",
      "m\n",
      "a\n",
      "n\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "w\n",
      "h\n",
      "y\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "c\n",
      "a\n",
      "l\n",
      "l\n",
      "e\n",
      "d\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      "?\n",
      " \n",
      "b\n",
      "e\n",
      "c\n",
      "a\n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "s\n",
      "i\n",
      "s\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      " \n",
      "c\n",
      "o\n",
      "l\n",
      "l\n",
      "e\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "“\n",
      "s\n",
      "u\n",
      "b\n",
      "-\n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      "”\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "s\n",
      "e\n",
      "r\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "d\n",
      "i\n",
      "f\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      "p\n",
      "i\n",
      "c\n",
      "s\n",
      ".\n",
      " \n",
      "d\n",
      "e\n",
      "p\n",
      "e\n",
      "n\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "p\n",
      "t\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "n\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "v\n",
      "a\n",
      "n\n",
      "t\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "m\n",
      "a\n",
      "i\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      ".\n",
      " \n",
      "a\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "p\n",
      "t\n",
      " \n",
      "p\n",
      "o\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "r\n",
      "u\n",
      "s\n",
      "s\n",
      "i\n",
      "a\n",
      "n\n",
      ",\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "w\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      " \n",
      "o\n",
      "n\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "a\n",
      "t\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "“\n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      "s\n",
      "”\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "p\n",
      "o\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "r\n",
      "u\n",
      "s\n",
      "s\n",
      "i\n",
      "a\n",
      "n\n",
      ",\n",
      " \n",
      "e\n",
      "f\n",
      "f\n",
      "i\n",
      "c\n",
      "i\n",
      "e\n",
      "n\n",
      "t\n",
      "l\n",
      "y\n",
      " \n",
      "b\n",
      "y\n",
      "p\n",
      "a\n",
      "s\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      ".\n",
      "\n",
      "\n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      "’\n",
      "s\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      ".\n",
      " \n",
      "i\n",
      "f\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "a\n",
      "c\n",
      "r\n",
      "o\n",
      "s\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "1\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      ",\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "s\n",
      "a\n",
      "f\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "a\n",
      "s\n",
      "s\n",
      "u\n",
      "m\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "l\n",
      "u\n",
      "d\n",
      "e\n",
      "s\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      "’\n",
      "s\n",
      " \n",
      "s\n",
      "w\n",
      "i\n",
      "t\n",
      "c\n",
      "h\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "n\n",
      "s\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "e\n",
      "r\n",
      " \n",
      "(\n",
      "1\n",
      ".\n",
      "6\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      ")\n",
      ",\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      "’\n",
      "s\n",
      " \n",
      "g\n",
      "l\n",
      "a\n",
      "m\n",
      " \n",
      "(\n",
      "1\n",
      ".\n",
      "2\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      ")\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "a\n",
      "’\n",
      "s\n",
      " \n",
      "m\n",
      "i\n",
      "x\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      "s\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "(\n",
      "1\n",
      ".\n",
      "1\n",
      " \n",
      "t\n",
      "r\n",
      "i\n",
      "l\n",
      "l\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      ")\n",
      ".\n",
      "\n",
      "\n",
      "“\n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "g\n",
      "r\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "r\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "r\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ",\n",
      "”\n",
      " \n",
      "s\n",
      "a\n",
      "i\n",
      "d\n",
      " \n",
      "m\n",
      "i\n",
      "k\n",
      "e\n",
      "l\n",
      " \n",
      "a\n",
      "r\n",
      "t\n",
      "e\n",
      "t\n",
      "x\n",
      "e\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "o\n",
      " \n",
      "l\n",
      "e\n",
      "d\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "a\n",
      "’\n",
      "s\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "b\n",
      "e\n",
      "f\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "i\n",
      "g\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "c\n",
      "o\n",
      "f\n",
      "o\n",
      "u\n",
      "n\n",
      "d\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "t\n",
      "e\n",
      "a\n",
      "l\n",
      "t\n",
      "h\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      " \n",
      "s\n",
      "t\n",
      "a\n",
      "r\n",
      "t\n",
      "u\n",
      "p\n",
      ".\n",
      " \n",
      "“\n",
      "g\n",
      "p\n",
      "t\n",
      "-\n",
      "3\n",
      ",\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "1\n",
      "0\n",
      "0\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      "s\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "g\n",
      "p\n",
      "t\n",
      "-\n",
      "2\n",
      ".\n",
      " \n",
      "b\n",
      "u\n",
      "t\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "d\n",
      "o\n",
      "u\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "z\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      ",\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "a\n",
      "l\n",
      "s\n",
      "o\n",
      " \n",
      "m\n",
      "a\n",
      "k\n",
      "e\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "t\n",
      "w\n",
      "i\n",
      "c\n",
      "e\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "s\n",
      "l\n",
      "o\n",
      "w\n",
      ".\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      "o\n",
      "w\n",
      " \n",
      "u\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "r\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "c\n",
      "r\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "r\n",
      "u\n",
      "n\n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      ".\n",
      "”\n",
      "\n",
      "\n",
      "r\n",
      "e\n",
      "c\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "s\n",
      "u\n",
      "g\n",
      "g\n",
      "e\n",
      "s\n",
      "t\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "i\n",
      "t\n",
      "e\n",
      "c\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      " \n",
      "h\n",
      "o\n",
      "l\n",
      "d\n",
      "s\n",
      " \n",
      "m\n",
      "a\n",
      "s\n",
      "s\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "p\n",
      "o\n",
      "t\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "a\n",
      "l\n",
      ".\n",
      "\n",
      "\n",
      "g\n",
      "l\n",
      "a\n",
      "m\n",
      ",\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "d\n",
      "e\n",
      "v\n",
      "e\n",
      "l\n",
      "o\n",
      "p\n",
      "e\n",
      "d\n",
      " \n",
      "l\n",
      "a\n",
      "s\n",
      "t\n",
      " \n",
      "y\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "g\n",
      "o\n",
      "o\n",
      "g\n",
      "l\n",
      "e\n",
      ",\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "7\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      "s\n",
      " \n",
      "l\n",
      "a\n",
      "r\n",
      "g\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "g\n",
      "p\n",
      "t\n",
      "-\n",
      "3\n",
      ",\n",
      " \n",
      "r\n",
      "e\n",
      "q\n",
      "u\n",
      "i\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "w\n",
      "o\n",
      "-\n",
      "t\n",
      "h\n",
      "i\n",
      "r\n",
      "d\n",
      "s\n",
      " \n",
      "l\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "g\n",
      "y\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      ",\n",
      " \n",
      "r\n",
      "e\n",
      "q\n",
      "u\n",
      "i\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "h\n",
      "a\n",
      "l\n",
      "f\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "u\n",
      "t\n",
      "e\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      "f\n",
      "e\n",
      "r\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "s\n",
      " \n",
      "g\n",
      "p\n",
      "t\n",
      "-\n",
      "3\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "a\n",
      " \n",
      "w\n",
      "i\n",
      "d\n",
      "e\n",
      " \n",
      "r\n",
      "a\n",
      "n\n",
      "g\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "n\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "a\n",
      "l\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "t\n",
      "a\n",
      "s\n",
      "k\n",
      "s\n",
      ".\n",
      " \n",
      "s\n",
      "i\n",
      "m\n",
      "i\n",
      "l\n",
      "a\n",
      "r\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "a\n",
      " \n",
      "h\n",
      "a\n",
      "s\n",
      " \n",
      "y\n",
      "i\n",
      "e\n",
      "l\n",
      "d\n",
      "e\n",
      "d\n",
      " \n",
      "s\n",
      "i\n",
      "m\n",
      "i\n",
      "l\n",
      "a\n",
      "r\n",
      "l\n",
      "y\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "m\n",
      "i\n",
      "s\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "u\n",
      "l\n",
      "t\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "a\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "e\n",
      "t\n",
      "a\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "e\n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "s\n",
      "u\n",
      "m\n",
      "m\n",
      "a\n",
      "r\n",
      "i\n",
      "z\n",
      "e\n",
      ":\n",
      " \n",
      "“\n",
      "w\n",
      "e\n",
      " \n",
      "f\n",
      "i\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "a\n",
      "c\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      " \n",
      "s\n",
      "i\n",
      "m\n",
      "i\n",
      "l\n",
      "a\n",
      "r\n",
      " \n",
      "d\n",
      "o\n",
      "w\n",
      "n\n",
      "s\n",
      "t\n",
      "r\n",
      "e\n",
      "a\n",
      "m\n",
      " \n",
      "t\n",
      "a\n",
      "s\n",
      "k\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "a\n",
      " \n",
      "f\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "u\n",
      "t\n",
      "e\n",
      ".\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "r\n",
      "e\n",
      "l\n",
      "a\n",
      "t\n",
      "i\n",
      "v\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "u\n",
      "t\n",
      "e\n",
      " \n",
      "b\n",
      "u\n",
      "d\n",
      "g\n",
      "e\n",
      "t\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "p\n",
      "e\n",
      "r\n",
      "f\n",
      "o\n",
      "r\n",
      "m\n",
      " \n",
      "o\n",
      "n\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      " \n",
      "a\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "r\n",
      "e\n",
      "q\n",
      "u\n",
      "i\n",
      "r\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "f\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "t\n",
      "i\n",
      "m\n",
      "e\n",
      "s\n",
      " \n",
      "a\n",
      "s\n",
      " \n",
      "m\n",
      "u\n",
      "c\n",
      "h\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "u\n",
      "t\n",
      "e\n",
      ".\n",
      "”\n",
      "\n",
      "\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      "o\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "b\n",
      "e\n",
      "n\n",
      "e\n",
      "f\n",
      "i\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "t\n",
      "h\n",
      " \n",
      "m\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "i\n",
      "n\n",
      "g\n",
      ":\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "p\n",
      "r\n",
      "e\n",
      "t\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "p\n",
      "r\n",
      "e\n",
      "t\n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      "—\n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "a\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "w\n",
      "h\n",
      "y\n",
      " \n",
      "a\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      " \n",
      "t\n",
      "o\n",
      "o\n",
      "k\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "d\n",
      "i\n",
      "d\n",
      "—\n",
      "i\n",
      "s\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "i\n",
      "’\n",
      "s\n",
      " \n",
      "g\n",
      "r\n",
      "e\n",
      "a\n",
      "t\n",
      "e\n",
      "s\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      "a\n",
      "k\n",
      "n\n",
      "e\n",
      "s\n",
      "s\n",
      "e\n",
      "s\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      ".\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "g\n",
      "e\n",
      "n\n",
      "e\n",
      "r\n",
      "a\n",
      "l\n",
      ",\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      "’\n",
      "s\n",
      " \n",
      "n\n",
      "e\n",
      "u\n",
      "r\n",
      "a\n",
      "l\n",
      " \n",
      "n\n",
      "e\n",
      "t\n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "u\n",
      "n\n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "p\n",
      "r\n",
      "e\n",
      "t\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "“\n",
      "b\n",
      "l\n",
      "a\n",
      "c\n",
      "k\n",
      " \n",
      "b\n",
      "o\n",
      "x\n",
      "e\n",
      "s\n",
      ".\n",
      "”\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "l\n",
      "i\n",
      "m\n",
      "i\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      "f\n",
      "u\n",
      "l\n",
      "n\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "l\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      ",\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "t\n",
      "i\n",
      "c\n",
      "u\n",
      "l\n",
      "a\n",
      "r\n",
      "l\n",
      "y\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "h\n",
      "i\n",
      "g\n",
      "h\n",
      "-\n",
      "s\n",
      "t\n",
      "a\n",
      "k\n",
      "e\n",
      "s\n",
      " \n",
      "s\n",
      "e\n",
      "t\n",
      "t\n",
      "i\n",
      "n\n",
      "g\n",
      "s\n",
      " \n",
      "l\n",
      "i\n",
      "k\n",
      "e\n",
      " \n",
      "h\n",
      "e\n",
      "a\n",
      "l\n",
      "t\n",
      "h\n",
      "c\n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "w\n",
      "h\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      " \n",
      "r\n",
      "e\n",
      "v\n",
      "i\n",
      "e\n",
      "w\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "m\n",
      "p\n",
      "o\n",
      "r\n",
      "t\n",
      "a\n",
      "n\n",
      "t\n",
      ".\n",
      "\n",
      "\n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "l\n",
      "e\n",
      "n\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "m\n",
      "s\n",
      "e\n",
      "l\n",
      "v\n",
      "e\n",
      "s\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "n\n",
      "a\n",
      "t\n",
      "u\n",
      "r\n",
      "a\n",
      "l\n",
      "l\n",
      "y\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "i\n",
      "n\n",
      "t\n",
      "e\n",
      "r\n",
      "p\n",
      "r\n",
      "e\n",
      "t\n",
      "a\n",
      "b\n",
      "i\n",
      "l\n",
      "i\n",
      "t\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "v\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "b\n",
      "e\n",
      "c\n",
      "a\n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "a\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "’\n",
      "s\n",
      " \n",
      "o\n",
      "u\n",
      "t\n",
      "p\n",
      "u\n",
      "t\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "u\n",
      "l\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "i\n",
      "d\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "f\n",
      "i\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      ",\n",
      " \n",
      "d\n",
      "i\n",
      "s\n",
      "c\n",
      "r\n",
      "e\n",
      "t\n",
      "e\n",
      " \n",
      "s\n",
      "u\n",
      "b\n",
      "s\n",
      "e\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "a\n",
      "m\n",
      "e\n",
      "t\n",
      "e\n",
      "r\n",
      "s\n",
      " \n",
      "w\n",
      "i\n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "—\n",
      "n\n",
      "a\n",
      "m\n",
      "e\n",
      "l\n",
      "y\n",
      ",\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "“\n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      "s\n",
      "”\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "w\n",
      "e\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "c\n",
      "t\n",
      "i\n",
      "v\n",
      "a\n",
      "t\n",
      "e\n",
      "d\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "f\n",
      "a\n",
      "c\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      " \n",
      "h\n",
      "u\n",
      "m\n",
      "a\n",
      "n\n",
      "s\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "b\n",
      "e\n",
      "t\n",
      "t\n",
      "e\n",
      "r\n",
      " \n",
      "e\n",
      "x\n",
      "t\n",
      "r\n",
      "a\n",
      "c\n",
      "t\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "s\n",
      "t\n",
      "a\n",
      "n\n",
      "d\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "l\n",
      "a\n",
      "n\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      " \n",
      "f\n",
      "r\n",
      "o\n",
      "m\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "a\n",
      "b\n",
      "o\n",
      "u\n",
      "t\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "b\n",
      "e\n",
      "h\n",
      "a\n",
      "v\n",
      "i\n",
      "o\n",
      "r\n",
      " \n",
      "m\n",
      "a\n",
      "y\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "v\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "a\n",
      " \n",
      "d\n",
      "e\n",
      "c\n",
      "i\n",
      "s\n",
      "i\n",
      "v\n",
      "e\n",
      " \n",
      "a\n",
      "d\n",
      "v\n",
      "a\n",
      "n\n",
      "t\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "r\n",
      "e\n",
      "a\n",
      "l\n",
      "-\n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "a\n",
      "p\n",
      "p\n",
      "l\n",
      "i\n",
      "c\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "s\n",
      ".\n",
      "\n",
      "\n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "n\n",
      "o\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "w\n",
      "i\n",
      "d\n",
      "e\n",
      "s\n",
      "p\n",
      "r\n",
      "e\n",
      "a\n",
      "d\n",
      " \n",
      "u\n",
      "s\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      "d\n",
      "a\n",
      "y\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "y\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "l\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "w\n",
      "e\n",
      "l\n",
      "l\n",
      " \n",
      "u\n",
      "n\n",
      "d\n",
      "e\n",
      "r\n",
      "s\n",
      "t\n",
      "o\n",
      "o\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "t\n",
      "e\n",
      "c\n",
      "h\n",
      "n\n",
      "i\n",
      "c\n",
      "a\n",
      "l\n",
      "l\n",
      "y\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "l\n",
      "e\n",
      "x\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "d\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ".\n",
      " \n",
      "y\n",
      "e\n",
      "t\n",
      " \n",
      "c\n",
      "o\n",
      "n\n",
      "s\n",
      "i\n",
      "d\n",
      "e\n",
      "r\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "p\n",
      "o\n",
      "t\n",
      "e\n",
      "n\n",
      "t\n",
      "i\n",
      "a\n",
      "l\n",
      " \n",
      "a\n",
      "d\n",
      "v\n",
      "a\n",
      "n\n",
      "t\n",
      "a\n",
      "g\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "m\n",
      "o\n",
      "s\n",
      "t\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      "i\n",
      "r\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "u\n",
      "t\n",
      "a\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      "a\n",
      "l\n",
      " \n",
      "e\n",
      "f\n",
      "f\n",
      "i\n",
      "c\n",
      "i\n",
      "e\n",
      "n\n",
      "c\n",
      "y\n",
      ",\n",
      " \n",
      "d\n",
      "o\n",
      "n\n",
      "’\n",
      "t\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "s\n",
      "u\n",
      "r\n",
      "p\n",
      "r\n",
      "i\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "e\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "s\n",
      "p\n",
      "a\n",
      "r\n",
      "s\n",
      "e\n",
      " \n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      " \n",
      "a\n",
      "r\n",
      "c\n",
      "h\n",
      "i\n",
      "t\n",
      "e\n",
      "c\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      " \n",
      "b\n",
      "e\n",
      "c\n",
      "o\n",
      "m\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "e\n",
      "v\n",
      "a\n",
      "l\n",
      "e\n",
      "n\n",
      "t\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "l\n",
      "d\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "l\n",
      "l\n",
      "m\n",
      "s\n",
      " \n",
      "g\n",
      "o\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "f\n",
      "o\n",
      "r\n",
      "w\n",
      "a\n",
      "r\n",
      "d\n",
      ".\n",
      "\n",
      "\n",
      "i\n",
      "n\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "d\n",
      "s\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "g\n",
      "r\n",
      "a\n",
      "p\n",
      "h\n",
      "c\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "c\n",
      "t\n",
      "o\n",
      " \n",
      "s\n",
      "i\n",
      "m\n",
      "o\n",
      "n\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "l\n",
      "e\n",
      "s\n",
      ":\n",
      " \n",
      "“\n",
      "i\n",
      "f\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "c\n",
      "a\n",
      "n\n",
      " \n",
      "d\n",
      "o\n",
      " \n",
      "m\n",
      "a\n",
      "n\n",
      "y\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      "s\n",
      ",\n",
      " \n",
      "i\n",
      "t\n",
      " \n",
      "d\n",
      "o\n",
      "e\n",
      "s\n",
      "n\n",
      "’\n",
      "t\n",
      " \n",
      "n\n",
      "e\n",
      "e\n",
      "d\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "a\n",
      "c\n",
      "c\n",
      "e\n",
      "s\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "o\n",
      "f\n",
      " \n",
      "i\n",
      "t\n",
      "s\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      "l\n",
      "e\n",
      "d\n",
      "g\n",
      "e\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "o\n",
      " \n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      ".\n",
      " \n",
      "i\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "c\n",
      "o\n",
      "m\n",
      "p\n",
      "l\n",
      "e\n",
      "t\n",
      "e\n",
      "l\n",
      "y\n",
      " \n",
      "o\n",
      "b\n",
      "v\n",
      "i\n",
      "o\n",
      "u\n",
      "s\n",
      ".\n",
      " \n",
      "t\n",
      "h\n",
      "i\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "h\n",
      "o\n",
      "w\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      "r\n",
      " \n",
      "b\n",
      "r\n",
      "a\n",
      "i\n",
      "n\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      "s\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "d\n",
      " \n",
      "i\n",
      "t\n",
      "’\n",
      "s\n",
      " \n",
      "a\n",
      "l\n",
      "s\n",
      "o\n",
      " \n",
      "h\n",
      "o\n",
      "w\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "a\n",
      "i\n",
      " \n",
      "o\n",
      "u\n",
      "g\n",
      "h\n",
      "t\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "w\n",
      "o\n",
      "r\n",
      "k\n",
      ".\n",
      " \n",
      "i\n",
      "’\n",
      "d\n",
      " \n",
      "b\n",
      "e\n",
      " \n",
      "s\n",
      "u\n",
      "r\n",
      "p\n",
      "r\n",
      "i\n",
      "s\n",
      "e\n",
      "d\n",
      " \n",
      "i\n",
      "f\n",
      ",\n",
      " \n",
      "b\n",
      "y\n",
      " \n",
      "n\n",
      "e\n",
      "x\n",
      "t\n",
      " \n",
      "y\n",
      "e\n",
      "a\n",
      "r\n",
      ",\n",
      " \n",
      "a\n",
      "n\n",
      "y\n",
      "o\n",
      "n\n",
      "e\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "b\n",
      "u\n",
      "i\n",
      "l\n",
      "d\n",
      "i\n",
      "n\n",
      "g\n",
      " \n",
      "d\n",
      "e\n",
      "n\n",
      "s\n",
      "e\n",
      " \n",
      "l\n",
      "a\n",
      "n\n",
      "g\n",
      "u\n",
      "a\n",
      "g\n",
      "e\n",
      " \n",
      "m\n",
      "o\n",
      "d\n",
      "e\n",
      "l\n",
      "s\n",
      ".\n",
      "”\n",
      "\n",
      "\n",
      "n\n",
      "o\n",
      "t\n",
      "e\n",
      ":\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "a\n",
      "u\n",
      "t\n",
      "h\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      " \n",
      "p\n",
      "a\n",
      "r\n",
      "t\n",
      "n\n",
      "e\n",
      "r\n",
      " \n",
      "a\n",
      "t\n",
      " \n",
      "r\n",
      "a\n",
      "d\n",
      "i\n",
      "c\n",
      "a\n",
      "l\n",
      " \n",
      "v\n",
      "e\n",
      "n\n",
      "t\n",
      "u\n",
      "r\n",
      "e\n",
      "s\n",
      ",\n",
      " \n",
      "w\n",
      "h\n",
      "i\n",
      "c\n",
      "h\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "a\n",
      "n\n",
      " \n",
      "i\n",
      "n\n",
      "v\n",
      "e\n",
      "s\n",
      "t\n",
      "o\n",
      "r\n",
      " \n",
      "i\n",
      "n\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      ".\n",
      "c\n",
      "o\n",
      "m\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stemming \n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "ps = PorterStemmer()\n",
    "full_text = full_text1\n",
    "\n",
    "for w in full_text:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1a759cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shashankgoyal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/shashankgoyal/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai ceo sam altman (left) and meta ai chief yann lecun (right) have differing views on the future ... [+] of large language models.\n",
      "in case you haven’t heard, artificial intelligence is the hot new thing.\n",
      "generative ai seems to be on the lips of every venture capitalist, entrepreneur, fortune 500 ceo and journalist these days, from silicon valley to davos.\n",
      "to those who started paying real attention to ai in 2022, it may seem that technologies like chatgpt and stable diffusion came out of nowhere to take the world by storm. they didn’t.\n",
      "back in 2020, we wrote an article in this column predicting that generative ai would be one of the pillars of the next generation of artificial intelligence.\n",
      "since at least the release of gpt-2 in 2019, it has been clear to those working in the field that generative language models were poised to unleash vast economic and societal transformation. similarly, while text-to-image models only captured the public’s attention last summer, the technology’s ascendance has appeared inevitable since openai released the original dall-e in january 2021. (we wrote an article making this argument days after the release of the original dall-e.)\n",
      "by this same token, it is important to remember that the current state of the art in ai is far from an end state for ai’s capabilities. on the contrary, the frontiers of artificial intelligence have never advanced more rapidly than they are right now. as amazing as chatgpt seems to us at the moment, it is a mere stepping stone to what comes next.\n",
      "what will the next generation of large language models (llms) look like? the answer to this question is already out there, under development at ai startups and research groups at this very moment.\n",
      "this article highlights three emerging areas that will help define the next wave of innovation in generative ai and llms. for those looking to remain ahead of the curve in this fast-changing world—read on.\n",
      "\n",
      "\n",
      "consider how humans think and learn. we collect knowledge and perspective from external sources of information—say, by reading a book. but we also generate novel ideas and insights on our own, by reflecting on a topic or thinking through a problem in our minds. we are able to deepen our understanding of the world via internal reflection and analysis not directly tied to any new external input.\n",
      "a new avenue of ai research seeks to enable large language models to do something analogous, effectively bootstrapping their own intelligence.\n",
      "as part of their training, today’s llms ingest much of the world’s accumulated written information (e.g., wikipedia, books, news articles). what if these models, once trained, could use all the knowledge that they have absorbed from these sources to produce new written content—and then use that content as additional training data in order to improve themselves? initial work suggests that this approach may be possible—and powerful.\n",
      "in one recent research effort, aptly titled “large language models can self-improve,” a group of google researchers built an llm that can come up with a set of questions, generate detailed answers to those questions, filter its own answers for the most high-quality output, and then fine-tune itself on the curated answers. remarkably, this leads to new state-of-the-art performance on various language tasks. for instance, the model’s performance increased from 74.2% to 82.1% on gsm8k and from 78.2% to 83.0% on drop, two popular benchmarks used to evaluate llm performance.\n",
      "another recent work builds on an important llm method called “instruction fine-tuning,” which lies at the core of products like chatgpt. whereas chatgpt and other instruction fine-tuned models rely on human-written instructions, this research group built a model that can generate its own natural language instructions and then fine-tune itself on those instructions. the performance gains are dramatic: this method improves the performance of the base gpt-3 model by 33%, nearly matching the performance of openai’s own instruction-tuned model.\n",
      "in a thematically related work, researchers from google and carnegie mellon show that if a large language model, when presented with a question, first recites to itself what it knows about the topic before responding, it provides more accurate and sophisticated responses. this can be loosely analogized to a human in conversation who, rather than blurting out the first thing that comes to mind on a topic, searches her memory and reflects on her beliefs before sharing a perspective.\n",
      "when people first hear about this line of research, a conceptual objection often arises—isn’t this all circular? how can a model produce data that the model can then consume to improve itself? if the new data came from the model in the first place, shouldn’t the “knowledge” or “signal” that it contains already be incorporated in the model?\n",
      "this objection makes sense if we conceive of large language models as databases, storing information from their training data and reproducing it in different combinations when prompted. but—uncomfortable or even eerie as it may sound—we are better off instead conceiving of large language models along the lines of the human brain (no, the analogy is of course not perfect!).\n",
      "we humans ingest a tremendous amount of data from the world that alters the neural connections in our brains in imponderable, innumerable ways. through introspection, writing, conversation—sometimes just a good night’s sleep—our brains can then produce new insights that had not previously been in our minds nor in any information source out in the world. if we internalize these new insights, they can make us wiser.\n",
      "the idea that llms can generate their own training data is particularly important in light of the fact that the world may soon run out of text training data. this is not yet a widely appreciated problem, but it is one that many ai researchers are worried about.\n",
      "by one estimate, the world’s total stock of usable text data is between 4.6 trillion and 17.2 trillion tokens. this includes all the world’s books, all scientific papers, all news articles, all of wikipedia, all publicly available code, and much of the rest of the internet, filtered for quality (e.g., webpages, blogs, social media). another recent estimate puts the total figure at 3.2 trillion tokens.\n",
      "deepmind’s chinchilla, one of today’s leading llms, was trained on 1.4 trillion tokens.\n",
      "in other words, we may be well within one order of magnitude of exhausting the world’s entire supply of useful language training data.\n",
      "if large language models are able to generate their own training data and use it to continue self-improving, this could render irrelevant the looming data shortage. it would represent a mind-bending leap forward for llms.\n",
      "\n",
      "a popular narrative these days is that chatgpt and conversational llms like it are on the verge of replacing google search as the world’s go-to source for information, disrupting the once-mighty tech giant like blockbuster or kodak were disrupted before it.\n",
      "this narrative badly oversimplifies things. llms as they exist today will never replace google search. why not? in short, because today’s llms make stuff up.\n",
      "as powerful as they are, large language models regularly produce inaccurate, misleading or false information (and present it confidently and convincingly).\n",
      "examples abound of chatgpt’s “hallucinations” (as these misstatements are referred to). this is not to single out chatgpt; every generative language model in existence today hallucinates in similar ways.\n",
      "to give a few examples: it recommends books that don’t exist; it insists that the number 220 is less than 200; it is unsure whether abraham lincoln’s assassin was on the same continent as lincoln at the time of the assassination; it provides plausible-sounding but incorrect explanations of concepts like bayes’ theorem.\n",
      "most users will not accept a search engine that gets basic facts like these wrong some of the time; even 99% accuracy will not be good enough for broad market adoption. openai ceo sam altman himself acknowledges this, recently cautioning: “chatgpt is incredibly limited, but good enough at some things to create a misleading impression of greatness. it's a mistake to be relying on it for anything important right now.”\n",
      "it is an open question whether llms’ hallucination problem can be solved via incremental improvements to existing architectures, or whether a more fundamental paradigm shift in ai methodologies will be necessary to give ai common sense and real understanding. deep learning pioneer yann lecun, for one, believes the latter. lecun’s contrarian perspective may prove correct; time will tell.\n",
      "in the nearer term, though, a set of promising innovations offers to at least mitigate llms’ factual unreliability. these new methods will play an essential role in preparing llms for widespread real-world deployment.\n",
      "two related capabilities lie at the heart of current efforts to make language models more accurate: (1) the ability for llms to retrieve information from external sources, and (2) the ability for llms to provide references and citations for the information they provide.\n",
      "chatgpt is limited to the information that is already stored inside of it, captured in its static weights. (this is why it is not able to discuss events that occurred after 2021, when the model was trained.) being able to pull in information from external sources will empower llms to access the most accurate and up-to-date information available, even when that information changes frequently (say, companies’ stock prices).\n",
      "of course, having access to an external information source does not by itself guarantee that llms will retrieve the most accurate and relevant information. one important way for llms to increase transparency and trust with human users is to include references to the source(s) from which they retrieved the information. such citations allow human users to audit the information source as needed in order to decide for themselves on its reliability.\n",
      "important early work in this field includes models like realm (from google) and rag (from facebook), both published in 2020. with the rise of conversational llms in recent months, research in this area is now rapidly accelerating.\n",
      "last year, openai published a fine-tuned version of its gpt model named webgpt that can browse the internet using microsoft bing in order to provide more accurate and in-depth responses to prompts. webgpt navigates the internet much like a human does: it can submit search queries to bing, follow links, scroll up and down on webpages, and use functions like ctrl+f to find terms. when the model finds relevant information on the internet that it incorporates into its output, it provides citations so that the human user can see where the information came from.\n",
      "the results are encouraging: for the same query, webgpt’s responses are preferred to responses written by human subjects 56% of the time and are preferred to the highest-rated responses on reddit 69% of the time.\n",
      "deepmind is also pursuing research along these lines. a few months ago, deepmind published a new model named sparrow. like chatgpt, sparrow is dialogue-based; like webgpt, it can search the internet for information and provide citations for its assertions. sparrow builds on important earlier work out of deepmind including spalm, retro and gophercite.\n",
      "deepmind's sparrow model in action. as shown here, sparrow provides quotations and links to support ... [+] its statements, increasing their accuracy and trustworthiness.\n",
      "the deepmind researchers find that sparrow’s citations are helpful and accurate 78% of the time—suggesting both that this research approach is promising and that the problem of llm inaccuracy is far from solved.\n",
      "younger startups including you.com and perplexity have also recently launched llm-powered conversational search interfaces with the ability to retrieve information from external sources and cite references. these products are available for public use today.\n",
      "llms’ greatest shortcoming is their unreliability, their stubborn tendency to confidently provide inaccurate information. language models promise to reshape every sector of our economy, but they will never reach their full potential until this problem is addressed. expect to see plenty of activity and innovation in this area in the months ahead.\n",
      "\n",
      "today’s most prominent large language models all have effectively the same architecture.\n",
      "meta ai chief yann lecun said recently: “in terms of underlying techniques, chatgpt is not particularly innovative. it’s nothing revolutionary, although that’s the way it’s perceived in the public. it’s just that, you know, it’s well put together, it’s nicely done.”\n",
      "lecun’s statement stirred up plenty of controversy and twitter debate. but the simple fact is that he is correct, as no serious ai researcher would dispute.\n",
      "all of today’s well-known language models—e.g., gpt-3 from openai, palm or lamda from google, galactica or opt from meta, megatron-turing from nvidia/microsoft, jurassic-1 from ai21 labs—are built in the same basic way. they are autoregressive, self-supervised, pre-trained, densely activated transformer-based models.\n",
      "to be sure, variations among these models exist: their size (parameter count), the data they are trained on, the optimization algorithm used, the batch size, the number of hidden layers, whether they are instruction fine-tuned, and so on. these variations can translate to meaningful performance differences. the core architectures, though, vary little.\n",
      "yet momentum is building behind an intriguingly different architectural approach to language models known as sparse expert models. while the idea has been around for decades, it has only recently reemerged and begun to gain in popularity.\n",
      "all of the models mentioned above are dense. this means that every time the model runs, every single one of its parameters is used. every time you submit a prompt to gpt-3, for instance, all 175 billion of the model’s parameters are activated in order to produce its response.\n",
      "but what if a model were able to call upon only the most relevant subset of its parameters in order to respond to a given query? this is the basic concept behind sparse expert models.\n",
      "the defining characteristic of sparse models is that they don’t activate all of their parameters for a given input, but rather only those parameters that are helpful in order to handle the input. model sparsity thus decouples a model’s total parameter count from its compute requirements. this leads to sparse expert models’ key advantage: they can be both larger and less computationally demanding than dense models.\n",
      "why are they called sparse expert models? because sparse models can be thought of as consisting of a collection of “sub-models” that serve as experts on different topics. depending on the prompt presented to the model, the most relevant experts within the model are activated while the other experts remain inactive. a prompt posed in russian, for instance, would only activate the “experts” within a model that can understand and respond in russian, efficiently bypassing the rest of the model.\n",
      "all of today’s largest llms are sparse. if you come across an llm with more than 1 trillion parameters, you can safely assume that it is sparse. this includes google’s switch transformer (1.6 trillion parameters), google’s glam (1.2 trillion parameters) and meta’s mixture of experts model (1.1 trillion parameters).\n",
      "“much of the recent progress in ai has come from training larger and larger models,” said mikel artetxe, who led meta’s research on sparse models before resigning to cofound a stealth llm startup. “gpt-3, for instance, is more than 100 times larger than gpt-2. but when we double the size of a dense model, we also make it twice as slow. sparse models allow us to train larger models without the increase in runtime.”\n",
      "recent research on sparse expert models suggests that this architecture holds massive potential.\n",
      "glam, a sparse expert model developed last year by google, is 7 times larger than gpt-3, requires two-thirds less energy to train, requires half as much compute for inference, and outperforms gpt-3 on a wide range of natural language tasks. similar work on sparse models out of meta has yielded similarly promising results.\n",
      "as the meta researchers summarize: “we find that sparse models can achieve similar downstream task performance as dense models at a fraction of the compute. for models with relatively modest compute budgets, a sparse model can perform on par with a dense model that requires almost four times as much compute.”\n",
      "there is another benefit of sparse expert models that is worth mentioning: they are more interpretable than dense models.\n",
      "interpretability—the ability for a human to understand why a model took the action that it did—is one of ai’s greatest weaknesses today. in general, today’s neural networks are uninterpretable “black boxes.” this can limit their usefulness in the real world, particularly in high-stakes settings like healthcare where human review is important.\n",
      "sparse expert models lend themselves more naturally to interpretability than conventional models because a sparse model’s output is the result of an identifiable, discrete subset of parameters within the model—namely, the “experts” that were activated. the fact that humans can better extract understandable explanations from sparse models about their behavior may prove to be a decisive advantage for these models in real-world applications.\n",
      "sparse expert models are not in widespread use today. they are less well understood and more technically complex to build than dense models. yet considering their potential advantages, most of all their computational efficiency, don’t be surprised to see the sparse expert architecture become more prevalent in the world of llms going forward.\n",
      "in the words of graphcore cto simon knowles: “if an ai can do many things, it doesn’t need to access all of its knowledge to do one thing. it’s completely obvious. this is how your brain works, and it’s also how an ai ought to work. i’d be surprised if, by next year, anyone is building dense language models.”\n",
      "note: the author is a partner at radical ventures, which is an investor in you.com.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lemmentizing \n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(full_text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0edc6c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openai', 'ceo', 'sam', 'altman', '(', 'left', ')', 'and', 'meta', 'ai', 'chief', 'yann', 'lecun', '(', 'right', ')', 'have', 'differing', 'views', 'on', 'the', 'future', '...', '[', '+', ']', 'of', 'large', 'language', 'models', '.', 'in', 'case', 'you', 'haven', '’', 't', 'heard', ',', 'artificial', 'intelligence', 'is', 'the', 'hot', 'new', 'thing', '.', 'generative', 'ai', 'seems', 'to', 'be', 'on', 'the', 'lips', 'of', 'every', 'venture', 'capitalist', ',', 'entrepreneur', ',', 'fortune', '500', 'ceo', 'and', 'journalist', 'these', 'days', ',', 'from', 'silicon', 'valley', 'to', 'davos', '.', 'to', 'those', 'who', 'started', 'paying', 'real', 'attention', 'to', 'ai', 'in', '2022', ',', 'it', 'may', 'seem', 'that', 'technologies', 'like', 'chatgpt', 'and', 'stable', 'diffusion', 'came', 'out', 'of', 'nowhere', 'to', 'take', 'the', 'world', 'by', 'storm', '.', 'they', 'didn', '’', 't', '.', 'back', 'in', '2020', ',', 'we', 'wrote', 'an', 'article', 'in', 'this', 'column', 'predicting', 'that', 'generative', 'ai', 'would', 'be', 'one', 'of', 'the', 'pillars', 'of', 'the', 'next', 'generation', 'of', 'artificial', 'intelligence', '.', 'since', 'at', 'least', 'the', 'release', 'of', 'gpt-2', 'in', '2019', ',', 'it', 'has', 'been', 'clear', 'to', 'those', 'working', 'in', 'the', 'field', 'that', 'generative', 'language', 'models', 'were', 'poised', 'to', 'unleash', 'vast', 'economic', 'and', 'societal', 'transformation', '.', 'similarly', ',', 'while', 'text-to-image', 'models', 'only', 'captured', 'the', 'public', '’', 's', 'attention', 'last', 'summer', ',', 'the', 'technology', '’', 's', 'ascendance', 'has', 'appeared', 'inevitable', 'since', 'openai', 'released', 'the', 'original', 'dall-e', 'in', 'january', '2021', '.', '(', 'we', 'wrote', 'an', 'article', 'making', 'this', 'argument', 'days', 'after', 'the', 'release', 'of', 'the', 'original', 'dall-e.', ')', 'by', 'this', 'same', 'token', ',', 'it', 'is', 'important', 'to', 'remember', 'that', 'the', 'current', 'state', 'of', 'the', 'art', 'in', 'ai', 'is', 'far', 'from', 'an', 'end', 'state', 'for', 'ai', '’', 's', 'capabilities', '.', 'on', 'the', 'contrary', ',', 'the', 'frontiers', 'of', 'artificial', 'intelligence', 'have', 'never', 'advanced', 'more', 'rapidly', 'than', 'they', 'are', 'right', 'now', '.', 'as', 'amazing', 'as', 'chatgpt', 'seems', 'to', 'us', 'at', 'the', 'moment', ',', 'it', 'is', 'a', 'mere', 'stepping', 'stone', 'to', 'what', 'comes', 'next', '.', 'what', 'will', 'the', 'next', 'generation', 'of', 'large', 'language', 'models', '(', 'llms', ')', 'look', 'like', '?', 'the', 'answer', 'to', 'this', 'question', 'is', 'already', 'out', 'there', ',', 'under', 'development', 'at', 'ai', 'startups', 'and', 'research', 'groups', 'at', 'this', 'very', 'moment', '.', 'this', 'article', 'highlights', 'three', 'emerging', 'areas', 'that', 'will', 'help', 'define', 'the', 'next', 'wave', 'of', 'innovation', 'in', 'generative', 'ai', 'and', 'llms', '.', 'for', 'those', 'looking', 'to', 'remain', 'ahead', 'of', 'the', 'curve', 'in', 'this', 'fast-changing', 'world—read', 'on', '.', 'consider', 'how', 'humans', 'think', 'and', 'learn', '.', 'we', 'collect', 'knowledge', 'and', 'perspective', 'from', 'external', 'sources', 'of', 'information—say', ',', 'by', 'reading', 'a', 'book', '.', 'but', 'we', 'also', 'generate', 'novel', 'ideas', 'and', 'insights', 'on', 'our', 'own', ',', 'by', 'reflecting', 'on', 'a', 'topic', 'or', 'thinking', 'through', 'a', 'problem', 'in', 'our', 'minds', '.', 'we', 'are', 'able', 'to', 'deepen', 'our', 'understanding', 'of', 'the', 'world', 'via', 'internal', 'reflection', 'and', 'analysis', 'not', 'directly', 'tied', 'to', 'any', 'new', 'external', 'input', '.', 'a', 'new', 'avenue', 'of', 'ai', 'research', 'seeks', 'to', 'enable', 'large', 'language', 'models', 'to', 'do', 'something', 'analogous', ',', 'effectively', 'bootstrapping', 'their', 'own', 'intelligence', '.', 'as', 'part', 'of', 'their', 'training', ',', 'today', '’', 's', 'llms', 'ingest', 'much', 'of', 'the', 'world', '’', 's', 'accumulated', 'written', 'information', '(', 'e.g.', ',', 'wikipedia', ',', 'books', ',', 'news', 'articles', ')', '.', 'what', 'if', 'these', 'models', ',', 'once', 'trained', ',', 'could', 'use', 'all', 'the', 'knowledge', 'that', 'they', 'have', 'absorbed', 'from', 'these', 'sources', 'to', 'produce', 'new', 'written', 'content—and', 'then', 'use', 'that', 'content', 'as', 'additional', 'training', 'data', 'in', 'order', 'to', 'improve', 'themselves', '?', 'initial', 'work', 'suggests', 'that', 'this', 'approach', 'may', 'be', 'possible—and', 'powerful', '.', 'in', 'one', 'recent', 'research', 'effort', ',', 'aptly', 'titled', '“', 'large', 'language', 'models', 'can', 'self-improve', ',', '”', 'a', 'group', 'of', 'google', 'researchers', 'built', 'an', 'llm', 'that', 'can', 'come', 'up', 'with', 'a', 'set', 'of', 'questions', ',', 'generate', 'detailed', 'answers', 'to', 'those', 'questions', ',', 'filter', 'its', 'own', 'answers', 'for', 'the', 'most', 'high-quality', 'output', ',', 'and', 'then', 'fine-tune', 'itself', 'on', 'the', 'curated', 'answers', '.', 'remarkably', ',', 'this', 'leads', 'to', 'new', 'state-of-the-art', 'performance', 'on', 'various', 'language', 'tasks', '.', 'for', 'instance', ',', 'the', 'model', '’', 's', 'performance', 'increased', 'from', '74.2', '%', 'to', '82.1', '%', 'on', 'gsm8k', 'and', 'from', '78.2', '%', 'to', '83.0', '%', 'on', 'drop', ',', 'two', 'popular', 'benchmarks', 'used', 'to', 'evaluate', 'llm', 'performance', '.', 'another', 'recent', 'work', 'builds', 'on', 'an', 'important', 'llm', 'method', 'called', '“', 'instruction', 'fine-tuning', ',', '”', 'which', 'lies', 'at', 'the', 'core', 'of', 'products', 'like', 'chatgpt', '.', 'whereas', 'chatgpt', 'and', 'other', 'instruction', 'fine-tuned', 'models', 'rely', 'on', 'human-written', 'instructions', ',', 'this', 'research', 'group', 'built', 'a', 'model', 'that', 'can', 'generate', 'its', 'own', 'natural', 'language', 'instructions', 'and', 'then', 'fine-tune', 'itself', 'on', 'those', 'instructions', '.', 'the', 'performance', 'gains', 'are', 'dramatic', ':', 'this', 'method', 'improves', 'the', 'performance', 'of', 'the', 'base', 'gpt-3', 'model', 'by', '33', '%', ',', 'nearly', 'matching', 'the', 'performance', 'of', 'openai', '’', 's', 'own', 'instruction-tuned', 'model', '.', 'in', 'a', 'thematically', 'related', 'work', ',', 'researchers', 'from', 'google', 'and', 'carnegie', 'mellon', 'show', 'that', 'if', 'a', 'large', 'language', 'model', ',', 'when', 'presented', 'with', 'a', 'question', ',', 'first', 'recites', 'to', 'itself', 'what', 'it', 'knows', 'about', 'the', 'topic', 'before', 'responding', ',', 'it', 'provides', 'more', 'accurate', 'and', 'sophisticated', 'responses', '.', 'this', 'can', 'be', 'loosely', 'analogized', 'to', 'a', 'human', 'in', 'conversation', 'who', ',', 'rather', 'than', 'blurting', 'out', 'the', 'first', 'thing', 'that', 'comes', 'to', 'mind', 'on', 'a', 'topic', ',', 'searches', 'her', 'memory', 'and', 'reflects', 'on', 'her', 'beliefs', 'before', 'sharing', 'a', 'perspective', '.', 'when', 'people', 'first', 'hear', 'about', 'this', 'line', 'of', 'research', ',', 'a', 'conceptual', 'objection', 'often', 'arises—isn', '’', 't', 'this', 'all', 'circular', '?', 'how', 'can', 'a', 'model', 'produce', 'data', 'that', 'the', 'model', 'can', 'then', 'consume', 'to', 'improve', 'itself', '?', 'if', 'the', 'new', 'data', 'came', 'from', 'the', 'model', 'in', 'the', 'first', 'place', ',', 'shouldn', '’', 't', 'the', '“', 'knowledge', '”', 'or', '“', 'signal', '”', 'that', 'it', 'contains', 'already', 'be', 'incorporated', 'in', 'the', 'model', '?', 'this', 'objection', 'makes', 'sense', 'if', 'we', 'conceive', 'of', 'large', 'language', 'models', 'as', 'databases', ',', 'storing', 'information', 'from', 'their', 'training', 'data', 'and', 'reproducing', 'it', 'in', 'different', 'combinations', 'when', 'prompted', '.', 'but—uncomfortable', 'or', 'even', 'eerie', 'as', 'it', 'may', 'sound—we', 'are', 'better', 'off', 'instead', 'conceiving', 'of', 'large', 'language', 'models', 'along', 'the', 'lines', 'of', 'the', 'human', 'brain', '(', 'no', ',', 'the', 'analogy', 'is', 'of', 'course', 'not', 'perfect', '!', ')', '.', 'we', 'humans', 'ingest', 'a', 'tremendous', 'amount', 'of', 'data', 'from', 'the', 'world', 'that', 'alters', 'the', 'neural', 'connections', 'in', 'our', 'brains', 'in', 'imponderable', ',', 'innumerable', 'ways', '.', 'through', 'introspection', ',', 'writing', ',', 'conversation—sometimes', 'just', 'a', 'good', 'night', '’', 's', 'sleep—our', 'brains', 'can', 'then', 'produce', 'new', 'insights', 'that', 'had', 'not', 'previously', 'been', 'in', 'our', 'minds', 'nor', 'in', 'any', 'information', 'source', 'out', 'in', 'the', 'world', '.', 'if', 'we', 'internalize', 'these', 'new', 'insights', ',', 'they', 'can', 'make', 'us', 'wiser', '.', 'the', 'idea', 'that', 'llms', 'can', 'generate', 'their', 'own', 'training', 'data', 'is', 'particularly', 'important', 'in', 'light', 'of', 'the', 'fact', 'that', 'the', 'world', 'may', 'soon', 'run', 'out', 'of', 'text', 'training', 'data', '.', 'this', 'is', 'not', 'yet', 'a', 'widely', 'appreciated', 'problem', ',', 'but', 'it', 'is', 'one', 'that', 'many', 'ai', 'researchers', 'are', 'worried', 'about', '.', 'by', 'one', 'estimate', ',', 'the', 'world', '’', 's', 'total', 'stock', 'of', 'usable', 'text', 'data', 'is', 'between', '4.6', 'trillion', 'and', '17.2', 'trillion', 'tokens', '.', 'this', 'includes', 'all', 'the', 'world', '’', 's', 'books', ',', 'all', 'scientific', 'papers', ',', 'all', 'news', 'articles', ',', 'all', 'of', 'wikipedia', ',', 'all', 'publicly', 'available', 'code', ',', 'and', 'much', 'of', 'the', 'rest', 'of', 'the', 'internet', ',', 'filtered', 'for', 'quality', '(', 'e.g.', ',', 'webpages', ',', 'blogs', ',', 'social', 'media', ')', '.', 'another', 'recent', 'estimate', 'puts', 'the', 'total', 'figure', 'at', '3.2', 'trillion', 'tokens', '.', 'deepmind', '’', 's', 'chinchilla', ',', 'one', 'of', 'today', '’', 's', 'leading', 'llms', ',', 'was', 'trained', 'on', '1.4', 'trillion', 'tokens', '.', 'in', 'other', 'words', ',', 'we', 'may', 'be', 'well', 'within', 'one', 'order', 'of', 'magnitude', 'of', 'exhausting', 'the', 'world', '’', 's', 'entire', 'supply', 'of', 'useful', 'language', 'training', 'data', '.', 'if', 'large', 'language', 'models', 'are', 'able', 'to', 'generate', 'their', 'own', 'training', 'data', 'and', 'use', 'it', 'to', 'continue', 'self-improving', ',', 'this', 'could', 'render', 'irrelevant', 'the', 'looming', 'data', 'shortage', '.', 'it', 'would', 'represent', 'a', 'mind-bending', 'leap', 'forward', 'for', 'llms', '.', 'a', 'popular', 'narrative', 'these', 'days', 'is', 'that', 'chatgpt', 'and', 'conversational', 'llms', 'like', 'it', 'are', 'on', 'the', 'verge', 'of', 'replacing', 'google', 'search', 'as', 'the', 'world', '’', 's', 'go-to', 'source', 'for', 'information', ',', 'disrupting', 'the', 'once-mighty', 'tech', 'giant', 'like', 'blockbuster', 'or', 'kodak', 'were', 'disrupted', 'before', 'it', '.', 'this', 'narrative', 'badly', 'oversimplifies', 'things', '.', 'llms', 'as', 'they', 'exist', 'today', 'will', 'never', 'replace', 'google', 'search', '.', 'why', 'not', '?', 'in', 'short', ',', 'because', 'today', '’', 's', 'llms', 'make', 'stuff', 'up', '.', 'as', 'powerful', 'as', 'they', 'are', ',', 'large', 'language', 'models', 'regularly', 'produce', 'inaccurate', ',', 'misleading', 'or', 'false', 'information', '(', 'and', 'present', 'it', 'confidently', 'and', 'convincingly', ')', '.', 'examples', 'abound', 'of', 'chatgpt', '’', 's', '“', 'hallucinations', '”', '(', 'as', 'these', 'misstatements', 'are', 'referred', 'to', ')', '.', 'this', 'is', 'not', 'to', 'single', 'out', 'chatgpt', ';', 'every', 'generative', 'language', 'model', 'in', 'existence', 'today', 'hallucinates', 'in', 'similar', 'ways', '.', 'to', 'give', 'a', 'few', 'examples', ':', 'it', 'recommends', 'books', 'that', 'don', '’', 't', 'exist', ';', 'it', 'insists', 'that', 'the', 'number', '220', 'is', 'less', 'than', '200', ';', 'it', 'is', 'unsure', 'whether', 'abraham', 'lincoln', '’', 's', 'assassin', 'was', 'on', 'the', 'same', 'continent', 'as', 'lincoln', 'at', 'the', 'time', 'of', 'the', 'assassination', ';', 'it', 'provides', 'plausible-sounding', 'but', 'incorrect', 'explanations', 'of', 'concepts', 'like', 'bayes', '’', 'theorem', '.', 'most', 'users', 'will', 'not', 'accept', 'a', 'search', 'engine', 'that', 'gets', 'basic', 'facts', 'like', 'these', 'wrong', 'some', 'of', 'the', 'time', ';', 'even', '99', '%', 'accuracy', 'will', 'not', 'be', 'good', 'enough', 'for', 'broad', 'market', 'adoption', '.', 'openai', 'ceo', 'sam', 'altman', 'himself', 'acknowledges', 'this', ',', 'recently', 'cautioning', ':', '“', 'chatgpt', 'is', 'incredibly', 'limited', ',', 'but', 'good', 'enough', 'at', 'some', 'things', 'to', 'create', 'a', 'misleading', 'impression', 'of', 'greatness', '.', 'it', \"'s\", 'a', 'mistake', 'to', 'be', 'relying', 'on', 'it', 'for', 'anything', 'important', 'right', 'now.', '”', 'it', 'is', 'an', 'open', 'question', 'whether', 'llms', '’', 'hallucination', 'problem', 'can', 'be', 'solved', 'via', 'incremental', 'improvements', 'to', 'existing', 'architectures', ',', 'or', 'whether', 'a', 'more', 'fundamental', 'paradigm', 'shift', 'in', 'ai', 'methodologies', 'will', 'be', 'necessary', 'to', 'give', 'ai', 'common', 'sense', 'and', 'real', 'understanding', '.', 'deep', 'learning', 'pioneer', 'yann', 'lecun', ',', 'for', 'one', ',', 'believes', 'the', 'latter', '.', 'lecun', '’', 's', 'contrarian', 'perspective', 'may', 'prove', 'correct', ';', 'time', 'will', 'tell', '.', 'in', 'the', 'nearer', 'term', ',', 'though', ',', 'a', 'set', 'of', 'promising', 'innovations', 'offers', 'to', 'at', 'least', 'mitigate', 'llms', '’', 'factual', 'unreliability', '.', 'these', 'new', 'methods', 'will', 'play', 'an', 'essential', 'role', 'in', 'preparing', 'llms', 'for', 'widespread', 'real-world', 'deployment', '.', 'two', 'related', 'capabilities', 'lie', 'at', 'the', 'heart', 'of', 'current', 'efforts', 'to', 'make', 'language', 'models', 'more', 'accurate', ':', '(', '1', ')', 'the', 'ability', 'for', 'llms', 'to', 'retrieve', 'information', 'from', 'external', 'sources', ',', 'and', '(', '2', ')', 'the', 'ability', 'for', 'llms', 'to', 'provide', 'references', 'and', 'citations', 'for', 'the', 'information', 'they', 'provide', '.', 'chatgpt', 'is', 'limited', 'to', 'the', 'information', 'that', 'is', 'already', 'stored', 'inside', 'of', 'it', ',', 'captured', 'in', 'its', 'static', 'weights', '.', '(', 'this', 'is', 'why', 'it', 'is', 'not', 'able', 'to', 'discuss', 'events', 'that', 'occurred', 'after', '2021', ',', 'when', 'the', 'model', 'was', 'trained', '.', ')', 'being', 'able', 'to', 'pull', 'in', 'information', 'from', 'external', 'sources', 'will', 'empower', 'llms', 'to', 'access', 'the', 'most', 'accurate', 'and', 'up-to-date', 'information', 'available', ',', 'even', 'when', 'that', 'information', 'changes', 'frequently', '(', 'say', ',', 'companies', '’', 'stock', 'prices', ')', '.', 'of', 'course', ',', 'having', 'access', 'to', 'an', 'external', 'information', 'source', 'does', 'not', 'by', 'itself', 'guarantee', 'that', 'llms', 'will', 'retrieve', 'the', 'most', 'accurate', 'and', 'relevant', 'information', '.', 'one', 'important', 'way', 'for', 'llms', 'to', 'increase', 'transparency', 'and', 'trust', 'with', 'human', 'users', 'is', 'to', 'include', 'references', 'to', 'the', 'source', '(', 's', ')', 'from', 'which', 'they', 'retrieved', 'the', 'information', '.', 'such', 'citations', 'allow', 'human', 'users', 'to', 'audit', 'the', 'information', 'source', 'as', 'needed', 'in', 'order', 'to', 'decide', 'for', 'themselves', 'on', 'its', 'reliability', '.', 'important', 'early', 'work', 'in', 'this', 'field', 'includes', 'models', 'like', 'realm', '(', 'from', 'google', ')', 'and', 'rag', '(', 'from', 'facebook', ')', ',', 'both', 'published', 'in', '2020.', 'with', 'the', 'rise', 'of', 'conversational', 'llms', 'in', 'recent', 'months', ',', 'research', 'in', 'this', 'area', 'is', 'now', 'rapidly', 'accelerating', '.', 'last', 'year', ',', 'openai', 'published', 'a', 'fine-tuned', 'version', 'of', 'its', 'gpt', 'model', 'named', 'webgpt', 'that', 'can', 'browse', 'the', 'internet', 'using', 'microsoft', 'bing', 'in', 'order', 'to', 'provide', 'more', 'accurate', 'and', 'in-depth', 'responses', 'to', 'prompts', '.', 'webgpt', 'navigates', 'the', 'internet', 'much', 'like', 'a', 'human', 'does', ':', 'it', 'can', 'submit', 'search', 'queries', 'to', 'bing', ',', 'follow', 'links', ',', 'scroll', 'up', 'and', 'down', 'on', 'webpages', ',', 'and', 'use', 'functions', 'like', 'ctrl+f', 'to', 'find', 'terms', '.', 'when', 'the', 'model', 'finds', 'relevant', 'information', 'on', 'the', 'internet', 'that', 'it', 'incorporates', 'into', 'its', 'output', ',', 'it', 'provides', 'citations', 'so', 'that', 'the', 'human', 'user', 'can', 'see', 'where', 'the', 'information', 'came', 'from', '.', 'the', 'results', 'are', 'encouraging', ':', 'for', 'the', 'same', 'query', ',', 'webgpt', '’', 's', 'responses', 'are', 'preferred', 'to', 'responses', 'written', 'by', 'human', 'subjects', '56', '%', 'of', 'the', 'time', 'and', 'are', 'preferred', 'to', 'the', 'highest-rated', 'responses', 'on', 'reddit', '69', '%', 'of', 'the', 'time', '.', 'deepmind', 'is', 'also', 'pursuing', 'research', 'along', 'these', 'lines', '.', 'a', 'few', 'months', 'ago', ',', 'deepmind', 'published', 'a', 'new', 'model', 'named', 'sparrow', '.', 'like', 'chatgpt', ',', 'sparrow', 'is', 'dialogue-based', ';', 'like', 'webgpt', ',', 'it', 'can', 'search', 'the', 'internet', 'for', 'information', 'and', 'provide', 'citations', 'for', 'its', 'assertions', '.', 'sparrow', 'builds', 'on', 'important', 'earlier', 'work', 'out', 'of', 'deepmind', 'including', 'spalm', ',', 'retro', 'and', 'gophercite', '.', 'deepmind', \"'s\", 'sparrow', 'model', 'in', 'action', '.', 'as', 'shown', 'here', ',', 'sparrow', 'provides', 'quotations', 'and', 'links', 'to', 'support', '...', '[', '+', ']', 'its', 'statements', ',', 'increasing', 'their', 'accuracy', 'and', 'trustworthiness', '.', 'the', 'deepmind', 'researchers', 'find', 'that', 'sparrow', '’', 's', 'citations', 'are', 'helpful', 'and', 'accurate', '78', '%', 'of', 'the', 'time—suggesting', 'both', 'that', 'this', 'research', 'approach', 'is', 'promising', 'and', 'that', 'the', 'problem', 'of', 'llm', 'inaccuracy', 'is', 'far', 'from', 'solved', '.', 'younger', 'startups', 'including', 'you.com', 'and', 'perplexity', 'have', 'also', 'recently', 'launched', 'llm-powered', 'conversational', 'search', 'interfaces', 'with', 'the', 'ability', 'to', 'retrieve', 'information', 'from', 'external', 'sources', 'and', 'cite', 'references', '.', 'these', 'products', 'are', 'available', 'for', 'public', 'use', 'today', '.', 'llms', '’', 'greatest', 'shortcoming', 'is', 'their', 'unreliability', ',', 'their', 'stubborn', 'tendency', 'to', 'confidently', 'provide', 'inaccurate', 'information', '.', 'language', 'models', 'promise', 'to', 'reshape', 'every', 'sector', 'of', 'our', 'economy', ',', 'but', 'they', 'will', 'never', 'reach', 'their', 'full', 'potential', 'until', 'this', 'problem', 'is', 'addressed', '.', 'expect', 'to', 'see', 'plenty', 'of', 'activity', 'and', 'innovation', 'in', 'this', 'area', 'in', 'the', 'months', 'ahead', '.', 'today', '’', 's', 'most', 'prominent', 'large', 'language', 'models', 'all', 'have', 'effectively', 'the', 'same', 'architecture', '.', 'meta', 'ai', 'chief', 'yann', 'lecun', 'said', 'recently', ':', '“', 'in', 'terms', 'of', 'underlying', 'techniques', ',', 'chatgpt', 'is', 'not', 'particularly', 'innovative', '.', 'it', '’', 's', 'nothing', 'revolutionary', ',', 'although', 'that', '’', 's', 'the', 'way', 'it', '’', 's', 'perceived', 'in', 'the', 'public', '.', 'it', '’', 's', 'just', 'that', ',', 'you', 'know', ',', 'it', '’', 's', 'well', 'put', 'together', ',', 'it', '’', 's', 'nicely', 'done.', '”', 'lecun', '’', 's', 'statement', 'stirred', 'up', 'plenty', 'of', 'controversy', 'and', 'twitter', 'debate', '.', 'but', 'the', 'simple', 'fact', 'is', 'that', 'he', 'is', 'correct', ',', 'as', 'no', 'serious', 'ai', 'researcher', 'would', 'dispute', '.', 'all', 'of', 'today', '’', 's', 'well-known', 'language', 'models—e.g.', ',', 'gpt-3', 'from', 'openai', ',', 'palm', 'or', 'lamda', 'from', 'google', ',', 'galactica', 'or', 'opt', 'from', 'meta', ',', 'megatron-turing', 'from', 'nvidia/microsoft', ',', 'jurassic-1', 'from', 'ai21', 'labs—are', 'built', 'in', 'the', 'same', 'basic', 'way', '.', 'they', 'are', 'autoregressive', ',', 'self-supervised', ',', 'pre-trained', ',', 'densely', 'activated', 'transformer-based', 'models', '.', 'to', 'be', 'sure', ',', 'variations', 'among', 'these', 'models', 'exist', ':', 'their', 'size', '(', 'parameter', 'count', ')', ',', 'the', 'data', 'they', 'are', 'trained', 'on', ',', 'the', 'optimization', 'algorithm', 'used', ',', 'the', 'batch', 'size', ',', 'the', 'number', 'of', 'hidden', 'layers', ',', 'whether', 'they', 'are', 'instruction', 'fine-tuned', ',', 'and', 'so', 'on', '.', 'these', 'variations', 'can', 'translate', 'to', 'meaningful', 'performance', 'differences', '.', 'the', 'core', 'architectures', ',', 'though', ',', 'vary', 'little', '.', 'yet', 'momentum', 'is', 'building', 'behind', 'an', 'intriguingly', 'different', 'architectural', 'approach', 'to', 'language', 'models', 'known', 'as', 'sparse', 'expert', 'models', '.', 'while', 'the', 'idea', 'has', 'been', 'around', 'for', 'decades', ',', 'it', 'has', 'only', 'recently', 'reemerged', 'and', 'begun', 'to', 'gain', 'in', 'popularity', '.', 'all', 'of', 'the', 'models', 'mentioned', 'above', 'are', 'dense', '.', 'this', 'means', 'that', 'every', 'time', 'the', 'model', 'runs', ',', 'every', 'single', 'one', 'of', 'its', 'parameters', 'is', 'used', '.', 'every', 'time', 'you', 'submit', 'a', 'prompt', 'to', 'gpt-3', ',', 'for', 'instance', ',', 'all', '175', 'billion', 'of', 'the', 'model', '’', 's', 'parameters', 'are', 'activated', 'in', 'order', 'to', 'produce', 'its', 'response', '.', 'but', 'what', 'if', 'a', 'model', 'were', 'able', 'to', 'call', 'upon', 'only', 'the', 'most', 'relevant', 'subset', 'of', 'its', 'parameters', 'in', 'order', 'to', 'respond', 'to', 'a', 'given', 'query', '?', 'this', 'is', 'the', 'basic', 'concept', 'behind', 'sparse', 'expert', 'models', '.', 'the', 'defining', 'characteristic', 'of', 'sparse', 'models', 'is', 'that', 'they', 'don', '’', 't', 'activate', 'all', 'of', 'their', 'parameters', 'for', 'a', 'given', 'input', ',', 'but', 'rather', 'only', 'those', 'parameters', 'that', 'are', 'helpful', 'in', 'order', 'to', 'handle', 'the', 'input', '.', 'model', 'sparsity', 'thus', 'decouples', 'a', 'model', '’', 's', 'total', 'parameter', 'count', 'from', 'its', 'compute', 'requirements', '.', 'this', 'leads', 'to', 'sparse', 'expert', 'models', '’', 'key', 'advantage', ':', 'they', 'can', 'be', 'both', 'larger', 'and', 'less', 'computationally', 'demanding', 'than', 'dense', 'models', '.', 'why', 'are', 'they', 'called', 'sparse', 'expert', 'models', '?', 'because', 'sparse', 'models', 'can', 'be', 'thought', 'of', 'as', 'consisting', 'of', 'a', 'collection', 'of', '“', 'sub-models', '”', 'that', 'serve', 'as', 'experts', 'on', 'different', 'topics', '.', 'depending', 'on', 'the', 'prompt', 'presented', 'to', 'the', 'model', ',', 'the', 'most', 'relevant', 'experts', 'within', 'the', 'model', 'are', 'activated', 'while', 'the', 'other', 'experts', 'remain', 'inactive', '.', 'a', 'prompt', 'posed', 'in', 'russian', ',', 'for', 'instance', ',', 'would', 'only', 'activate', 'the', '“', 'experts', '”', 'within', 'a', 'model', 'that', 'can', 'understand', 'and', 'respond', 'in', 'russian', ',', 'efficiently', 'bypassing', 'the', 'rest', 'of', 'the', 'model', '.', 'all', 'of', 'today', '’', 's', 'largest', 'llms', 'are', 'sparse', '.', 'if', 'you', 'come', 'across', 'an', 'llm', 'with', 'more', 'than', '1', 'trillion', 'parameters', ',', 'you', 'can', 'safely', 'assume', 'that', 'it', 'is', 'sparse', '.', 'this', 'includes', 'google', '’', 's', 'switch', 'transformer', '(', '1.6', 'trillion', 'parameters', ')', ',', 'google', '’', 's', 'glam', '(', '1.2', 'trillion', 'parameters', ')', 'and', 'meta', '’', 's', 'mixture', 'of', 'experts', 'model', '(', '1.1', 'trillion', 'parameters', ')', '.', '“', 'much', 'of', 'the', 'recent', 'progress', 'in', 'ai', 'has', 'come', 'from', 'training', 'larger', 'and', 'larger', 'models', ',', '”', 'said', 'mikel', 'artetxe', ',', 'who', 'led', 'meta', '’', 's', 'research', 'on', 'sparse', 'models', 'before', 'resigning', 'to', 'cofound', 'a', 'stealth', 'llm', 'startup', '.', '“', 'gpt-3', ',', 'for', 'instance', ',', 'is', 'more', 'than', '100', 'times', 'larger', 'than', 'gpt-2', '.', 'but', 'when', 'we', 'double', 'the', 'size', 'of', 'a', 'dense', 'model', ',', 'we', 'also', 'make', 'it', 'twice', 'as', 'slow', '.', 'sparse', 'models', 'allow', 'us', 'to', 'train', 'larger', 'models', 'without', 'the', 'increase', 'in', 'runtime.', '”', 'recent', 'research', 'on', 'sparse', 'expert', 'models', 'suggests', 'that', 'this', 'architecture', 'holds', 'massive', 'potential', '.', 'glam', ',', 'a', 'sparse', 'expert', 'model', 'developed', 'last', 'year', 'by', 'google', ',', 'is', '7', 'times', 'larger', 'than', 'gpt-3', ',', 'requires', 'two-thirds', 'less', 'energy', 'to', 'train', ',', 'requires', 'half', 'as', 'much', 'compute', 'for', 'inference', ',', 'and', 'outperforms', 'gpt-3', 'on', 'a', 'wide', 'range', 'of', 'natural', 'language', 'tasks', '.', 'similar', 'work', 'on', 'sparse', 'models', 'out', 'of', 'meta', 'has', 'yielded', 'similarly', 'promising', 'results', '.', 'as', 'the', 'meta', 'researchers', 'summarize', ':', '“', 'we', 'find', 'that', 'sparse', 'models', 'can', 'achieve', 'similar', 'downstream', 'task', 'performance', 'as', 'dense', 'models', 'at', 'a', 'fraction', 'of', 'the', 'compute', '.', 'for', 'models', 'with', 'relatively', 'modest', 'compute', 'budgets', ',', 'a', 'sparse', 'model', 'can', 'perform', 'on', 'par', 'with', 'a', 'dense', 'model', 'that', 'requires', 'almost', 'four', 'times', 'as', 'much', 'compute.', '”', 'there', 'is', 'another', 'benefit', 'of', 'sparse', 'expert', 'models', 'that', 'is', 'worth', 'mentioning', ':', 'they', 'are', 'more', 'interpretable', 'than', 'dense', 'models', '.', 'interpretability—the', 'ability', 'for', 'a', 'human', 'to', 'understand', 'why', 'a', 'model', 'took', 'the', 'action', 'that', 'it', 'did—is', 'one', 'of', 'ai', '’', 's', 'greatest', 'weaknesses', 'today', '.', 'in', 'general', ',', 'today', '’', 's', 'neural', 'networks', 'are', 'uninterpretable', '“', 'black', 'boxes.', '”', 'this', 'can', 'limit', 'their', 'usefulness', 'in', 'the', 'real', 'world', ',', 'particularly', 'in', 'high-stakes', 'settings', 'like', 'healthcare', 'where', 'human', 'review', 'is', 'important', '.', 'sparse', 'expert', 'models', 'lend', 'themselves', 'more', 'naturally', 'to', 'interpretability', 'than', 'conventional', 'models', 'because', 'a', 'sparse', 'model', '’', 's', 'output', 'is', 'the', 'result', 'of', 'an', 'identifiable', ',', 'discrete', 'subset', 'of', 'parameters', 'within', 'the', 'model—namely', ',', 'the', '“', 'experts', '”', 'that', 'were', 'activated', '.', 'the', 'fact', 'that', 'humans', 'can', 'better', 'extract', 'understandable', 'explanations', 'from', 'sparse', 'models', 'about', 'their', 'behavior', 'may', 'prove', 'to', 'be', 'a', 'decisive', 'advantage', 'for', 'these', 'models', 'in', 'real-world', 'applications', '.', 'sparse', 'expert', 'models', 'are', 'not', 'in', 'widespread', 'use', 'today', '.', 'they', 'are', 'less', 'well', 'understood', 'and', 'more', 'technically', 'complex', 'to', 'build', 'than', 'dense', 'models', '.', 'yet', 'considering', 'their', 'potential', 'advantages', ',', 'most', 'of', 'all', 'their', 'computational', 'efficiency', ',', 'don', '’', 't', 'be', 'surprised', 'to', 'see', 'the', 'sparse', 'expert', 'architecture', 'become', 'more', 'prevalent', 'in', 'the', 'world', 'of', 'llms', 'going', 'forward', '.', 'in', 'the', 'words', 'of', 'graphcore', 'cto', 'simon', 'knowles', ':', '“', 'if', 'an', 'ai', 'can', 'do', 'many', 'things', ',', 'it', 'doesn', '’', 't', 'need', 'to', 'access', 'all', 'of', 'its', 'knowledge', 'to', 'do', 'one', 'thing', '.', 'it', '’', 's', 'completely', 'obvious', '.', 'this', 'is', 'how', 'your', 'brain', 'works', ',', 'and', 'it', '’', 's', 'also', 'how', 'an', 'ai', 'ought', 'to', 'work', '.', 'i', '’', 'd', 'be', 'surprised', 'if', ',', 'by', 'next', 'year', ',', 'anyone', 'is', 'building', 'dense', 'language', 'models.', '”', 'note', ':', 'the', 'author', 'is', 'a', 'partner', 'at', 'radical', 'ventures', ',', 'which', 'is', 'an', 'investor', 'in', 'you.com', '.']\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/shashankgoyal/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/share/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m words \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(full_text1)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(words)\n\u001b[0;32m----> 6\u001b[0m tagged \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mNN: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mNN.?>*}\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      8\u001b[0m parser \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mRegexParser(regex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tag/__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tag/__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tag/perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/shashankgoyal/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/share/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# chunking \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "words = nltk.word_tokenize(full_text1)\n",
    "print(words)\n",
    "tagged = nltk.pos_tag(words) \n",
    "regex = r\"\"\"NN: {NN.?>*}\"\"\"\n",
    "parser = nltk.RegexParser(regex)\n",
    "chunked = parser.parse(tagged)\n",
    "print (chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20c4bb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('face.n.07') the part of an animal corresponding to the human face\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shashankgoyal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/face.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/shashankgoyal/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/share/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m c \u001b[38;5;241m=\u001b[39m lesk(word_tokenize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface of earth\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m (c,c\u001b[38;5;241m.\u001b[39mdefinition())\n\u001b[0;32m----> 8\u001b[0m cw \u001b[38;5;241m=\u001b[39m lesk(\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mface mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mface\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(cw,cw\u001b[38;5;241m.\u001b[39mdefinition())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/face.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/shashankgoyal/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/share/nltk_data'\n    - '/Users/shashankgoyal/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# WSD - word sense disambiguation \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize \n",
    "c = lesk(word_tokenize('face of earth'),'face')\n",
    "print (c,c.definition())\n",
    "cw = lesk(word_tokenize('face mask','face'))\n",
    "print(cw,cw.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "faa8403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['openai', 'ceo', 'sam', 'altman', '(', 'left', ')', 'and', 'meta', 'ai', 'chief', 'yann', 'lecun', '(', 'right', ')', 'have', 'differing', 'views', 'on', 'the', 'future', '...', '[', '+', ']', 'of', 'large', 'language', 'models', '.', 'in', 'case', 'you', 'haven', '’', 't', 'heard', ',', 'artificial', 'intelligence', 'is', 'the', 'hot', 'new', 'thing', '.', 'generative', 'ai', 'seems', 'to', 'be', 'on', 'the', 'lips', 'of', 'every', 'venture', 'capitalist', ',', 'entrepreneur', ',', 'fortune', '500', 'ceo', 'and', 'journalist', 'these', 'days', ',', 'from', 'silicon', 'valley', 'to', 'davos', '.', 'to', 'those', 'who', 'started', 'paying', 'real', 'attention', 'to', 'ai', 'in', '2022', ',', 'it', 'may', 'seem', 'that', 'technologies', 'like', 'chatgpt', 'and', 'stable', 'diffusion', 'came', 'out', 'of', 'nowhere', 'to', 'take', 'the', 'world', 'by', 'storm', '.', 'they', 'didn', '’', 't', '.', 'back', 'in', '2020', ',', 'we', 'wrote', 'an', 'article', 'in', 'this', 'column', 'predicting', 'that', 'generative', 'ai', 'would', 'be', 'one', 'of', 'the', 'pillars', 'of', 'the', 'next', 'generation', 'of', 'artificial', 'intelligence', '.', 'since', 'at', 'least', 'the', 'release', 'of', 'gpt-2', 'in', '2019', ',', 'it', 'has', 'been', 'clear', 'to', 'those', 'working', 'in', 'the', 'field', 'that', 'generative', 'language', 'models', 'were', 'poised', 'to', 'unleash', 'vast', 'economic', 'and', 'societal', 'transformation', '.', 'similarly', ',', 'while', 'text-to-image', 'models', 'only', 'captured', 'the', 'public', '’', 's', 'attention', 'last', 'summer', ',', 'the', 'technology', '’', 's', 'ascendance', 'has', 'appeared', 'inevitable', 'since', 'openai', 'released', 'the', 'original', 'dall-e', 'in', 'january', '2021', '.', '(', 'we', 'wrote', 'an', 'article', 'making', 'this', 'argument', 'days', 'after', 'the', 'release', 'of', 'the', 'original', 'dall-e.', ')', 'by', 'this', 'same', 'token', ',', 'it', 'is', 'important', 'to', 'remember', 'that', 'the', 'current', 'state', 'of', 'the', 'art', 'in', 'ai', 'is', 'far', 'from', 'an', 'end', 'state', 'for', 'ai', '’', 's', 'capabilities', '.', 'on', 'the', 'contrary', ',', 'the', 'frontiers', 'of', 'artificial', 'intelligence', 'have', 'never', 'advanced', 'more', 'rapidly', 'than', 'they', 'are', 'right', 'now', '.', 'as', 'amazing', 'as', 'chatgpt', 'seems', 'to', 'us', 'at', 'the', 'moment', ',', 'it', 'is', 'a', 'mere', 'stepping', 'stone', 'to', 'what', 'comes', 'next', '.', 'what', 'will', 'the', 'next', 'generation', 'of', 'large', 'language', 'models', '(', 'llms', ')', 'look', 'like', '?', 'the', 'answer', 'to', 'this', 'question', 'is', 'already', 'out', 'there', ',', 'under', 'development', 'at', 'ai', 'startups', 'and', 'research', 'groups', 'at', 'this', 'very', 'moment', '.', 'this', 'article', 'highlights', 'three', 'emerging', 'areas', 'that', 'will', 'help', 'define', 'the', 'next', 'wave', 'of', 'innovation', 'in', 'generative', 'ai', 'and', 'llms', '.', 'for', 'those', 'looking', 'to', 'remain', 'ahead', 'of', 'the', 'curve', 'in', 'this', 'fast-changing', 'world—read', 'on', '.', 'consider', 'how', 'humans', 'think', 'and', 'learn', '.', 'we', 'collect', 'knowledge', 'and', 'perspective', 'from', 'external', 'sources', 'of', 'information—say', ',', 'by', 'reading', 'a', 'book', '.', 'but', 'we', 'also', 'generate', 'novel', 'ideas', 'and', 'insights', 'on', 'our', 'own', ',', 'by', 'reflecting', 'on', 'a', 'topic', 'or', 'thinking', 'through', 'a', 'problem', 'in', 'our', 'minds', '.', 'we', 'are', 'able', 'to', 'deepen', 'our', 'understanding', 'of', 'the', 'world', 'via', 'internal', 'reflection', 'and', 'analysis', 'not', 'directly', 'tied', 'to', 'any', 'new', 'external', 'input', '.', 'a', 'new', 'avenue', 'of', 'ai', 'research', 'seeks', 'to', 'enable', 'large', 'language', 'models', 'to', 'do', 'something', 'analogous', ',', 'effectively', 'bootstrapping', 'their', 'own', 'intelligence', '.', 'as', 'part', 'of', 'their', 'training', ',', 'today', '’', 's', 'llms', 'ingest', 'much', 'of', 'the', 'world', '’', 's', 'accumulated', 'written', 'information', '(', 'e.g.', ',', 'wikipedia', ',', 'books', ',', 'news', 'articles', ')', '.', 'what', 'if', 'these', 'models', ',', 'once', 'trained', ',', 'could', 'use', 'all', 'the', 'knowledge', 'that', 'they', 'have', 'absorbed', 'from', 'these', 'sources', 'to', 'produce', 'new', 'written', 'content—and', 'then', 'use', 'that', 'content', 'as', 'additional', 'training', 'data', 'in', 'order', 'to', 'improve', 'themselves', '?', 'initial', 'work', 'suggests', 'that', 'this', 'approach', 'may', 'be', 'possible—and', 'powerful', '.', 'in', 'one', 'recent', 'research', 'effort', ',', 'aptly', 'titled', '“', 'large', 'language', 'models', 'can', 'self-improve', ',', '”', 'a', 'group', 'of', 'google', 'researchers', 'built', 'an', 'llm', 'that', 'can', 'come', 'up', 'with', 'a', 'set', 'of', 'questions', ',', 'generate', 'detailed', 'answers', 'to', 'those', 'questions', ',', 'filter', 'its', 'own', 'answers', 'for', 'the', 'most', 'high-quality', 'output', ',', 'and', 'then', 'fine-tune', 'itself', 'on', 'the', 'curated', 'answers', '.', 'remarkably', ',', 'this', 'leads', 'to', 'new', 'state-of-the-art', 'performance', 'on', 'various', 'language', 'tasks', '.', 'for', 'instance', ',', 'the', 'model', '’', 's', 'performance', 'increased', 'from', '74.2', '%', 'to', '82.1', '%', 'on', 'gsm8k', 'and', 'from', '78.2', '%', 'to', '83.0', '%', 'on', 'drop', ',', 'two', 'popular', 'benchmarks', 'used', 'to', 'evaluate', 'llm', 'performance', '.', 'another', 'recent', 'work', 'builds', 'on', 'an', 'important', 'llm', 'method', 'called', '“', 'instruction', 'fine-tuning', ',', '”', 'which', 'lies', 'at', 'the', 'core', 'of', 'products', 'like', 'chatgpt', '.', 'whereas', 'chatgpt', 'and', 'other', 'instruction', 'fine-tuned', 'models', 'rely', 'on', 'human-written', 'instructions', ',', 'this', 'research', 'group', 'built', 'a', 'model', 'that', 'can', 'generate', 'its', 'own', 'natural', 'language', 'instructions', 'and', 'then', 'fine-tune', 'itself', 'on', 'those', 'instructions', '.', 'the', 'performance', 'gains', 'are', 'dramatic', ':', 'this', 'method', 'improves', 'the', 'performance', 'of', 'the', 'base', 'gpt-3', 'model', 'by', '33', '%', ',', 'nearly', 'matching', 'the', 'performance', 'of', 'openai', '’', 's', 'own', 'instruction-tuned', 'model', '.', 'in', 'a', 'thematically', 'related', 'work', ',', 'researchers', 'from', 'google', 'and', 'carnegie', 'mellon', 'show', 'that', 'if', 'a', 'large', 'language', 'model', ',', 'when', 'presented', 'with', 'a', 'question', ',', 'first', 'recites', 'to', 'itself', 'what', 'it', 'knows', 'about', 'the', 'topic', 'before', 'responding', ',', 'it', 'provides', 'more', 'accurate', 'and', 'sophisticated', 'responses', '.', 'this', 'can', 'be', 'loosely', 'analogized', 'to', 'a', 'human', 'in', 'conversation', 'who', ',', 'rather', 'than', 'blurting', 'out', 'the', 'first', 'thing', 'that', 'comes', 'to', 'mind', 'on', 'a', 'topic', ',', 'searches', 'her', 'memory', 'and', 'reflects', 'on', 'her', 'beliefs', 'before', 'sharing', 'a', 'perspective', '.', 'when', 'people', 'first', 'hear', 'about', 'this', 'line', 'of', 'research', ',', 'a', 'conceptual', 'objection', 'often', 'arises—isn', '’', 't', 'this', 'all', 'circular', '?', 'how', 'can', 'a', 'model', 'produce', 'data', 'that', 'the', 'model', 'can', 'then', 'consume', 'to', 'improve', 'itself', '?', 'if', 'the', 'new', 'data', 'came', 'from', 'the', 'model', 'in', 'the', 'first', 'place', ',', 'shouldn', '’', 't', 'the', '“', 'knowledge', '”', 'or', '“', 'signal', '”', 'that', 'it', 'contains', 'already', 'be', 'incorporated', 'in', 'the', 'model', '?', 'this', 'objection', 'makes', 'sense', 'if', 'we', 'conceive', 'of', 'large', 'language', 'models', 'as', 'databases', ',', 'storing', 'information', 'from', 'their', 'training', 'data', 'and', 'reproducing', 'it', 'in', 'different', 'combinations', 'when', 'prompted', '.', 'but—uncomfortable', 'or', 'even', 'eerie', 'as', 'it', 'may', 'sound—we', 'are', 'better', 'off', 'instead', 'conceiving', 'of', 'large', 'language', 'models', 'along', 'the', 'lines', 'of', 'the', 'human', 'brain', '(', 'no', ',', 'the', 'analogy', 'is', 'of', 'course', 'not', 'perfect', '!', ')', '.', 'we', 'humans', 'ingest', 'a', 'tremendous', 'amount', 'of', 'data', 'from', 'the', 'world', 'that', 'alters', 'the', 'neural', 'connections', 'in', 'our', 'brains', 'in', 'imponderable', ',', 'innumerable', 'ways', '.', 'through', 'introspection', ',', 'writing', ',', 'conversation—sometimes', 'just', 'a', 'good', 'night', '’', 's', 'sleep—our', 'brains', 'can', 'then', 'produce', 'new', 'insights', 'that', 'had', 'not', 'previously', 'been', 'in', 'our', 'minds', 'nor', 'in', 'any', 'information', 'source', 'out', 'in', 'the', 'world', '.', 'if', 'we', 'internalize', 'these', 'new', 'insights', ',', 'they', 'can', 'make', 'us', 'wiser', '.', 'the', 'idea', 'that', 'llms', 'can', 'generate', 'their', 'own', 'training', 'data', 'is', 'particularly', 'important', 'in', 'light', 'of', 'the', 'fact', 'that', 'the', 'world', 'may', 'soon', 'run', 'out', 'of', 'text', 'training', 'data', '.', 'this', 'is', 'not', 'yet', 'a', 'widely', 'appreciated', 'problem', ',', 'but', 'it', 'is', 'one', 'that', 'many', 'ai', 'researchers', 'are', 'worried', 'about', '.', 'by', 'one', 'estimate', ',', 'the', 'world', '’', 's', 'total', 'stock', 'of', 'usable', 'text', 'data', 'is', 'between', '4.6', 'trillion', 'and', '17.2', 'trillion', 'tokens', '.', 'this', 'includes', 'all', 'the', 'world', '’', 's', 'books', ',', 'all', 'scientific', 'papers', ',', 'all', 'news', 'articles', ',', 'all', 'of', 'wikipedia', ',', 'all', 'publicly', 'available', 'code', ',', 'and', 'much', 'of', 'the', 'rest', 'of', 'the', 'internet', ',', 'filtered', 'for', 'quality', '(', 'e.g.', ',', 'webpages', ',', 'blogs', ',', 'social', 'media', ')', '.', 'another', 'recent', 'estimate', 'puts', 'the', 'total', 'figure', 'at', '3.2', 'trillion', 'tokens', '.', 'deepmind', '’', 's', 'chinchilla', ',', 'one', 'of', 'today', '’', 's', 'leading', 'llms', ',', 'was', 'trained', 'on', '1.4', 'trillion', 'tokens', '.', 'in', 'other', 'words', ',', 'we', 'may', 'be', 'well', 'within', 'one', 'order', 'of', 'magnitude', 'of', 'exhausting', 'the', 'world', '’', 's', 'entire', 'supply', 'of', 'useful', 'language', 'training', 'data', '.', 'if', 'large', 'language', 'models', 'are', 'able', 'to', 'generate', 'their', 'own', 'training', 'data', 'and', 'use', 'it', 'to', 'continue', 'self-improving', ',', 'this', 'could', 'render', 'irrelevant', 'the', 'looming', 'data', 'shortage', '.', 'it', 'would', 'represent', 'a', 'mind-bending', 'leap', 'forward', 'for', 'llms', '.', 'a', 'popular', 'narrative', 'these', 'days', 'is', 'that', 'chatgpt', 'and', 'conversational', 'llms', 'like', 'it', 'are', 'on', 'the', 'verge', 'of', 'replacing', 'google', 'search', 'as', 'the', 'world', '’', 's', 'go-to', 'source', 'for', 'information', ',', 'disrupting', 'the', 'once-mighty', 'tech', 'giant', 'like', 'blockbuster', 'or', 'kodak', 'were', 'disrupted', 'before', 'it', '.', 'this', 'narrative', 'badly', 'oversimplifies', 'things', '.', 'llms', 'as', 'they', 'exist', 'today', 'will', 'never', 'replace', 'google', 'search', '.', 'why', 'not', '?', 'in', 'short', ',', 'because', 'today', '’', 's', 'llms', 'make', 'stuff', 'up', '.', 'as', 'powerful', 'as', 'they', 'are', ',', 'large', 'language', 'models', 'regularly', 'produce', 'inaccurate', ',', 'misleading', 'or', 'false', 'information', '(', 'and', 'present', 'it', 'confidently', 'and', 'convincingly', ')', '.', 'examples', 'abound', 'of', 'chatgpt', '’', 's', '“', 'hallucinations', '”', '(', 'as', 'these', 'misstatements', 'are', 'referred', 'to', ')', '.', 'this', 'is', 'not', 'to', 'single', 'out', 'chatgpt', ';', 'every', 'generative', 'language', 'model', 'in', 'existence', 'today', 'hallucinates', 'in', 'similar', 'ways', '.', 'to', 'give', 'a', 'few', 'examples', ':', 'it', 'recommends', 'books', 'that', 'don', '’', 't', 'exist', ';', 'it', 'insists', 'that', 'the', 'number', '220', 'is', 'less', 'than', '200', ';', 'it', 'is', 'unsure', 'whether', 'abraham', 'lincoln', '’', 's', 'assassin', 'was', 'on', 'the', 'same', 'continent', 'as', 'lincoln', 'at', 'the', 'time', 'of', 'the', 'assassination', ';', 'it', 'provides', 'plausible-sounding', 'but', 'incorrect', 'explanations', 'of', 'concepts', 'like', 'bayes', '’', 'theorem', '.', 'most', 'users', 'will', 'not', 'accept', 'a', 'search', 'engine', 'that', 'gets', 'basic', 'facts', 'like', 'these', 'wrong', 'some', 'of', 'the', 'time', ';', 'even', '99', '%', 'accuracy', 'will', 'not', 'be', 'good', 'enough', 'for', 'broad', 'market', 'adoption', '.', 'openai', 'ceo', 'sam', 'altman', 'himself', 'acknowledges', 'this', ',', 'recently', 'cautioning', ':', '“', 'chatgpt', 'is', 'incredibly', 'limited', ',', 'but', 'good', 'enough', 'at', 'some', 'things', 'to', 'create', 'a', 'misleading', 'impression', 'of', 'greatness', '.', 'it', \"'s\", 'a', 'mistake', 'to', 'be', 'relying', 'on', 'it', 'for', 'anything', 'important', 'right', 'now.', '”', 'it', 'is', 'an', 'open', 'question', 'whether', 'llms', '’', 'hallucination', 'problem', 'can', 'be', 'solved', 'via', 'incremental', 'improvements', 'to', 'existing', 'architectures', ',', 'or', 'whether', 'a', 'more', 'fundamental', 'paradigm', 'shift', 'in', 'ai', 'methodologies', 'will', 'be', 'necessary', 'to', 'give', 'ai', 'common', 'sense', 'and', 'real', 'understanding', '.', 'deep', 'learning', 'pioneer', 'yann', 'lecun', ',', 'for', 'one', ',', 'believes', 'the', 'latter', '.', 'lecun', '’', 's', 'contrarian', 'perspective', 'may', 'prove', 'correct', ';', 'time', 'will', 'tell', '.', 'in', 'the', 'nearer', 'term', ',', 'though', ',', 'a', 'set', 'of', 'promising', 'innovations', 'offers', 'to', 'at', 'least', 'mitigate', 'llms', '’', 'factual', 'unreliability', '.', 'these', 'new', 'methods', 'will', 'play', 'an', 'essential', 'role', 'in', 'preparing', 'llms', 'for', 'widespread', 'real-world', 'deployment', '.', 'two', 'related', 'capabilities', 'lie', 'at', 'the', 'heart', 'of', 'current', 'efforts', 'to', 'make', 'language', 'models', 'more', 'accurate', ':', '(', '1', ')', 'the', 'ability', 'for', 'llms', 'to', 'retrieve', 'information', 'from', 'external', 'sources', ',', 'and', '(', '2', ')', 'the', 'ability', 'for', 'llms', 'to', 'provide', 'references', 'and', 'citations', 'for', 'the', 'information', 'they', 'provide', '.', 'chatgpt', 'is', 'limited', 'to', 'the', 'information', 'that', 'is', 'already', 'stored', 'inside', 'of', 'it', ',', 'captured', 'in', 'its', 'static', 'weights', '.', '(', 'this', 'is', 'why', 'it', 'is', 'not', 'able', 'to', 'discuss', 'events', 'that', 'occurred', 'after', '2021', ',', 'when', 'the', 'model', 'was', 'trained', '.', ')', 'being', 'able', 'to', 'pull', 'in', 'information', 'from', 'external', 'sources', 'will', 'empower', 'llms', 'to', 'access', 'the', 'most', 'accurate', 'and', 'up-to-date', 'information', 'available', ',', 'even', 'when', 'that', 'information', 'changes', 'frequently', '(', 'say', ',', 'companies', '’', 'stock', 'prices', ')', '.', 'of', 'course', ',', 'having', 'access', 'to', 'an', 'external', 'information', 'source', 'does', 'not', 'by', 'itself', 'guarantee', 'that', 'llms', 'will', 'retrieve', 'the', 'most', 'accurate', 'and', 'relevant', 'information', '.', 'one', 'important', 'way', 'for', 'llms', 'to', 'increase', 'transparency', 'and', 'trust', 'with', 'human', 'users', 'is', 'to', 'include', 'references', 'to', 'the', 'source', '(', 's', ')', 'from', 'which', 'they', 'retrieved', 'the', 'information', '.', 'such', 'citations', 'allow', 'human', 'users', 'to', 'audit', 'the', 'information', 'source', 'as', 'needed', 'in', 'order', 'to', 'decide', 'for', 'themselves', 'on', 'its', 'reliability', '.', 'important', 'early', 'work', 'in', 'this', 'field', 'includes', 'models', 'like', 'realm', '(', 'from', 'google', ')', 'and', 'rag', '(', 'from', 'facebook', ')', ',', 'both', 'published', 'in', '2020.', 'with', 'the', 'rise', 'of', 'conversational', 'llms', 'in', 'recent', 'months', ',', 'research', 'in', 'this', 'area', 'is', 'now', 'rapidly', 'accelerating', '.', 'last', 'year', ',', 'openai', 'published', 'a', 'fine-tuned', 'version', 'of', 'its', 'gpt', 'model', 'named', 'webgpt', 'that', 'can', 'browse', 'the', 'internet', 'using', 'microsoft', 'bing', 'in', 'order', 'to', 'provide', 'more', 'accurate', 'and', 'in-depth', 'responses', 'to', 'prompts', '.', 'webgpt', 'navigates', 'the', 'internet', 'much', 'like', 'a', 'human', 'does', ':', 'it', 'can', 'submit', 'search', 'queries', 'to', 'bing', ',', 'follow', 'links', ',', 'scroll', 'up', 'and', 'down', 'on', 'webpages', ',', 'and', 'use', 'functions', 'like', 'ctrl+f', 'to', 'find', 'terms', '.', 'when', 'the', 'model', 'finds', 'relevant', 'information', 'on', 'the', 'internet', 'that', 'it', 'incorporates', 'into', 'its', 'output', ',', 'it', 'provides', 'citations', 'so', 'that', 'the', 'human', 'user', 'can', 'see', 'where', 'the', 'information', 'came', 'from', '.', 'the', 'results', 'are', 'encouraging', ':', 'for', 'the', 'same', 'query', ',', 'webgpt', '’', 's', 'responses', 'are', 'preferred', 'to', 'responses', 'written', 'by', 'human', 'subjects', '56', '%', 'of', 'the', 'time', 'and', 'are', 'preferred', 'to', 'the', 'highest-rated', 'responses', 'on', 'reddit', '69', '%', 'of', 'the', 'time', '.', 'deepmind', 'is', 'also', 'pursuing', 'research', 'along', 'these', 'lines', '.', 'a', 'few', 'months', 'ago', ',', 'deepmind', 'published', 'a', 'new', 'model', 'named', 'sparrow', '.', 'like', 'chatgpt', ',', 'sparrow', 'is', 'dialogue-based', ';', 'like', 'webgpt', ',', 'it', 'can', 'search', 'the', 'internet', 'for', 'information', 'and', 'provide', 'citations', 'for', 'its', 'assertions', '.', 'sparrow', 'builds', 'on', 'important', 'earlier', 'work', 'out', 'of', 'deepmind', 'including', 'spalm', ',', 'retro', 'and', 'gophercite', '.', 'deepmind', \"'s\", 'sparrow', 'model', 'in', 'action', '.', 'as', 'shown', 'here', ',', 'sparrow', 'provides', 'quotations', 'and', 'links', 'to', 'support', '...', '[', '+', ']', 'its', 'statements', ',', 'increasing', 'their', 'accuracy', 'and', 'trustworthiness', '.', 'the', 'deepmind', 'researchers', 'find', 'that', 'sparrow', '’', 's', 'citations', 'are', 'helpful', 'and', 'accurate', '78', '%', 'of', 'the', 'time—suggesting', 'both', 'that', 'this', 'research', 'approach', 'is', 'promising', 'and', 'that', 'the', 'problem', 'of', 'llm', 'inaccuracy', 'is', 'far', 'from', 'solved', '.', 'younger', 'startups', 'including', 'you.com', 'and', 'perplexity', 'have', 'also', 'recently', 'launched', 'llm-powered', 'conversational', 'search', 'interfaces', 'with', 'the', 'ability', 'to', 'retrieve', 'information', 'from', 'external', 'sources', 'and', 'cite', 'references', '.', 'these', 'products', 'are', 'available', 'for', 'public', 'use', 'today', '.', 'llms', '’', 'greatest', 'shortcoming', 'is', 'their', 'unreliability', ',', 'their', 'stubborn', 'tendency', 'to', 'confidently', 'provide', 'inaccurate', 'information', '.', 'language', 'models', 'promise', 'to', 'reshape', 'every', 'sector', 'of', 'our', 'economy', ',', 'but', 'they', 'will', 'never', 'reach', 'their', 'full', 'potential', 'until', 'this', 'problem', 'is', 'addressed', '.', 'expect', 'to', 'see', 'plenty', 'of', 'activity', 'and', 'innovation', 'in', 'this', 'area', 'in', 'the', 'months', 'ahead', '.', 'today', '’', 's', 'most', 'prominent', 'large', 'language', 'models', 'all', 'have', 'effectively', 'the', 'same', 'architecture', '.', 'meta', 'ai', 'chief', 'yann', 'lecun', 'said', 'recently', ':', '“', 'in', 'terms', 'of', 'underlying', 'techniques', ',', 'chatgpt', 'is', 'not', 'particularly', 'innovative', '.', 'it', '’', 's', 'nothing', 'revolutionary', ',', 'although', 'that', '’', 's', 'the', 'way', 'it', '’', 's', 'perceived', 'in', 'the', 'public', '.', 'it', '’', 's', 'just', 'that', ',', 'you', 'know', ',', 'it', '’', 's', 'well', 'put', 'together', ',', 'it', '’', 's', 'nicely', 'done.', '”', 'lecun', '’', 's', 'statement', 'stirred', 'up', 'plenty', 'of', 'controversy', 'and', 'twitter', 'debate', '.', 'but', 'the', 'simple', 'fact', 'is', 'that', 'he', 'is', 'correct', ',', 'as', 'no', 'serious', 'ai', 'researcher', 'would', 'dispute', '.', 'all', 'of', 'today', '’', 's', 'well-known', 'language', 'models—e.g.', ',', 'gpt-3', 'from', 'openai', ',', 'palm', 'or', 'lamda', 'from', 'google', ',', 'galactica', 'or', 'opt', 'from', 'meta', ',', 'megatron-turing', 'from', 'nvidia/microsoft', ',', 'jurassic-1', 'from', 'ai21', 'labs—are', 'built', 'in', 'the', 'same', 'basic', 'way', '.', 'they', 'are', 'autoregressive', ',', 'self-supervised', ',', 'pre-trained', ',', 'densely', 'activated', 'transformer-based', 'models', '.', 'to', 'be', 'sure', ',', 'variations', 'among', 'these', 'models', 'exist', ':', 'their', 'size', '(', 'parameter', 'count', ')', ',', 'the', 'data', 'they', 'are', 'trained', 'on', ',', 'the', 'optimization', 'algorithm', 'used', ',', 'the', 'batch', 'size', ',', 'the', 'number', 'of', 'hidden', 'layers', ',', 'whether', 'they', 'are', 'instruction', 'fine-tuned', ',', 'and', 'so', 'on', '.', 'these', 'variations', 'can', 'translate', 'to', 'meaningful', 'performance', 'differences', '.', 'the', 'core', 'architectures', ',', 'though', ',', 'vary', 'little', '.', 'yet', 'momentum', 'is', 'building', 'behind', 'an', 'intriguingly', 'different', 'architectural', 'approach', 'to', 'language', 'models', 'known', 'as', 'sparse', 'expert', 'models', '.', 'while', 'the', 'idea', 'has', 'been', 'around', 'for', 'decades', ',', 'it', 'has', 'only', 'recently', 'reemerged', 'and', 'begun', 'to', 'gain', 'in', 'popularity', '.', 'all', 'of', 'the', 'models', 'mentioned', 'above', 'are', 'dense', '.', 'this', 'means', 'that', 'every', 'time', 'the', 'model', 'runs', ',', 'every', 'single', 'one', 'of', 'its', 'parameters', 'is', 'used', '.', 'every', 'time', 'you', 'submit', 'a', 'prompt', 'to', 'gpt-3', ',', 'for', 'instance', ',', 'all', '175', 'billion', 'of', 'the', 'model', '’', 's', 'parameters', 'are', 'activated', 'in', 'order', 'to', 'produce', 'its', 'response', '.', 'but', 'what', 'if', 'a', 'model', 'were', 'able', 'to', 'call', 'upon', 'only', 'the', 'most', 'relevant', 'subset', 'of', 'its', 'parameters', 'in', 'order', 'to', 'respond', 'to', 'a', 'given', 'query', '?', 'this', 'is', 'the', 'basic', 'concept', 'behind', 'sparse', 'expert', 'models', '.', 'the', 'defining', 'characteristic', 'of', 'sparse', 'models', 'is', 'that', 'they', 'don', '’', 't', 'activate', 'all', 'of', 'their', 'parameters', 'for', 'a', 'given', 'input', ',', 'but', 'rather', 'only', 'those', 'parameters', 'that', 'are', 'helpful', 'in', 'order', 'to', 'handle', 'the', 'input', '.', 'model', 'sparsity', 'thus', 'decouples', 'a', 'model', '’', 's', 'total', 'parameter', 'count', 'from', 'its', 'compute', 'requirements', '.', 'this', 'leads', 'to', 'sparse', 'expert', 'models', '’', 'key', 'advantage', ':', 'they', 'can', 'be', 'both', 'larger', 'and', 'less', 'computationally', 'demanding', 'than', 'dense', 'models', '.', 'why', 'are', 'they', 'called', 'sparse', 'expert', 'models', '?', 'because', 'sparse', 'models', 'can', 'be', 'thought', 'of', 'as', 'consisting', 'of', 'a', 'collection', 'of', '“', 'sub-models', '”', 'that', 'serve', 'as', 'experts', 'on', 'different', 'topics', '.', 'depending', 'on', 'the', 'prompt', 'presented', 'to', 'the', 'model', ',', 'the', 'most', 'relevant', 'experts', 'within', 'the', 'model', 'are', 'activated', 'while', 'the', 'other', 'experts', 'remain', 'inactive', '.', 'a', 'prompt', 'posed', 'in', 'russian', ',', 'for', 'instance', ',', 'would', 'only', 'activate', 'the', '“', 'experts', '”', 'within', 'a', 'model', 'that', 'can', 'understand', 'and', 'respond', 'in', 'russian', ',', 'efficiently', 'bypassing', 'the', 'rest', 'of', 'the', 'model', '.', 'all', 'of', 'today', '’', 's', 'largest', 'llms', 'are', 'sparse', '.', 'if', 'you', 'come', 'across', 'an', 'llm', 'with', 'more', 'than', '1', 'trillion', 'parameters', ',', 'you', 'can', 'safely', 'assume', 'that', 'it', 'is', 'sparse', '.', 'this', 'includes', 'google', '’', 's', 'switch', 'transformer', '(', '1.6', 'trillion', 'parameters', ')', ',', 'google', '’', 's', 'glam', '(', '1.2', 'trillion', 'parameters', ')', 'and', 'meta', '’', 's', 'mixture', 'of', 'experts', 'model', '(', '1.1', 'trillion', 'parameters', ')', '.', '“', 'much', 'of', 'the', 'recent', 'progress', 'in', 'ai', 'has', 'come', 'from', 'training', 'larger', 'and', 'larger', 'models', ',', '”', 'said', 'mikel', 'artetxe', ',', 'who', 'led', 'meta', '’', 's', 'research', 'on', 'sparse', 'models', 'before', 'resigning', 'to', 'cofound', 'a', 'stealth', 'llm', 'startup', '.', '“', 'gpt-3', ',', 'for', 'instance', ',', 'is', 'more', 'than', '100', 'times', 'larger', 'than', 'gpt-2', '.', 'but', 'when', 'we', 'double', 'the', 'size', 'of', 'a', 'dense', 'model', ',', 'we', 'also', 'make', 'it', 'twice', 'as', 'slow', '.', 'sparse', 'models', 'allow', 'us', 'to', 'train', 'larger', 'models', 'without', 'the', 'increase', 'in', 'runtime.', '”', 'recent', 'research', 'on', 'sparse', 'expert', 'models', 'suggests', 'that', 'this', 'architecture', 'holds', 'massive', 'potential', '.', 'glam', ',', 'a', 'sparse', 'expert', 'model', 'developed', 'last', 'year', 'by', 'google', ',', 'is', '7', 'times', 'larger', 'than', 'gpt-3', ',', 'requires', 'two-thirds', 'less', 'energy', 'to', 'train', ',', 'requires', 'half', 'as', 'much', 'compute', 'for', 'inference', ',', 'and', 'outperforms', 'gpt-3', 'on', 'a', 'wide', 'range', 'of', 'natural', 'language', 'tasks', '.', 'similar', 'work', 'on', 'sparse', 'models', 'out', 'of', 'meta', 'has', 'yielded', 'similarly', 'promising', 'results', '.', 'as', 'the', 'meta', 'researchers', 'summarize', ':', '“', 'we', 'find', 'that', 'sparse', 'models', 'can', 'achieve', 'similar', 'downstream', 'task', 'performance', 'as', 'dense', 'models', 'at', 'a', 'fraction', 'of', 'the', 'compute', '.', 'for', 'models', 'with', 'relatively', 'modest', 'compute', 'budgets', ',', 'a', 'sparse', 'model', 'can', 'perform', 'on', 'par', 'with', 'a', 'dense', 'model', 'that', 'requires', 'almost', 'four', 'times', 'as', 'much', 'compute.', '”', 'there', 'is', 'another', 'benefit', 'of', 'sparse', 'expert', 'models', 'that', 'is', 'worth', 'mentioning', ':', 'they', 'are', 'more', 'interpretable', 'than', 'dense', 'models', '.', 'interpretability—the', 'ability', 'for', 'a', 'human', 'to', 'understand', 'why', 'a', 'model', 'took', 'the', 'action', 'that', 'it', 'did—is', 'one', 'of', 'ai', '’', 's', 'greatest', 'weaknesses', 'today', '.', 'in', 'general', ',', 'today', '’', 's', 'neural', 'networks', 'are', 'uninterpretable', '“', 'black', 'boxes.', '”', 'this', 'can', 'limit', 'their', 'usefulness', 'in', 'the', 'real', 'world', ',', 'particularly', 'in', 'high-stakes', 'settings', 'like', 'healthcare', 'where', 'human', 'review', 'is', 'important', '.', 'sparse', 'expert', 'models', 'lend', 'themselves', 'more', 'naturally', 'to', 'interpretability', 'than', 'conventional', 'models', 'because', 'a', 'sparse', 'model', '’', 's', 'output', 'is', 'the', 'result', 'of', 'an', 'identifiable', ',', 'discrete', 'subset', 'of', 'parameters', 'within', 'the', 'model—namely', ',', 'the', '“', 'experts', '”', 'that', 'were', 'activated', '.', 'the', 'fact', 'that', 'humans', 'can', 'better', 'extract', 'understandable', 'explanations', 'from', 'sparse', 'models', 'about', 'their', 'behavior', 'may', 'prove', 'to', 'be', 'a', 'decisive', 'advantage', 'for', 'these', 'models', 'in', 'real-world', 'applications', '.', 'sparse', 'expert', 'models', 'are', 'not', 'in', 'widespread', 'use', 'today', '.', 'they', 'are', 'less', 'well', 'understood', 'and', 'more', 'technically', 'complex', 'to', 'build', 'than', 'dense', 'models', '.', 'yet', 'considering', 'their', 'potential', 'advantages', ',', 'most', 'of', 'all', 'their', 'computational', 'efficiency', ',', 'don', '’', 't', 'be', 'surprised', 'to', 'see', 'the', 'sparse', 'expert', 'architecture', 'become', 'more', 'prevalent', 'in', 'the', 'world', 'of', 'llms', 'going', 'forward', '.', 'in', 'the', 'words', 'of', 'graphcore', 'cto', 'simon', 'knowles', ':', '“', 'if', 'an', 'ai', 'can', 'do', 'many', 'things', ',', 'it', 'doesn', '’', 't', 'need', 'to', 'access', 'all', 'of', 'its', 'knowledge', 'to', 'do', 'one', 'thing', '.', 'it', '’', 's', 'completely', 'obvious', '.', 'this', 'is', 'how', 'your', 'brain', 'works', ',', 'and', 'it', '’', 's', 'also', 'how', 'an', 'ai', 'ought', 'to', 'work', '.', 'i', '’', 'd', 'be', 'surprised', 'if', ',', 'by', 'next', 'year', ',', 'anyone', 'is', 'building', 'dense', 'language', 'models.', '”', 'note', ':', 'the', 'author', 'is', 'a', 'partner', 'at', 'radical', 'ventures', ',', 'which', 'is', 'an', 'investor', 'in', 'you.com', '.']\n"
     ]
    }
   ],
   "source": [
    "#TASK 6 \n",
    "# Bag of Words \n",
    "import nltk \n",
    "text = \"full_text1\"\n",
    "from nltk.tokenize import word_tokenize \n",
    "result = word_tokenize(full_text1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38e8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "462ed9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai 6\n",
      "ceo 3\n",
      "sam 2\n",
      "altman 2\n",
      "( 20\n",
      "left 1\n",
      ") 20\n",
      "and 54\n",
      "meta 7\n",
      "ai 18\n",
      "chief 2\n",
      "yann 3\n",
      "lecun 5\n",
      "( 20\n",
      "right 3\n",
      ") 20\n",
      "have 5\n",
      "differing 1\n",
      "views 1\n",
      "on 33\n",
      "the 137\n",
      "future 1\n",
      "... 2\n",
      "[ 2\n",
      "+ 2\n",
      "] 2\n",
      "of 87\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      ". 126\n",
      "in 61\n",
      "case 1\n",
      "you 5\n",
      "haven 1\n",
      "’ 56\n",
      "t 8\n",
      "heard 1\n",
      ", 151\n",
      "artificial 3\n",
      "intelligence 4\n",
      "is 46\n",
      "the 137\n",
      "hot 1\n",
      "new 10\n",
      "thing 3\n",
      ". 126\n",
      "generative 5\n",
      "ai 18\n",
      "seems 2\n",
      "to 84\n",
      "be 16\n",
      "on 33\n",
      "the 137\n",
      "lips 1\n",
      "of 87\n",
      "every 6\n",
      "venture 1\n",
      "capitalist 1\n",
      ", 151\n",
      "entrepreneur 1\n",
      ", 151\n",
      "fortune 1\n",
      "500 1\n",
      "ceo 3\n",
      "and 54\n",
      "journalist 1\n",
      "these 13\n",
      "days 3\n",
      ", 151\n",
      "from 26\n",
      "silicon 1\n",
      "valley 1\n",
      "to 84\n",
      "davos 1\n",
      ". 126\n",
      "to 84\n",
      "those 6\n",
      "who 3\n",
      "started 1\n",
      "paying 1\n",
      "real 3\n",
      "attention 2\n",
      "to 84\n",
      "ai 18\n",
      "in 61\n",
      "2022 1\n",
      ", 151\n",
      "it 40\n",
      "may 7\n",
      "seem 1\n",
      "that 49\n",
      "technologies 1\n",
      "like 13\n",
      "chatgpt 11\n",
      "and 54\n",
      "stable 1\n",
      "diffusion 1\n",
      "came 3\n",
      "out 8\n",
      "of 87\n",
      "nowhere 1\n",
      "to 84\n",
      "take 1\n",
      "the 137\n",
      "world 12\n",
      "by 10\n",
      "storm 1\n",
      ". 126\n",
      "they 17\n",
      "didn 1\n",
      "’ 56\n",
      "t 8\n",
      ". 126\n",
      "back 1\n",
      "in 61\n",
      "2020 1\n",
      ", 151\n",
      "we 12\n",
      "wrote 2\n",
      "an 14\n",
      "article 3\n",
      "in 61\n",
      "this 34\n",
      "column 1\n",
      "predicting 1\n",
      "that 49\n",
      "generative 5\n",
      "ai 18\n",
      "would 4\n",
      "be 16\n",
      "one 11\n",
      "of 87\n",
      "the 137\n",
      "pillars 1\n",
      "of 87\n",
      "the 137\n",
      "next 5\n",
      "generation 2\n",
      "of 87\n",
      "artificial 3\n",
      "intelligence 4\n",
      ". 126\n",
      "since 2\n",
      "at 12\n",
      "least 2\n",
      "the 137\n",
      "release 2\n",
      "of 87\n",
      "gpt-2 2\n",
      "in 61\n",
      "2019 1\n",
      ", 151\n",
      "it 40\n",
      "has 6\n",
      "been 3\n",
      "clear 1\n",
      "to 84\n",
      "those 6\n",
      "working 1\n",
      "in 61\n",
      "the 137\n",
      "field 2\n",
      "that 49\n",
      "generative 5\n",
      "language 21\n",
      "models 44\n",
      "were 4\n",
      "poised 1\n",
      "to 84\n",
      "unleash 1\n",
      "vast 1\n",
      "economic 1\n",
      "and 54\n",
      "societal 1\n",
      "transformation 1\n",
      ". 126\n",
      "similarly 2\n",
      ", 151\n",
      "while 3\n",
      "text-to-image 1\n",
      "models 44\n",
      "only 5\n",
      "captured 2\n",
      "the 137\n",
      "public 3\n",
      "’ 56\n",
      "s 42\n",
      "attention 2\n",
      "last 3\n",
      "summer 1\n",
      ", 151\n",
      "the 137\n",
      "technology 1\n",
      "’ 56\n",
      "s 42\n",
      "ascendance 1\n",
      "has 6\n",
      "appeared 1\n",
      "inevitable 1\n",
      "since 2\n",
      "openai 6\n",
      "released 1\n",
      "the 137\n",
      "original 2\n",
      "dall-e 1\n",
      "in 61\n",
      "january 1\n",
      "2021 2\n",
      ". 126\n",
      "( 20\n",
      "we 12\n",
      "wrote 2\n",
      "an 14\n",
      "article 3\n",
      "making 1\n",
      "this 34\n",
      "argument 1\n",
      "days 3\n",
      "after 2\n",
      "the 137\n",
      "release 2\n",
      "of 87\n",
      "the 137\n",
      "original 2\n",
      "dall-e. 1\n",
      ") 20\n",
      "by 10\n",
      "this 34\n",
      "same 5\n",
      "token 1\n",
      ", 151\n",
      "it 40\n",
      "is 46\n",
      "important 8\n",
      "to 84\n",
      "remember 1\n",
      "that 49\n",
      "the 137\n",
      "current 2\n",
      "state 2\n",
      "of 87\n",
      "the 137\n",
      "art 1\n",
      "in 61\n",
      "ai 18\n",
      "is 46\n",
      "far 2\n",
      "from 26\n",
      "an 14\n",
      "end 1\n",
      "state 2\n",
      "for 29\n",
      "ai 18\n",
      "’ 56\n",
      "s 42\n",
      "capabilities 2\n",
      ". 126\n",
      "on 33\n",
      "the 137\n",
      "contrary 1\n",
      ", 151\n",
      "the 137\n",
      "frontiers 1\n",
      "of 87\n",
      "artificial 3\n",
      "intelligence 4\n",
      "have 5\n",
      "never 3\n",
      "advanced 1\n",
      "more 11\n",
      "rapidly 2\n",
      "than 11\n",
      "they 17\n",
      "are 27\n",
      "right 3\n",
      "now 2\n",
      ". 126\n",
      "as 23\n",
      "amazing 1\n",
      "as 23\n",
      "chatgpt 11\n",
      "seems 2\n",
      "to 84\n",
      "us 3\n",
      "at 12\n",
      "the 137\n",
      "moment 2\n",
      ", 151\n",
      "it 40\n",
      "is 46\n",
      "a 51\n",
      "mere 1\n",
      "stepping 1\n",
      "stone 1\n",
      "to 84\n",
      "what 5\n",
      "comes 2\n",
      "next 5\n",
      ". 126\n",
      "what 5\n",
      "will 11\n",
      "the 137\n",
      "next 5\n",
      "generation 2\n",
      "of 87\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      "( 20\n",
      "llms 21\n",
      ") 20\n",
      "look 1\n",
      "like 13\n",
      "? 8\n",
      "the 137\n",
      "answer 1\n",
      "to 84\n",
      "this 34\n",
      "question 3\n",
      "is 46\n",
      "already 3\n",
      "out 8\n",
      "there 2\n",
      ", 151\n",
      "under 1\n",
      "development 1\n",
      "at 12\n",
      "ai 18\n",
      "startups 2\n",
      "and 54\n",
      "research 10\n",
      "groups 1\n",
      "at 12\n",
      "this 34\n",
      "very 1\n",
      "moment 2\n",
      ". 126\n",
      "this 34\n",
      "article 3\n",
      "highlights 1\n",
      "three 1\n",
      "emerging 1\n",
      "areas 1\n",
      "that 49\n",
      "will 11\n",
      "help 1\n",
      "define 1\n",
      "the 137\n",
      "next 5\n",
      "wave 1\n",
      "of 87\n",
      "innovation 2\n",
      "in 61\n",
      "generative 5\n",
      "ai 18\n",
      "and 54\n",
      "llms 21\n",
      ". 126\n",
      "for 29\n",
      "those 6\n",
      "looking 1\n",
      "to 84\n",
      "remain 2\n",
      "ahead 2\n",
      "of 87\n",
      "the 137\n",
      "curve 1\n",
      "in 61\n",
      "this 34\n",
      "fast-changing 1\n",
      "world—read 1\n",
      "on 33\n",
      ". 126\n",
      "consider 1\n",
      "how 4\n",
      "humans 3\n",
      "think 1\n",
      "and 54\n",
      "learn 1\n",
      ". 126\n",
      "we 12\n",
      "collect 1\n",
      "knowledge 4\n",
      "and 54\n",
      "perspective 3\n",
      "from 26\n",
      "external 6\n",
      "sources 5\n",
      "of 87\n",
      "information—say 1\n",
      ", 151\n",
      "by 10\n",
      "reading 1\n",
      "a 51\n",
      "book 1\n",
      ". 126\n",
      "but 9\n",
      "we 12\n",
      "also 5\n",
      "generate 5\n",
      "novel 1\n",
      "ideas 1\n",
      "and 54\n",
      "insights 3\n",
      "on 33\n",
      "our 6\n",
      "own 7\n",
      ", 151\n",
      "by 10\n",
      "reflecting 1\n",
      "on 33\n",
      "a 51\n",
      "topic 3\n",
      "or 8\n",
      "thinking 1\n",
      "through 2\n",
      "a 51\n",
      "problem 5\n",
      "in 61\n",
      "our 6\n",
      "minds 2\n",
      ". 126\n",
      "we 12\n",
      "are 27\n",
      "able 5\n",
      "to 84\n",
      "deepen 1\n",
      "our 6\n",
      "understanding 2\n",
      "of 87\n",
      "the 137\n",
      "world 12\n",
      "via 2\n",
      "internal 1\n",
      "reflection 1\n",
      "and 54\n",
      "analysis 1\n",
      "not 12\n",
      "directly 1\n",
      "tied 1\n",
      "to 84\n",
      "any 2\n",
      "new 10\n",
      "external 6\n",
      "input 3\n",
      ". 126\n",
      "a 51\n",
      "new 10\n",
      "avenue 1\n",
      "of 87\n",
      "ai 18\n",
      "research 10\n",
      "seeks 1\n",
      "to 84\n",
      "enable 1\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      "to 84\n",
      "do 3\n",
      "something 1\n",
      "analogous 1\n",
      ", 151\n",
      "effectively 2\n",
      "bootstrapping 1\n",
      "their 15\n",
      "own 7\n",
      "intelligence 4\n",
      ". 126\n",
      "as 23\n",
      "part 1\n",
      "of 87\n",
      "their 15\n",
      "training 8\n",
      ", 151\n",
      "today 12\n",
      "’ 56\n",
      "s 42\n",
      "llms 21\n",
      "ingest 2\n",
      "much 6\n",
      "of 87\n",
      "the 137\n",
      "world 12\n",
      "’ 56\n",
      "s 42\n",
      "accumulated 1\n",
      "written 3\n",
      "information 20\n",
      "( 20\n",
      "e.g. 2\n",
      ", 151\n",
      "wikipedia 2\n",
      ", 151\n",
      "books 3\n",
      ", 151\n",
      "news 2\n",
      "articles 2\n",
      ") 20\n",
      ". 126\n",
      "what 5\n",
      "if 10\n",
      "these 13\n",
      "models 44\n",
      ", 151\n",
      "once 1\n",
      "trained 4\n",
      ", 151\n",
      "could 2\n",
      "use 6\n",
      "all 15\n",
      "the 137\n",
      "knowledge 4\n",
      "that 49\n",
      "they 17\n",
      "have 5\n",
      "absorbed 1\n",
      "from 26\n",
      "these 13\n",
      "sources 5\n",
      "to 84\n",
      "produce 5\n",
      "new 10\n",
      "written 3\n",
      "content—and 1\n",
      "then 5\n",
      "use 6\n",
      "that 49\n",
      "content 1\n",
      "as 23\n",
      "additional 1\n",
      "training 8\n",
      "data 12\n",
      "in 61\n",
      "order 7\n",
      "to 84\n",
      "improve 2\n",
      "themselves 3\n",
      "? 8\n",
      "initial 1\n",
      "work 7\n",
      "suggests 2\n",
      "that 49\n",
      "this 34\n",
      "approach 3\n",
      "may 7\n",
      "be 16\n",
      "possible—and 1\n",
      "powerful 2\n",
      ". 126\n",
      "in 61\n",
      "one 11\n",
      "recent 6\n",
      "research 10\n",
      "effort 1\n",
      ", 151\n",
      "aptly 1\n",
      "titled 1\n",
      "“ 15\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      "can 24\n",
      "self-improve 1\n",
      ", 151\n",
      "” 15\n",
      "a 51\n",
      "group 2\n",
      "of 87\n",
      "google 9\n",
      "researchers 5\n",
      "built 3\n",
      "an 14\n",
      "llm 6\n",
      "that 49\n",
      "can 24\n",
      "come 3\n",
      "up 4\n",
      "with 8\n",
      "a 51\n",
      "set 2\n",
      "of 87\n",
      "questions 2\n",
      ", 151\n",
      "generate 5\n",
      "detailed 1\n",
      "answers 3\n",
      "to 84\n",
      "those 6\n",
      "questions 2\n",
      ", 151\n",
      "filter 1\n",
      "its 13\n",
      "own 7\n",
      "answers 3\n",
      "for 29\n",
      "the 137\n",
      "most 8\n",
      "high-quality 1\n",
      "output 3\n",
      ", 151\n",
      "and 54\n",
      "then 5\n",
      "fine-tune 2\n",
      "itself 5\n",
      "on 33\n",
      "the 137\n",
      "curated 1\n",
      "answers 3\n",
      ". 126\n",
      "remarkably 1\n",
      ", 151\n",
      "this 34\n",
      "leads 2\n",
      "to 84\n",
      "new 10\n",
      "state-of-the-art 1\n",
      "performance 8\n",
      "on 33\n",
      "various 1\n",
      "language 21\n",
      "tasks 2\n",
      ". 126\n",
      "for 29\n",
      "instance 4\n",
      ", 151\n",
      "the 137\n",
      "model 31\n",
      "’ 56\n",
      "s 42\n",
      "performance 8\n",
      "increased 1\n",
      "from 26\n",
      "74.2 1\n",
      "% 9\n",
      "to 84\n",
      "82.1 1\n",
      "% 9\n",
      "on 33\n",
      "gsm8k 1\n",
      "and 54\n",
      "from 26\n",
      "78.2 1\n",
      "% 9\n",
      "to 84\n",
      "83.0 1\n",
      "% 9\n",
      "on 33\n",
      "drop 1\n",
      ", 151\n",
      "two 2\n",
      "popular 2\n",
      "benchmarks 1\n",
      "used 3\n",
      "to 84\n",
      "evaluate 1\n",
      "llm 6\n",
      "performance 8\n",
      ". 126\n",
      "another 3\n",
      "recent 6\n",
      "work 7\n",
      "builds 2\n",
      "on 33\n",
      "an 14\n",
      "important 8\n",
      "llm 6\n",
      "method 2\n",
      "called 2\n",
      "“ 15\n",
      "instruction 3\n",
      "fine-tuning 1\n",
      ", 151\n",
      "” 15\n",
      "which 3\n",
      "lies 1\n",
      "at 12\n",
      "the 137\n",
      "core 2\n",
      "of 87\n",
      "products 2\n",
      "like 13\n",
      "chatgpt 11\n",
      ". 126\n",
      "whereas 1\n",
      "chatgpt 11\n",
      "and 54\n",
      "other 3\n",
      "instruction 3\n",
      "fine-tuned 3\n",
      "models 44\n",
      "rely 1\n",
      "on 33\n",
      "human-written 1\n",
      "instructions 3\n",
      ", 151\n",
      "this 34\n",
      "research 10\n",
      "group 2\n",
      "built 3\n",
      "a 51\n",
      "model 31\n",
      "that 49\n",
      "can 24\n",
      "generate 5\n",
      "its 13\n",
      "own 7\n",
      "natural 2\n",
      "language 21\n",
      "instructions 3\n",
      "and 54\n",
      "then 5\n",
      "fine-tune 2\n",
      "itself 5\n",
      "on 33\n",
      "those 6\n",
      "instructions 3\n",
      ". 126\n",
      "the 137\n",
      "performance 8\n",
      "gains 1\n",
      "are 27\n",
      "dramatic 1\n",
      ": 13\n",
      "this 34\n",
      "method 2\n",
      "improves 1\n",
      "the 137\n",
      "performance 8\n",
      "of 87\n",
      "the 137\n",
      "base 1\n",
      "gpt-3 6\n",
      "model 31\n",
      "by 10\n",
      "33 1\n",
      "% 9\n",
      ", 151\n",
      "nearly 1\n",
      "matching 1\n",
      "the 137\n",
      "performance 8\n",
      "of 87\n",
      "openai 6\n",
      "’ 56\n",
      "s 42\n",
      "own 7\n",
      "instruction-tuned 1\n",
      "model 31\n",
      ". 126\n",
      "in 61\n",
      "a 51\n",
      "thematically 1\n",
      "related 2\n",
      "work 7\n",
      ", 151\n",
      "researchers 5\n",
      "from 26\n",
      "google 9\n",
      "and 54\n",
      "carnegie 1\n",
      "mellon 1\n",
      "show 1\n",
      "that 49\n",
      "if 10\n",
      "a 51\n",
      "large 10\n",
      "language 21\n",
      "model 31\n",
      ", 151\n",
      "when 7\n",
      "presented 2\n",
      "with 8\n",
      "a 51\n",
      "question 3\n",
      ", 151\n",
      "first 4\n",
      "recites 1\n",
      "to 84\n",
      "itself 5\n",
      "what 5\n",
      "it 40\n",
      "knows 1\n",
      "about 4\n",
      "the 137\n",
      "topic 3\n",
      "before 4\n",
      "responding 1\n",
      ", 151\n",
      "it 40\n",
      "provides 4\n",
      "more 11\n",
      "accurate 6\n",
      "and 54\n",
      "sophisticated 1\n",
      "responses 5\n",
      ". 126\n",
      "this 34\n",
      "can 24\n",
      "be 16\n",
      "loosely 1\n",
      "analogized 1\n",
      "to 84\n",
      "a 51\n",
      "human 9\n",
      "in 61\n",
      "conversation 1\n",
      "who 3\n",
      ", 151\n",
      "rather 2\n",
      "than 11\n",
      "blurting 1\n",
      "out 8\n",
      "the 137\n",
      "first 4\n",
      "thing 3\n",
      "that 49\n",
      "comes 2\n",
      "to 84\n",
      "mind 1\n",
      "on 33\n",
      "a 51\n",
      "topic 3\n",
      ", 151\n",
      "searches 1\n",
      "her 2\n",
      "memory 1\n",
      "and 54\n",
      "reflects 1\n",
      "on 33\n",
      "her 2\n",
      "beliefs 1\n",
      "before 4\n",
      "sharing 1\n",
      "a 51\n",
      "perspective 3\n",
      ". 126\n",
      "when 7\n",
      "people 1\n",
      "first 4\n",
      "hear 1\n",
      "about 4\n",
      "this 34\n",
      "line 1\n",
      "of 87\n",
      "research 10\n",
      ", 151\n",
      "a 51\n",
      "conceptual 1\n",
      "objection 2\n",
      "often 1\n",
      "arises—isn 1\n",
      "’ 56\n",
      "t 8\n",
      "this 34\n",
      "all 15\n",
      "circular 1\n",
      "? 8\n",
      "how 4\n",
      "can 24\n",
      "a 51\n",
      "model 31\n",
      "produce 5\n",
      "data 12\n",
      "that 49\n",
      "the 137\n",
      "model 31\n",
      "can 24\n",
      "then 5\n",
      "consume 1\n",
      "to 84\n",
      "improve 2\n",
      "itself 5\n",
      "? 8\n",
      "if 10\n",
      "the 137\n",
      "new 10\n",
      "data 12\n",
      "came 3\n",
      "from 26\n",
      "the 137\n",
      "model 31\n",
      "in 61\n",
      "the 137\n",
      "first 4\n",
      "place 1\n",
      ", 151\n",
      "shouldn 1\n",
      "’ 56\n",
      "t 8\n",
      "the 137\n",
      "“ 15\n",
      "knowledge 4\n",
      "” 15\n",
      "or 8\n",
      "“ 15\n",
      "signal 1\n",
      "” 15\n",
      "that 49\n",
      "it 40\n",
      "contains 1\n",
      "already 3\n",
      "be 16\n",
      "incorporated 1\n",
      "in 61\n",
      "the 137\n",
      "model 31\n",
      "? 8\n",
      "this 34\n",
      "objection 2\n",
      "makes 1\n",
      "sense 2\n",
      "if 10\n",
      "we 12\n",
      "conceive 1\n",
      "of 87\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      "as 23\n",
      "databases 1\n",
      ", 151\n",
      "storing 1\n",
      "information 20\n",
      "from 26\n",
      "their 15\n",
      "training 8\n",
      "data 12\n",
      "and 54\n",
      "reproducing 1\n",
      "it 40\n",
      "in 61\n",
      "different 3\n",
      "combinations 1\n",
      "when 7\n",
      "prompted 1\n",
      ". 126\n",
      "but—uncomfortable 1\n",
      "or 8\n",
      "even 3\n",
      "eerie 1\n",
      "as 23\n",
      "it 40\n",
      "may 7\n",
      "sound—we 1\n",
      "are 27\n",
      "better 2\n",
      "off 1\n",
      "instead 1\n",
      "conceiving 1\n",
      "of 87\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      "along 2\n",
      "the 137\n",
      "lines 2\n",
      "of 87\n",
      "the 137\n",
      "human 9\n",
      "brain 2\n",
      "( 20\n",
      "no 2\n",
      ", 151\n",
      "the 137\n",
      "analogy 1\n",
      "is 46\n",
      "of 87\n",
      "course 2\n",
      "not 12\n",
      "perfect 1\n",
      "! 1\n",
      ") 20\n",
      ". 126\n",
      "we 12\n",
      "humans 3\n",
      "ingest 2\n",
      "a 51\n",
      "tremendous 1\n",
      "amount 1\n",
      "of 87\n",
      "data 12\n",
      "from 26\n",
      "the 137\n",
      "world 12\n",
      "that 49\n",
      "alters 1\n",
      "the 137\n",
      "neural 2\n",
      "connections 1\n",
      "in 61\n",
      "our 6\n",
      "brains 2\n",
      "in 61\n",
      "imponderable 1\n",
      ", 151\n",
      "innumerable 1\n",
      "ways 2\n",
      ". 126\n",
      "through 2\n",
      "introspection 1\n",
      ", 151\n",
      "writing 1\n",
      ", 151\n",
      "conversation—sometimes 1\n",
      "just 2\n",
      "a 51\n",
      "good 3\n",
      "night 1\n",
      "’ 56\n",
      "s 42\n",
      "sleep—our 1\n",
      "brains 2\n",
      "can 24\n",
      "then 5\n",
      "produce 5\n",
      "new 10\n",
      "insights 3\n",
      "that 49\n",
      "had 1\n",
      "not 12\n",
      "previously 1\n",
      "been 3\n",
      "in 61\n",
      "our 6\n",
      "minds 2\n",
      "nor 1\n",
      "in 61\n",
      "any 2\n",
      "information 20\n",
      "source 5\n",
      "out 8\n",
      "in 61\n",
      "the 137\n",
      "world 12\n",
      ". 126\n",
      "if 10\n",
      "we 12\n",
      "internalize 1\n",
      "these 13\n",
      "new 10\n",
      "insights 3\n",
      ", 151\n",
      "they 17\n",
      "can 24\n",
      "make 4\n",
      "us 3\n",
      "wiser 1\n",
      ". 126\n",
      "the 137\n",
      "idea 2\n",
      "that 49\n",
      "llms 21\n",
      "can 24\n",
      "generate 5\n",
      "their 15\n",
      "own 7\n",
      "training 8\n",
      "data 12\n",
      "is 46\n",
      "particularly 3\n",
      "important 8\n",
      "in 61\n",
      "light 1\n",
      "of 87\n",
      "the 137\n",
      "fact 3\n",
      "that 49\n",
      "the 137\n",
      "world 12\n",
      "may 7\n",
      "soon 1\n",
      "run 1\n",
      "out 8\n",
      "of 87\n",
      "text 2\n",
      "training 8\n",
      "data 12\n",
      ". 126\n",
      "this 34\n",
      "is 46\n",
      "not 12\n",
      "yet 3\n",
      "a 51\n",
      "widely 1\n",
      "appreciated 1\n",
      "problem 5\n",
      ", 151\n",
      "but 9\n",
      "it 40\n",
      "is 46\n",
      "one 11\n",
      "that 49\n",
      "many 2\n",
      "ai 18\n",
      "researchers 5\n",
      "are 27\n",
      "worried 1\n",
      "about 4\n",
      ". 126\n",
      "by 10\n",
      "one 11\n",
      "estimate 2\n",
      ", 151\n",
      "the 137\n",
      "world 12\n",
      "’ 56\n",
      "s 42\n",
      "total 3\n",
      "stock 2\n",
      "of 87\n",
      "usable 1\n",
      "text 2\n",
      "data 12\n",
      "is 46\n",
      "between 1\n",
      "4.6 1\n",
      "trillion 8\n",
      "and 54\n",
      "17.2 1\n",
      "trillion 8\n",
      "tokens 3\n",
      ". 126\n",
      "this 34\n",
      "includes 3\n",
      "all 15\n",
      "the 137\n",
      "world 12\n",
      "’ 56\n",
      "s 42\n",
      "books 3\n",
      ", 151\n",
      "all 15\n",
      "scientific 1\n",
      "papers 1\n",
      ", 151\n",
      "all 15\n",
      "news 2\n",
      "articles 2\n",
      ", 151\n",
      "all 15\n",
      "of 87\n",
      "wikipedia 2\n",
      ", 151\n",
      "all 15\n",
      "publicly 1\n",
      "available 3\n",
      "code 1\n",
      ", 151\n",
      "and 54\n",
      "much 6\n",
      "of 87\n",
      "the 137\n",
      "rest 2\n",
      "of 87\n",
      "the 137\n",
      "internet 5\n",
      ", 151\n",
      "filtered 1\n",
      "for 29\n",
      "quality 1\n",
      "( 20\n",
      "e.g. 2\n",
      ", 151\n",
      "webpages 2\n",
      ", 151\n",
      "blogs 1\n",
      ", 151\n",
      "social 1\n",
      "media 1\n",
      ") 20\n",
      ". 126\n",
      "another 3\n",
      "recent 6\n",
      "estimate 2\n",
      "puts 1\n",
      "the 137\n",
      "total 3\n",
      "figure 1\n",
      "at 12\n",
      "3.2 1\n",
      "trillion 8\n",
      "tokens 3\n",
      ". 126\n",
      "deepmind 6\n",
      "’ 56\n",
      "s 42\n",
      "chinchilla 1\n",
      ", 151\n",
      "one 11\n",
      "of 87\n",
      "today 12\n",
      "’ 56\n",
      "s 42\n",
      "leading 1\n",
      "llms 21\n",
      ", 151\n",
      "was 3\n",
      "trained 4\n",
      "on 33\n",
      "1.4 1\n",
      "trillion 8\n",
      "tokens 3\n",
      ". 126\n",
      "in 61\n",
      "other 3\n",
      "words 2\n",
      ", 151\n",
      "we 12\n",
      "may 7\n",
      "be 16\n",
      "well 3\n",
      "within 4\n",
      "one 11\n",
      "order 7\n",
      "of 87\n",
      "magnitude 1\n",
      "of 87\n",
      "exhausting 1\n",
      "the 137\n",
      "world 12\n",
      "’ 56\n",
      "s 42\n",
      "entire 1\n",
      "supply 1\n",
      "of 87\n",
      "useful 1\n",
      "language 21\n",
      "training 8\n",
      "data 12\n",
      ". 126\n",
      "if 10\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      "are 27\n",
      "able 5\n",
      "to 84\n",
      "generate 5\n",
      "their 15\n",
      "own 7\n",
      "training 8\n",
      "data 12\n",
      "and 54\n",
      "use 6\n",
      "it 40\n",
      "to 84\n",
      "continue 1\n",
      "self-improving 1\n",
      ", 151\n",
      "this 34\n",
      "could 2\n",
      "render 1\n",
      "irrelevant 1\n",
      "the 137\n",
      "looming 1\n",
      "data 12\n",
      "shortage 1\n",
      ". 126\n",
      "it 40\n",
      "would 4\n",
      "represent 1\n",
      "a 51\n",
      "mind-bending 1\n",
      "leap 1\n",
      "forward 2\n",
      "for 29\n",
      "llms 21\n",
      ". 126\n",
      "a 51\n",
      "popular 2\n",
      "narrative 2\n",
      "these 13\n",
      "days 3\n",
      "is 46\n",
      "that 49\n",
      "chatgpt 11\n",
      "and 54\n",
      "conversational 3\n",
      "llms 21\n",
      "like 13\n",
      "it 40\n",
      "are 27\n",
      "on 33\n",
      "the 137\n",
      "verge 1\n",
      "of 87\n",
      "replacing 1\n",
      "google 9\n",
      "search 6\n",
      "as 23\n",
      "the 137\n",
      "world 12\n",
      "’ 56\n",
      "s 42\n",
      "go-to 1\n",
      "source 5\n",
      "for 29\n",
      "information 20\n",
      ", 151\n",
      "disrupting 1\n",
      "the 137\n",
      "once-mighty 1\n",
      "tech 1\n",
      "giant 1\n",
      "like 13\n",
      "blockbuster 1\n",
      "or 8\n",
      "kodak 1\n",
      "were 4\n",
      "disrupted 1\n",
      "before 4\n",
      "it 40\n",
      ". 126\n",
      "this 34\n",
      "narrative 2\n",
      "badly 1\n",
      "oversimplifies 1\n",
      "things 3\n",
      ". 126\n",
      "llms 21\n",
      "as 23\n",
      "they 17\n",
      "exist 3\n",
      "today 12\n",
      "will 11\n",
      "never 3\n",
      "replace 1\n",
      "google 9\n",
      "search 6\n",
      ". 126\n",
      "why 4\n",
      "not 12\n",
      "? 8\n",
      "in 61\n",
      "short 1\n",
      ", 151\n",
      "because 3\n",
      "today 12\n",
      "’ 56\n",
      "s 42\n",
      "llms 21\n",
      "make 4\n",
      "stuff 1\n",
      "up 4\n",
      ". 126\n",
      "as 23\n",
      "powerful 2\n",
      "as 23\n",
      "they 17\n",
      "are 27\n",
      ", 151\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      "regularly 1\n",
      "produce 5\n",
      "inaccurate 2\n",
      ", 151\n",
      "misleading 2\n",
      "or 8\n",
      "false 1\n",
      "information 20\n",
      "( 20\n",
      "and 54\n",
      "present 1\n",
      "it 40\n",
      "confidently 2\n",
      "and 54\n",
      "convincingly 1\n",
      ") 20\n",
      ". 126\n",
      "examples 2\n",
      "abound 1\n",
      "of 87\n",
      "chatgpt 11\n",
      "’ 56\n",
      "s 42\n",
      "“ 15\n",
      "hallucinations 1\n",
      "” 15\n",
      "( 20\n",
      "as 23\n",
      "these 13\n",
      "misstatements 1\n",
      "are 27\n",
      "referred 1\n",
      "to 84\n",
      ") 20\n",
      ". 126\n",
      "this 34\n",
      "is 46\n",
      "not 12\n",
      "to 84\n",
      "single 2\n",
      "out 8\n",
      "chatgpt 11\n",
      "; 7\n",
      "every 6\n",
      "generative 5\n",
      "language 21\n",
      "model 31\n",
      "in 61\n",
      "existence 1\n",
      "today 12\n",
      "hallucinates 1\n",
      "in 61\n",
      "similar 3\n",
      "ways 2\n",
      ". 126\n",
      "to 84\n",
      "give 2\n",
      "a 51\n",
      "few 2\n",
      "examples 2\n",
      ": 13\n",
      "it 40\n",
      "recommends 1\n",
      "books 3\n",
      "that 49\n",
      "don 3\n",
      "’ 56\n",
      "t 8\n",
      "exist 3\n",
      "; 7\n",
      "it 40\n",
      "insists 1\n",
      "that 49\n",
      "the 137\n",
      "number 2\n",
      "220 1\n",
      "is 46\n",
      "less 4\n",
      "than 11\n",
      "200 1\n",
      "; 7\n",
      "it 40\n",
      "is 46\n",
      "unsure 1\n",
      "whether 4\n",
      "abraham 1\n",
      "lincoln 2\n",
      "’ 56\n",
      "s 42\n",
      "assassin 1\n",
      "was 3\n",
      "on 33\n",
      "the 137\n",
      "same 5\n",
      "continent 1\n",
      "as 23\n",
      "lincoln 2\n",
      "at 12\n",
      "the 137\n",
      "time 7\n",
      "of 87\n",
      "the 137\n",
      "assassination 1\n",
      "; 7\n",
      "it 40\n",
      "provides 4\n",
      "plausible-sounding 1\n",
      "but 9\n",
      "incorrect 1\n",
      "explanations 2\n",
      "of 87\n",
      "concepts 1\n",
      "like 13\n",
      "bayes 1\n",
      "’ 56\n",
      "theorem 1\n",
      ". 126\n",
      "most 8\n",
      "users 3\n",
      "will 11\n",
      "not 12\n",
      "accept 1\n",
      "a 51\n",
      "search 6\n",
      "engine 1\n",
      "that 49\n",
      "gets 1\n",
      "basic 3\n",
      "facts 1\n",
      "like 13\n",
      "these 13\n",
      "wrong 1\n",
      "some 2\n",
      "of 87\n",
      "the 137\n",
      "time 7\n",
      "; 7\n",
      "even 3\n",
      "99 1\n",
      "% 9\n",
      "accuracy 2\n",
      "will 11\n",
      "not 12\n",
      "be 16\n",
      "good 3\n",
      "enough 2\n",
      "for 29\n",
      "broad 1\n",
      "market 1\n",
      "adoption 1\n",
      ". 126\n",
      "openai 6\n",
      "ceo 3\n",
      "sam 2\n",
      "altman 2\n",
      "himself 1\n",
      "acknowledges 1\n",
      "this 34\n",
      ", 151\n",
      "recently 4\n",
      "cautioning 1\n",
      ": 13\n",
      "“ 15\n",
      "chatgpt 11\n",
      "is 46\n",
      "incredibly 1\n",
      "limited 2\n",
      ", 151\n",
      "but 9\n",
      "good 3\n",
      "enough 2\n",
      "at 12\n",
      "some 2\n",
      "things 3\n",
      "to 84\n",
      "create 1\n",
      "a 51\n",
      "misleading 2\n",
      "impression 1\n",
      "of 87\n",
      "greatness 1\n",
      ". 126\n",
      "it 40\n",
      "'s 2\n",
      "a 51\n",
      "mistake 1\n",
      "to 84\n",
      "be 16\n",
      "relying 1\n",
      "on 33\n",
      "it 40\n",
      "for 29\n",
      "anything 1\n",
      "important 8\n",
      "right 3\n",
      "now. 1\n",
      "” 15\n",
      "it 40\n",
      "is 46\n",
      "an 14\n",
      "open 1\n",
      "question 3\n",
      "whether 4\n",
      "llms 21\n",
      "’ 56\n",
      "hallucination 1\n",
      "problem 5\n",
      "can 24\n",
      "be 16\n",
      "solved 2\n",
      "via 2\n",
      "incremental 1\n",
      "improvements 1\n",
      "to 84\n",
      "existing 1\n",
      "architectures 2\n",
      ", 151\n",
      "or 8\n",
      "whether 4\n",
      "a 51\n",
      "more 11\n",
      "fundamental 1\n",
      "paradigm 1\n",
      "shift 1\n",
      "in 61\n",
      "ai 18\n",
      "methodologies 1\n",
      "will 11\n",
      "be 16\n",
      "necessary 1\n",
      "to 84\n",
      "give 2\n",
      "ai 18\n",
      "common 1\n",
      "sense 2\n",
      "and 54\n",
      "real 3\n",
      "understanding 2\n",
      ". 126\n",
      "deep 1\n",
      "learning 1\n",
      "pioneer 1\n",
      "yann 3\n",
      "lecun 5\n",
      ", 151\n",
      "for 29\n",
      "one 11\n",
      ", 151\n",
      "believes 1\n",
      "the 137\n",
      "latter 1\n",
      ". 126\n",
      "lecun 5\n",
      "’ 56\n",
      "s 42\n",
      "contrarian 1\n",
      "perspective 3\n",
      "may 7\n",
      "prove 2\n",
      "correct 2\n",
      "; 7\n",
      "time 7\n",
      "will 11\n",
      "tell 1\n",
      ". 126\n",
      "in 61\n",
      "the 137\n",
      "nearer 1\n",
      "term 1\n",
      ", 151\n",
      "though 2\n",
      ", 151\n",
      "a 51\n",
      "set 2\n",
      "of 87\n",
      "promising 3\n",
      "innovations 1\n",
      "offers 1\n",
      "to 84\n",
      "at 12\n",
      "least 2\n",
      "mitigate 1\n",
      "llms 21\n",
      "’ 56\n",
      "factual 1\n",
      "unreliability 2\n",
      ". 126\n",
      "these 13\n",
      "new 10\n",
      "methods 1\n",
      "will 11\n",
      "play 1\n",
      "an 14\n",
      "essential 1\n",
      "role 1\n",
      "in 61\n",
      "preparing 1\n",
      "llms 21\n",
      "for 29\n",
      "widespread 2\n",
      "real-world 2\n",
      "deployment 1\n",
      ". 126\n",
      "two 2\n",
      "related 2\n",
      "capabilities 2\n",
      "lie 1\n",
      "at 12\n",
      "the 137\n",
      "heart 1\n",
      "of 87\n",
      "current 2\n",
      "efforts 1\n",
      "to 84\n",
      "make 4\n",
      "language 21\n",
      "models 44\n",
      "more 11\n",
      "accurate 6\n",
      ": 13\n",
      "( 20\n",
      "1 2\n",
      ") 20\n",
      "the 137\n",
      "ability 4\n",
      "for 29\n",
      "llms 21\n",
      "to 84\n",
      "retrieve 3\n",
      "information 20\n",
      "from 26\n",
      "external 6\n",
      "sources 5\n",
      ", 151\n",
      "and 54\n",
      "( 20\n",
      "2 1\n",
      ") 20\n",
      "the 137\n",
      "ability 4\n",
      "for 29\n",
      "llms 21\n",
      "to 84\n",
      "provide 5\n",
      "references 3\n",
      "and 54\n",
      "citations 5\n",
      "for 29\n",
      "the 137\n",
      "information 20\n",
      "they 17\n",
      "provide 5\n",
      ". 126\n",
      "chatgpt 11\n",
      "is 46\n",
      "limited 2\n",
      "to 84\n",
      "the 137\n",
      "information 20\n",
      "that 49\n",
      "is 46\n",
      "already 3\n",
      "stored 1\n",
      "inside 1\n",
      "of 87\n",
      "it 40\n",
      ", 151\n",
      "captured 2\n",
      "in 61\n",
      "its 13\n",
      "static 1\n",
      "weights 1\n",
      ". 126\n",
      "( 20\n",
      "this 34\n",
      "is 46\n",
      "why 4\n",
      "it 40\n",
      "is 46\n",
      "not 12\n",
      "able 5\n",
      "to 84\n",
      "discuss 1\n",
      "events 1\n",
      "that 49\n",
      "occurred 1\n",
      "after 2\n",
      "2021 2\n",
      ", 151\n",
      "when 7\n",
      "the 137\n",
      "model 31\n",
      "was 3\n",
      "trained 4\n",
      ". 126\n",
      ") 20\n",
      "being 1\n",
      "able 5\n",
      "to 84\n",
      "pull 1\n",
      "in 61\n",
      "information 20\n",
      "from 26\n",
      "external 6\n",
      "sources 5\n",
      "will 11\n",
      "empower 1\n",
      "llms 21\n",
      "to 84\n",
      "access 3\n",
      "the 137\n",
      "most 8\n",
      "accurate 6\n",
      "and 54\n",
      "up-to-date 1\n",
      "information 20\n",
      "available 3\n",
      ", 151\n",
      "even 3\n",
      "when 7\n",
      "that 49\n",
      "information 20\n",
      "changes 1\n",
      "frequently 1\n",
      "( 20\n",
      "say 1\n",
      ", 151\n",
      "companies 1\n",
      "’ 56\n",
      "stock 2\n",
      "prices 1\n",
      ") 20\n",
      ". 126\n",
      "of 87\n",
      "course 2\n",
      ", 151\n",
      "having 1\n",
      "access 3\n",
      "to 84\n",
      "an 14\n",
      "external 6\n",
      "information 20\n",
      "source 5\n",
      "does 2\n",
      "not 12\n",
      "by 10\n",
      "itself 5\n",
      "guarantee 1\n",
      "that 49\n",
      "llms 21\n",
      "will 11\n",
      "retrieve 3\n",
      "the 137\n",
      "most 8\n",
      "accurate 6\n",
      "and 54\n",
      "relevant 4\n",
      "information 20\n",
      ". 126\n",
      "one 11\n",
      "important 8\n",
      "way 3\n",
      "for 29\n",
      "llms 21\n",
      "to 84\n",
      "increase 2\n",
      "transparency 1\n",
      "and 54\n",
      "trust 1\n",
      "with 8\n",
      "human 9\n",
      "users 3\n",
      "is 46\n",
      "to 84\n",
      "include 1\n",
      "references 3\n",
      "to 84\n",
      "the 137\n",
      "source 5\n",
      "( 20\n",
      "s 42\n",
      ") 20\n",
      "from 26\n",
      "which 3\n",
      "they 17\n",
      "retrieved 1\n",
      "the 137\n",
      "information 20\n",
      ". 126\n",
      "such 1\n",
      "citations 5\n",
      "allow 2\n",
      "human 9\n",
      "users 3\n",
      "to 84\n",
      "audit 1\n",
      "the 137\n",
      "information 20\n",
      "source 5\n",
      "as 23\n",
      "needed 1\n",
      "in 61\n",
      "order 7\n",
      "to 84\n",
      "decide 1\n",
      "for 29\n",
      "themselves 3\n",
      "on 33\n",
      "its 13\n",
      "reliability 1\n",
      ". 126\n",
      "important 8\n",
      "early 1\n",
      "work 7\n",
      "in 61\n",
      "this 34\n",
      "field 2\n",
      "includes 3\n",
      "models 44\n",
      "like 13\n",
      "realm 1\n",
      "( 20\n",
      "from 26\n",
      "google 9\n",
      ") 20\n",
      "and 54\n",
      "rag 1\n",
      "( 20\n",
      "from 26\n",
      "facebook 1\n",
      ") 20\n",
      ", 151\n",
      "both 3\n",
      "published 3\n",
      "in 61\n",
      "2020. 1\n",
      "with 8\n",
      "the 137\n",
      "rise 1\n",
      "of 87\n",
      "conversational 3\n",
      "llms 21\n",
      "in 61\n",
      "recent 6\n",
      "months 3\n",
      ", 151\n",
      "research 10\n",
      "in 61\n",
      "this 34\n",
      "area 2\n",
      "is 46\n",
      "now 2\n",
      "rapidly 2\n",
      "accelerating 1\n",
      ". 126\n",
      "last 3\n",
      "year 3\n",
      ", 151\n",
      "openai 6\n",
      "published 3\n",
      "a 51\n",
      "fine-tuned 3\n",
      "version 1\n",
      "of 87\n",
      "its 13\n",
      "gpt 1\n",
      "model 31\n",
      "named 2\n",
      "webgpt 4\n",
      "that 49\n",
      "can 24\n",
      "browse 1\n",
      "the 137\n",
      "internet 5\n",
      "using 1\n",
      "microsoft 1\n",
      "bing 2\n",
      "in 61\n",
      "order 7\n",
      "to 84\n",
      "provide 5\n",
      "more 11\n",
      "accurate 6\n",
      "and 54\n",
      "in-depth 1\n",
      "responses 5\n",
      "to 84\n",
      "prompts 1\n",
      ". 126\n",
      "webgpt 4\n",
      "navigates 1\n",
      "the 137\n",
      "internet 5\n",
      "much 6\n",
      "like 13\n",
      "a 51\n",
      "human 9\n",
      "does 2\n",
      ": 13\n",
      "it 40\n",
      "can 24\n",
      "submit 2\n",
      "search 6\n",
      "queries 1\n",
      "to 84\n",
      "bing 2\n",
      ", 151\n",
      "follow 1\n",
      "links 2\n",
      ", 151\n",
      "scroll 1\n",
      "up 4\n",
      "and 54\n",
      "down 1\n",
      "on 33\n",
      "webpages 2\n",
      ", 151\n",
      "and 54\n",
      "use 6\n",
      "functions 1\n",
      "like 13\n",
      "ctrl+f 1\n",
      "to 84\n",
      "find 3\n",
      "terms 2\n",
      ". 126\n",
      "when 7\n",
      "the 137\n",
      "model 31\n",
      "finds 1\n",
      "relevant 4\n",
      "information 20\n",
      "on 33\n",
      "the 137\n",
      "internet 5\n",
      "that 49\n",
      "it 40\n",
      "incorporates 1\n",
      "into 1\n",
      "its 13\n",
      "output 3\n",
      ", 151\n",
      "it 40\n",
      "provides 4\n",
      "citations 5\n",
      "so 2\n",
      "that 49\n",
      "the 137\n",
      "human 9\n",
      "user 1\n",
      "can 24\n",
      "see 3\n",
      "where 2\n",
      "the 137\n",
      "information 20\n",
      "came 3\n",
      "from 26\n",
      ". 126\n",
      "the 137\n",
      "results 2\n",
      "are 27\n",
      "encouraging 1\n",
      ": 13\n",
      "for 29\n",
      "the 137\n",
      "same 5\n",
      "query 2\n",
      ", 151\n",
      "webgpt 4\n",
      "’ 56\n",
      "s 42\n",
      "responses 5\n",
      "are 27\n",
      "preferred 2\n",
      "to 84\n",
      "responses 5\n",
      "written 3\n",
      "by 10\n",
      "human 9\n",
      "subjects 1\n",
      "56 1\n",
      "% 9\n",
      "of 87\n",
      "the 137\n",
      "time 7\n",
      "and 54\n",
      "are 27\n",
      "preferred 2\n",
      "to 84\n",
      "the 137\n",
      "highest-rated 1\n",
      "responses 5\n",
      "on 33\n",
      "reddit 1\n",
      "69 1\n",
      "% 9\n",
      "of 87\n",
      "the 137\n",
      "time 7\n",
      ". 126\n",
      "deepmind 6\n",
      "is 46\n",
      "also 5\n",
      "pursuing 1\n",
      "research 10\n",
      "along 2\n",
      "these 13\n",
      "lines 2\n",
      ". 126\n",
      "a 51\n",
      "few 2\n",
      "months 3\n",
      "ago 1\n",
      ", 151\n",
      "deepmind 6\n",
      "published 3\n",
      "a 51\n",
      "new 10\n",
      "model 31\n",
      "named 2\n",
      "sparrow 6\n",
      ". 126\n",
      "like 13\n",
      "chatgpt 11\n",
      ", 151\n",
      "sparrow 6\n",
      "is 46\n",
      "dialogue-based 1\n",
      "; 7\n",
      "like 13\n",
      "webgpt 4\n",
      ", 151\n",
      "it 40\n",
      "can 24\n",
      "search 6\n",
      "the 137\n",
      "internet 5\n",
      "for 29\n",
      "information 20\n",
      "and 54\n",
      "provide 5\n",
      "citations 5\n",
      "for 29\n",
      "its 13\n",
      "assertions 1\n",
      ". 126\n",
      "sparrow 6\n",
      "builds 2\n",
      "on 33\n",
      "important 8\n",
      "earlier 1\n",
      "work 7\n",
      "out 8\n",
      "of 87\n",
      "deepmind 6\n",
      "including 2\n",
      "spalm 1\n",
      ", 151\n",
      "retro 1\n",
      "and 54\n",
      "gophercite 1\n",
      ". 126\n",
      "deepmind 6\n",
      "'s 2\n",
      "sparrow 6\n",
      "model 31\n",
      "in 61\n",
      "action 2\n",
      ". 126\n",
      "as 23\n",
      "shown 1\n",
      "here 1\n",
      ", 151\n",
      "sparrow 6\n",
      "provides 4\n",
      "quotations 1\n",
      "and 54\n",
      "links 2\n",
      "to 84\n",
      "support 1\n",
      "... 2\n",
      "[ 2\n",
      "+ 2\n",
      "] 2\n",
      "its 13\n",
      "statements 1\n",
      ", 151\n",
      "increasing 1\n",
      "their 15\n",
      "accuracy 2\n",
      "and 54\n",
      "trustworthiness 1\n",
      ". 126\n",
      "the 137\n",
      "deepmind 6\n",
      "researchers 5\n",
      "find 3\n",
      "that 49\n",
      "sparrow 6\n",
      "’ 56\n",
      "s 42\n",
      "citations 5\n",
      "are 27\n",
      "helpful 2\n",
      "and 54\n",
      "accurate 6\n",
      "78 1\n",
      "% 9\n",
      "of 87\n",
      "the 137\n",
      "time—suggesting 1\n",
      "both 3\n",
      "that 49\n",
      "this 34\n",
      "research 10\n",
      "approach 3\n",
      "is 46\n",
      "promising 3\n",
      "and 54\n",
      "that 49\n",
      "the 137\n",
      "problem 5\n",
      "of 87\n",
      "llm 6\n",
      "inaccuracy 1\n",
      "is 46\n",
      "far 2\n",
      "from 26\n",
      "solved 2\n",
      ". 126\n",
      "younger 1\n",
      "startups 2\n",
      "including 2\n",
      "you.com 2\n",
      "and 54\n",
      "perplexity 1\n",
      "have 5\n",
      "also 5\n",
      "recently 4\n",
      "launched 1\n",
      "llm-powered 1\n",
      "conversational 3\n",
      "search 6\n",
      "interfaces 1\n",
      "with 8\n",
      "the 137\n",
      "ability 4\n",
      "to 84\n",
      "retrieve 3\n",
      "information 20\n",
      "from 26\n",
      "external 6\n",
      "sources 5\n",
      "and 54\n",
      "cite 1\n",
      "references 3\n",
      ". 126\n",
      "these 13\n",
      "products 2\n",
      "are 27\n",
      "available 3\n",
      "for 29\n",
      "public 3\n",
      "use 6\n",
      "today 12\n",
      ". 126\n",
      "llms 21\n",
      "’ 56\n",
      "greatest 2\n",
      "shortcoming 1\n",
      "is 46\n",
      "their 15\n",
      "unreliability 2\n",
      ", 151\n",
      "their 15\n",
      "stubborn 1\n",
      "tendency 1\n",
      "to 84\n",
      "confidently 2\n",
      "provide 5\n",
      "inaccurate 2\n",
      "information 20\n",
      ". 126\n",
      "language 21\n",
      "models 44\n",
      "promise 1\n",
      "to 84\n",
      "reshape 1\n",
      "every 6\n",
      "sector 1\n",
      "of 87\n",
      "our 6\n",
      "economy 1\n",
      ", 151\n",
      "but 9\n",
      "they 17\n",
      "will 11\n",
      "never 3\n",
      "reach 1\n",
      "their 15\n",
      "full 1\n",
      "potential 3\n",
      "until 1\n",
      "this 34\n",
      "problem 5\n",
      "is 46\n",
      "addressed 1\n",
      ". 126\n",
      "expect 1\n",
      "to 84\n",
      "see 3\n",
      "plenty 2\n",
      "of 87\n",
      "activity 1\n",
      "and 54\n",
      "innovation 2\n",
      "in 61\n",
      "this 34\n",
      "area 2\n",
      "in 61\n",
      "the 137\n",
      "months 3\n",
      "ahead 2\n",
      ". 126\n",
      "today 12\n",
      "’ 56\n",
      "s 42\n",
      "most 8\n",
      "prominent 1\n",
      "large 10\n",
      "language 21\n",
      "models 44\n",
      "all 15\n",
      "have 5\n",
      "effectively 2\n",
      "the 137\n",
      "same 5\n",
      "architecture 3\n",
      ". 126\n",
      "meta 7\n",
      "ai 18\n",
      "chief 2\n",
      "yann 3\n",
      "lecun 5\n",
      "said 2\n",
      "recently 4\n",
      ": 13\n",
      "“ 15\n",
      "in 61\n",
      "terms 2\n",
      "of 87\n",
      "underlying 1\n",
      "techniques 1\n",
      ", 151\n",
      "chatgpt 11\n",
      "is 46\n",
      "not 12\n",
      "particularly 3\n",
      "innovative 1\n",
      ". 126\n",
      "it 40\n",
      "’ 56\n",
      "s 42\n",
      "nothing 1\n",
      "revolutionary 1\n",
      ", 151\n",
      "although 1\n",
      "that 49\n",
      "’ 56\n",
      "s 42\n",
      "the 137\n",
      "way 3\n",
      "it 40\n",
      "’ 56\n",
      "s 42\n",
      "perceived 1\n",
      "in 61\n",
      "the 137\n",
      "public 3\n",
      ". 126\n",
      "it 40\n",
      "’ 56\n",
      "s 42\n",
      "just 2\n",
      "that 49\n",
      ", 151\n",
      "you 5\n",
      "know 1\n",
      ", 151\n",
      "it 40\n",
      "’ 56\n",
      "s 42\n",
      "well 3\n",
      "put 1\n",
      "together 1\n",
      ", 151\n",
      "it 40\n",
      "’ 56\n",
      "s 42\n",
      "nicely 1\n",
      "done. 1\n",
      "” 15\n",
      "lecun 5\n",
      "’ 56\n",
      "s 42\n",
      "statement 1\n",
      "stirred 1\n",
      "up 4\n",
      "plenty 2\n",
      "of 87\n",
      "controversy 1\n",
      "and 54\n",
      "twitter 1\n",
      "debate 1\n",
      ". 126\n",
      "but 9\n",
      "the 137\n",
      "simple 1\n",
      "fact 3\n",
      "is 46\n",
      "that 49\n",
      "he 1\n",
      "is 46\n",
      "correct 2\n",
      ", 151\n",
      "as 23\n",
      "no 2\n",
      "serious 1\n",
      "ai 18\n",
      "researcher 1\n",
      "would 4\n",
      "dispute 1\n",
      ". 126\n",
      "all 15\n",
      "of 87\n",
      "today 12\n",
      "’ 56\n",
      "s 42\n",
      "well-known 1\n",
      "language 21\n",
      "models—e.g. 1\n",
      ", 151\n",
      "gpt-3 6\n",
      "from 26\n",
      "openai 6\n",
      ", 151\n",
      "palm 1\n",
      "or 8\n",
      "lamda 1\n",
      "from 26\n",
      "google 9\n",
      ", 151\n",
      "galactica 1\n",
      "or 8\n",
      "opt 1\n",
      "from 26\n",
      "meta 7\n",
      ", 151\n",
      "megatron-turing 1\n",
      "from 26\n",
      "nvidia/microsoft 1\n",
      ", 151\n",
      "jurassic-1 1\n",
      "from 26\n",
      "ai21 1\n",
      "labs—are 1\n",
      "built 3\n",
      "in 61\n",
      "the 137\n",
      "same 5\n",
      "basic 3\n",
      "way 3\n",
      ". 126\n",
      "they 17\n",
      "are 27\n",
      "autoregressive 1\n",
      ", 151\n",
      "self-supervised 1\n",
      ", 151\n",
      "pre-trained 1\n",
      ", 151\n",
      "densely 1\n",
      "activated 4\n",
      "transformer-based 1\n",
      "models 44\n",
      ". 126\n",
      "to 84\n",
      "be 16\n",
      "sure 1\n",
      ", 151\n",
      "variations 2\n",
      "among 1\n",
      "these 13\n",
      "models 44\n",
      "exist 3\n",
      ": 13\n",
      "their 15\n",
      "size 3\n",
      "( 20\n",
      "parameter 2\n",
      "count 2\n",
      ") 20\n",
      ", 151\n",
      "the 137\n",
      "data 12\n",
      "they 17\n",
      "are 27\n",
      "trained 4\n",
      "on 33\n",
      ", 151\n",
      "the 137\n",
      "optimization 1\n",
      "algorithm 1\n",
      "used 3\n",
      ", 151\n",
      "the 137\n",
      "batch 1\n",
      "size 3\n",
      ", 151\n",
      "the 137\n",
      "number 2\n",
      "of 87\n",
      "hidden 1\n",
      "layers 1\n",
      ", 151\n",
      "whether 4\n",
      "they 17\n",
      "are 27\n",
      "instruction 3\n",
      "fine-tuned 3\n",
      ", 151\n",
      "and 54\n",
      "so 2\n",
      "on 33\n",
      ". 126\n",
      "these 13\n",
      "variations 2\n",
      "can 24\n",
      "translate 1\n",
      "to 84\n",
      "meaningful 1\n",
      "performance 8\n",
      "differences 1\n",
      ". 126\n",
      "the 137\n",
      "core 2\n",
      "architectures 2\n",
      ", 151\n",
      "though 2\n",
      ", 151\n",
      "vary 1\n",
      "little 1\n",
      ". 126\n",
      "yet 3\n",
      "momentum 1\n",
      "is 46\n",
      "building 2\n",
      "behind 2\n",
      "an 14\n",
      "intriguingly 1\n",
      "different 3\n",
      "architectural 1\n",
      "approach 3\n",
      "to 84\n",
      "language 21\n",
      "models 44\n",
      "known 1\n",
      "as 23\n",
      "sparse 21\n",
      "expert 10\n",
      "models 44\n",
      ". 126\n",
      "while 3\n",
      "the 137\n",
      "idea 2\n",
      "has 6\n",
      "been 3\n",
      "around 1\n",
      "for 29\n",
      "decades 1\n",
      ", 151\n",
      "it 40\n",
      "has 6\n",
      "only 5\n",
      "recently 4\n",
      "reemerged 1\n",
      "and 54\n",
      "begun 1\n",
      "to 84\n",
      "gain 1\n",
      "in 61\n",
      "popularity 1\n",
      ". 126\n",
      "all 15\n",
      "of 87\n",
      "the 137\n",
      "models 44\n",
      "mentioned 1\n",
      "above 1\n",
      "are 27\n",
      "dense 8\n",
      ". 126\n",
      "this 34\n",
      "means 1\n",
      "that 49\n",
      "every 6\n",
      "time 7\n",
      "the 137\n",
      "model 31\n",
      "runs 1\n",
      ", 151\n",
      "every 6\n",
      "single 2\n",
      "one 11\n",
      "of 87\n",
      "its 13\n",
      "parameters 10\n",
      "is 46\n",
      "used 3\n",
      ". 126\n",
      "every 6\n",
      "time 7\n",
      "you 5\n",
      "submit 2\n",
      "a 51\n",
      "prompt 3\n",
      "to 84\n",
      "gpt-3 6\n",
      ", 151\n",
      "for 29\n",
      "instance 4\n",
      ", 151\n",
      "all 15\n",
      "175 1\n",
      "billion 1\n",
      "of 87\n",
      "the 137\n",
      "model 31\n",
      "’ 56\n",
      "s 42\n",
      "parameters 10\n",
      "are 27\n",
      "activated 4\n",
      "in 61\n",
      "order 7\n",
      "to 84\n",
      "produce 5\n",
      "its 13\n",
      "response 1\n",
      ". 126\n",
      "but 9\n",
      "what 5\n",
      "if 10\n",
      "a 51\n",
      "model 31\n",
      "were 4\n",
      "able 5\n",
      "to 84\n",
      "call 1\n",
      "upon 1\n",
      "only 5\n",
      "the 137\n",
      "most 8\n",
      "relevant 4\n",
      "subset 2\n",
      "of 87\n",
      "its 13\n",
      "parameters 10\n",
      "in 61\n",
      "order 7\n",
      "to 84\n",
      "respond 2\n",
      "to 84\n",
      "a 51\n",
      "given 2\n",
      "query 2\n",
      "? 8\n",
      "this 34\n",
      "is 46\n",
      "the 137\n",
      "basic 3\n",
      "concept 1\n",
      "behind 2\n",
      "sparse 21\n",
      "expert 10\n",
      "models 44\n",
      ". 126\n",
      "the 137\n",
      "defining 1\n",
      "characteristic 1\n",
      "of 87\n",
      "sparse 21\n",
      "models 44\n",
      "is 46\n",
      "that 49\n",
      "they 17\n",
      "don 3\n",
      "’ 56\n",
      "t 8\n",
      "activate 2\n",
      "all 15\n",
      "of 87\n",
      "their 15\n",
      "parameters 10\n",
      "for 29\n",
      "a 51\n",
      "given 2\n",
      "input 3\n",
      ", 151\n",
      "but 9\n",
      "rather 2\n",
      "only 5\n",
      "those 6\n",
      "parameters 10\n",
      "that 49\n",
      "are 27\n",
      "helpful 2\n",
      "in 61\n",
      "order 7\n",
      "to 84\n",
      "handle 1\n",
      "the 137\n",
      "input 3\n",
      ". 126\n",
      "model 31\n",
      "sparsity 1\n",
      "thus 1\n",
      "decouples 1\n",
      "a 51\n",
      "model 31\n",
      "’ 56\n",
      "s 42\n",
      "total 3\n",
      "parameter 2\n",
      "count 2\n",
      "from 26\n",
      "its 13\n",
      "compute 4\n",
      "requirements 1\n",
      ". 126\n",
      "this 34\n",
      "leads 2\n",
      "to 84\n",
      "sparse 21\n",
      "expert 10\n",
      "models 44\n",
      "’ 56\n",
      "key 1\n",
      "advantage 2\n",
      ": 13\n",
      "they 17\n",
      "can 24\n",
      "be 16\n",
      "both 3\n",
      "larger 6\n",
      "and 54\n",
      "less 4\n",
      "computationally 1\n",
      "demanding 1\n",
      "than 11\n",
      "dense 8\n",
      "models 44\n",
      ". 126\n",
      "why 4\n",
      "are 27\n",
      "they 17\n",
      "called 2\n",
      "sparse 21\n",
      "expert 10\n",
      "models 44\n",
      "? 8\n",
      "because 3\n",
      "sparse 21\n",
      "models 44\n",
      "can 24\n",
      "be 16\n",
      "thought 1\n",
      "of 87\n",
      "as 23\n",
      "consisting 1\n",
      "of 87\n",
      "a 51\n",
      "collection 1\n",
      "of 87\n",
      "“ 15\n",
      "sub-models 1\n",
      "” 15\n",
      "that 49\n",
      "serve 1\n",
      "as 23\n",
      "experts 6\n",
      "on 33\n",
      "different 3\n",
      "topics 1\n",
      ". 126\n",
      "depending 1\n",
      "on 33\n",
      "the 137\n",
      "prompt 3\n",
      "presented 2\n",
      "to 84\n",
      "the 137\n",
      "model 31\n",
      ", 151\n",
      "the 137\n",
      "most 8\n",
      "relevant 4\n",
      "experts 6\n",
      "within 4\n",
      "the 137\n",
      "model 31\n",
      "are 27\n",
      "activated 4\n",
      "while 3\n",
      "the 137\n",
      "other 3\n",
      "experts 6\n",
      "remain 2\n",
      "inactive 1\n",
      ". 126\n",
      "a 51\n",
      "prompt 3\n",
      "posed 1\n",
      "in 61\n",
      "russian 2\n",
      ", 151\n",
      "for 29\n",
      "instance 4\n",
      ", 151\n",
      "would 4\n",
      "only 5\n",
      "activate 2\n",
      "the 137\n",
      "“ 15\n",
      "experts 6\n",
      "” 15\n",
      "within 4\n",
      "a 51\n",
      "model 31\n",
      "that 49\n",
      "can 24\n",
      "understand 2\n",
      "and 54\n",
      "respond 2\n",
      "in 61\n",
      "russian 2\n",
      ", 151\n",
      "efficiently 1\n",
      "bypassing 1\n",
      "the 137\n",
      "rest 2\n",
      "of 87\n",
      "the 137\n",
      "model 31\n",
      ". 126\n",
      "all 15\n",
      "of 87\n",
      "today 12\n",
      "’ 56\n",
      "s 42\n",
      "largest 1\n",
      "llms 21\n",
      "are 27\n",
      "sparse 21\n",
      ". 126\n",
      "if 10\n",
      "you 5\n",
      "come 3\n",
      "across 1\n",
      "an 14\n",
      "llm 6\n",
      "with 8\n",
      "more 11\n",
      "than 11\n",
      "1 2\n",
      "trillion 8\n",
      "parameters 10\n",
      ", 151\n",
      "you 5\n",
      "can 24\n",
      "safely 1\n",
      "assume 1\n",
      "that 49\n",
      "it 40\n",
      "is 46\n",
      "sparse 21\n",
      ". 126\n",
      "this 34\n",
      "includes 3\n",
      "google 9\n",
      "’ 56\n",
      "s 42\n",
      "switch 1\n",
      "transformer 1\n",
      "( 20\n",
      "1.6 1\n",
      "trillion 8\n",
      "parameters 10\n",
      ") 20\n",
      ", 151\n",
      "google 9\n",
      "’ 56\n",
      "s 42\n",
      "glam 2\n",
      "( 20\n",
      "1.2 1\n",
      "trillion 8\n",
      "parameters 10\n",
      ") 20\n",
      "and 54\n",
      "meta 7\n",
      "’ 56\n",
      "s 42\n",
      "mixture 1\n",
      "of 87\n",
      "experts 6\n",
      "model 31\n",
      "( 20\n",
      "1.1 1\n",
      "trillion 8\n",
      "parameters 10\n",
      ") 20\n",
      ". 126\n",
      "“ 15\n",
      "much 6\n",
      "of 87\n",
      "the 137\n",
      "recent 6\n",
      "progress 1\n",
      "in 61\n",
      "ai 18\n",
      "has 6\n",
      "come 3\n",
      "from 26\n",
      "training 8\n",
      "larger 6\n",
      "and 54\n",
      "larger 6\n",
      "models 44\n",
      ", 151\n",
      "” 15\n",
      "said 2\n",
      "mikel 1\n",
      "artetxe 1\n",
      ", 151\n",
      "who 3\n",
      "led 1\n",
      "meta 7\n",
      "’ 56\n",
      "s 42\n",
      "research 10\n",
      "on 33\n",
      "sparse 21\n",
      "models 44\n",
      "before 4\n",
      "resigning 1\n",
      "to 84\n",
      "cofound 1\n",
      "a 51\n",
      "stealth 1\n",
      "llm 6\n",
      "startup 1\n",
      ". 126\n",
      "“ 15\n",
      "gpt-3 6\n",
      ", 151\n",
      "for 29\n",
      "instance 4\n",
      ", 151\n",
      "is 46\n",
      "more 11\n",
      "than 11\n",
      "100 1\n",
      "times 3\n",
      "larger 6\n",
      "than 11\n",
      "gpt-2 2\n",
      ". 126\n",
      "but 9\n",
      "when 7\n",
      "we 12\n",
      "double 1\n",
      "the 137\n",
      "size 3\n",
      "of 87\n",
      "a 51\n",
      "dense 8\n",
      "model 31\n",
      ", 151\n",
      "we 12\n",
      "also 5\n",
      "make 4\n",
      "it 40\n",
      "twice 1\n",
      "as 23\n",
      "slow 1\n",
      ". 126\n",
      "sparse 21\n",
      "models 44\n",
      "allow 2\n",
      "us 3\n",
      "to 84\n",
      "train 2\n",
      "larger 6\n",
      "models 44\n",
      "without 1\n",
      "the 137\n",
      "increase 2\n",
      "in 61\n",
      "runtime. 1\n",
      "” 15\n",
      "recent 6\n",
      "research 10\n",
      "on 33\n",
      "sparse 21\n",
      "expert 10\n",
      "models 44\n",
      "suggests 2\n",
      "that 49\n",
      "this 34\n",
      "architecture 3\n",
      "holds 1\n",
      "massive 1\n",
      "potential 3\n",
      ". 126\n",
      "glam 2\n",
      ", 151\n",
      "a 51\n",
      "sparse 21\n",
      "expert 10\n",
      "model 31\n",
      "developed 1\n",
      "last 3\n",
      "year 3\n",
      "by 10\n",
      "google 9\n",
      ", 151\n",
      "is 46\n",
      "7 1\n",
      "times 3\n",
      "larger 6\n",
      "than 11\n",
      "gpt-3 6\n",
      ", 151\n",
      "requires 3\n",
      "two-thirds 1\n",
      "less 4\n",
      "energy 1\n",
      "to 84\n",
      "train 2\n",
      ", 151\n",
      "requires 3\n",
      "half 1\n",
      "as 23\n",
      "much 6\n",
      "compute 4\n",
      "for 29\n",
      "inference 1\n",
      ", 151\n",
      "and 54\n",
      "outperforms 1\n",
      "gpt-3 6\n",
      "on 33\n",
      "a 51\n",
      "wide 1\n",
      "range 1\n",
      "of 87\n",
      "natural 2\n",
      "language 21\n",
      "tasks 2\n",
      ". 126\n",
      "similar 3\n",
      "work 7\n",
      "on 33\n",
      "sparse 21\n",
      "models 44\n",
      "out 8\n",
      "of 87\n",
      "meta 7\n",
      "has 6\n",
      "yielded 1\n",
      "similarly 2\n",
      "promising 3\n",
      "results 2\n",
      ". 126\n",
      "as 23\n",
      "the 137\n",
      "meta 7\n",
      "researchers 5\n",
      "summarize 1\n",
      ": 13\n",
      "“ 15\n",
      "we 12\n",
      "find 3\n",
      "that 49\n",
      "sparse 21\n",
      "models 44\n",
      "can 24\n",
      "achieve 1\n",
      "similar 3\n",
      "downstream 1\n",
      "task 1\n",
      "performance 8\n",
      "as 23\n",
      "dense 8\n",
      "models 44\n",
      "at 12\n",
      "a 51\n",
      "fraction 1\n",
      "of 87\n",
      "the 137\n",
      "compute 4\n",
      ". 126\n",
      "for 29\n",
      "models 44\n",
      "with 8\n",
      "relatively 1\n",
      "modest 1\n",
      "compute 4\n",
      "budgets 1\n",
      ", 151\n",
      "a 51\n",
      "sparse 21\n",
      "model 31\n",
      "can 24\n",
      "perform 1\n",
      "on 33\n",
      "par 1\n",
      "with 8\n",
      "a 51\n",
      "dense 8\n",
      "model 31\n",
      "that 49\n",
      "requires 3\n",
      "almost 1\n",
      "four 1\n",
      "times 3\n",
      "as 23\n",
      "much 6\n",
      "compute. 1\n",
      "” 15\n",
      "there 2\n",
      "is 46\n",
      "another 3\n",
      "benefit 1\n",
      "of 87\n",
      "sparse 21\n",
      "expert 10\n",
      "models 44\n",
      "that 49\n",
      "is 46\n",
      "worth 1\n",
      "mentioning 1\n",
      ": 13\n",
      "they 17\n",
      "are 27\n",
      "more 11\n",
      "interpretable 1\n",
      "than 11\n",
      "dense 8\n",
      "models 44\n",
      ". 126\n",
      "interpretability—the 1\n",
      "ability 4\n",
      "for 29\n",
      "a 51\n",
      "human 9\n",
      "to 84\n",
      "understand 2\n",
      "why 4\n",
      "a 51\n",
      "model 31\n",
      "took 1\n",
      "the 137\n",
      "action 2\n",
      "that 49\n",
      "it 40\n",
      "did—is 1\n",
      "one 11\n",
      "of 87\n",
      "ai 18\n",
      "’ 56\n",
      "s 42\n",
      "greatest 2\n",
      "weaknesses 1\n",
      "today 12\n",
      ". 126\n",
      "in 61\n",
      "general 1\n",
      ", 151\n",
      "today 12\n",
      "’ 56\n",
      "s 42\n",
      "neural 2\n",
      "networks 1\n",
      "are 27\n",
      "uninterpretable 1\n",
      "“ 15\n",
      "black 1\n",
      "boxes. 1\n",
      "” 15\n",
      "this 34\n",
      "can 24\n",
      "limit 1\n",
      "their 15\n",
      "usefulness 1\n",
      "in 61\n",
      "the 137\n",
      "real 3\n",
      "world 12\n",
      ", 151\n",
      "particularly 3\n",
      "in 61\n",
      "high-stakes 1\n",
      "settings 1\n",
      "like 13\n",
      "healthcare 1\n",
      "where 2\n",
      "human 9\n",
      "review 1\n",
      "is 46\n",
      "important 8\n",
      ". 126\n",
      "sparse 21\n",
      "expert 10\n",
      "models 44\n",
      "lend 1\n",
      "themselves 3\n",
      "more 11\n",
      "naturally 1\n",
      "to 84\n",
      "interpretability 1\n",
      "than 11\n",
      "conventional 1\n",
      "models 44\n",
      "because 3\n",
      "a 51\n",
      "sparse 21\n",
      "model 31\n",
      "’ 56\n",
      "s 42\n",
      "output 3\n",
      "is 46\n",
      "the 137\n",
      "result 1\n",
      "of 87\n",
      "an 14\n",
      "identifiable 1\n",
      ", 151\n",
      "discrete 1\n",
      "subset 2\n",
      "of 87\n",
      "parameters 10\n",
      "within 4\n",
      "the 137\n",
      "model—namely 1\n",
      ", 151\n",
      "the 137\n",
      "“ 15\n",
      "experts 6\n",
      "” 15\n",
      "that 49\n",
      "were 4\n",
      "activated 4\n",
      ". 126\n",
      "the 137\n",
      "fact 3\n",
      "that 49\n",
      "humans 3\n",
      "can 24\n",
      "better 2\n",
      "extract 1\n",
      "understandable 1\n",
      "explanations 2\n",
      "from 26\n",
      "sparse 21\n",
      "models 44\n",
      "about 4\n",
      "their 15\n",
      "behavior 1\n",
      "may 7\n",
      "prove 2\n",
      "to 84\n",
      "be 16\n",
      "a 51\n",
      "decisive 1\n",
      "advantage 2\n",
      "for 29\n",
      "these 13\n",
      "models 44\n",
      "in 61\n",
      "real-world 2\n",
      "applications 1\n",
      ". 126\n",
      "sparse 21\n",
      "expert 10\n",
      "models 44\n",
      "are 27\n",
      "not 12\n",
      "in 61\n",
      "widespread 2\n",
      "use 6\n",
      "today 12\n",
      ". 126\n",
      "they 17\n",
      "are 27\n",
      "less 4\n",
      "well 3\n",
      "understood 1\n",
      "and 54\n",
      "more 11\n",
      "technically 1\n",
      "complex 1\n",
      "to 84\n",
      "build 1\n",
      "than 11\n",
      "dense 8\n",
      "models 44\n",
      ". 126\n",
      "yet 3\n",
      "considering 1\n",
      "their 15\n",
      "potential 3\n",
      "advantages 1\n",
      ", 151\n",
      "most 8\n",
      "of 87\n",
      "all 15\n",
      "their 15\n",
      "computational 1\n",
      "efficiency 1\n",
      ", 151\n",
      "don 3\n",
      "’ 56\n",
      "t 8\n",
      "be 16\n",
      "surprised 2\n",
      "to 84\n",
      "see 3\n",
      "the 137\n",
      "sparse 21\n",
      "expert 10\n",
      "architecture 3\n",
      "become 1\n",
      "more 11\n",
      "prevalent 1\n",
      "in 61\n",
      "the 137\n",
      "world 12\n",
      "of 87\n",
      "llms 21\n",
      "going 1\n",
      "forward 2\n",
      ". 126\n",
      "in 61\n",
      "the 137\n",
      "words 2\n",
      "of 87\n",
      "graphcore 1\n",
      "cto 1\n",
      "simon 1\n",
      "knowles 1\n",
      ": 13\n",
      "“ 15\n",
      "if 10\n",
      "an 14\n",
      "ai 18\n",
      "can 24\n",
      "do 3\n",
      "many 2\n",
      "things 3\n",
      ", 151\n",
      "it 40\n",
      "doesn 1\n",
      "’ 56\n",
      "t 8\n",
      "need 1\n",
      "to 84\n",
      "access 3\n",
      "all 15\n",
      "of 87\n",
      "its 13\n",
      "knowledge 4\n",
      "to 84\n",
      "do 3\n",
      "one 11\n",
      "thing 3\n",
      ". 126\n",
      "it 40\n",
      "’ 56\n",
      "s 42\n",
      "completely 1\n",
      "obvious 1\n",
      ". 126\n",
      "this 34\n",
      "is 46\n",
      "how 4\n",
      "your 1\n",
      "brain 2\n",
      "works 1\n",
      ", 151\n",
      "and 54\n",
      "it 40\n",
      "’ 56\n",
      "s 42\n",
      "also 5\n",
      "how 4\n",
      "an 14\n",
      "ai 18\n",
      "ought 1\n",
      "to 84\n",
      "work 7\n",
      ". 126\n",
      "i 1\n",
      "’ 56\n",
      "d 1\n",
      "be 16\n",
      "surprised 2\n",
      "if 10\n",
      ", 151\n",
      "by 10\n",
      "next 5\n",
      "year 3\n",
      ", 151\n",
      "anyone 1\n",
      "is 46\n",
      "building 2\n",
      "dense 8\n",
      "language 21\n",
      "models. 1\n",
      "” 15\n",
      "note 1\n",
      ": 13\n",
      "the 137\n",
      "author 1\n",
      "is 46\n",
      "a 51\n",
      "partner 1\n",
      "at 12\n",
      "radical 1\n",
      "ventures 1\n",
      ", 151\n",
      "which 3\n",
      "is 46\n",
      "an 14\n",
      "investor 1\n",
      "in 61\n",
      "you.com 2\n",
      ". 126\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(result)):\n",
    "    for j in range (0,len(result)):\n",
    "        if(result[i]==result[j]):\n",
    "            count=count+1\n",
    "    print(result[i],count)\n",
    "    count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e467f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram \n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "wr = ['my','name','is','abc','his','name','is','xyz']\n",
    "find = BigramCollocationFinder.from_words(wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8057aa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(('my', 'name'), 1), (('name', 'is'), 2), (('is', 'abc'), 1), (('abc', 'his'), 1), (('his', 'name'), 1), (('is', 'xyz'), 1)])\n"
     ]
    }
   ],
   "source": [
    "print(find.ngram_fd.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26549b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
